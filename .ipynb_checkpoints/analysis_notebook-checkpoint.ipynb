{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e89c3f2-430c-4990-8fe3-5085e3444190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary PyTorch libraries\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Additional libraries for visualization and utilities\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from unet_decoder import UNetDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9430df8a-5d8e-41fd-8c15-4fd453a89e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    \"\"\"Selects the best available device for PyTorch computations.\n",
    "\n",
    "    Returns:\n",
    "        torch.device: The selected device.\n",
    "    \"\"\"\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "device = get_device()\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72f776dc-889a-4dbe-9d1c-eeec1694b466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Total number of images in the dataset: 19280\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor,Resize\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Define the transformation with resizing\n",
    "transform = transforms.Compose([\n",
    "    Resize((28,28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load the Omniglot dataset\n",
    "dataset = datasets.Omniglot(root='./data', download=True, transform=transform, background=True)\n",
    "\n",
    "# Print the total number of images in the dataset\n",
    "print(f\"Total number of images in the dataset: {len(dataset)}\")\n",
    "\n",
    "# Splitting dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcf6c4dc-5e89-438d-af09-a05c0d7b03ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3856 Omniglot images from the training dataset\n"
     ]
    }
   ],
   "source": [
    "# Extract images from the MNIST training data\n",
    "omniglot_images = [val_dataset[i][0] for i in range(len(val_dataset))]\n",
    "omniglot_images = torch.stack(omniglot_images)  # Convert list to tensor\n",
    "\n",
    "print(f\"Loaded {omniglot_images.size(0)} Omniglot images from the training dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "022279c2-884d-4dbf-896c-3580e14dd6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and normalized 4096 sampled images from omniglot_cold_l1_alg2.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the sampled data\n",
    "sampled_data_path = 'omniglot_cold_l1_alg2.pt'\n",
    "sampled_data = torch.load(sampled_data_path)\n",
    "\n",
    "# Extract the images from the dictionary\n",
    "sampled_images = [sampled_data[key]['sampled'] for key in sampled_data]\n",
    "sampled_images = torch.stack(sampled_images)  # Convert list to tensor\n",
    "\n",
    "# Normalize the sampled images\n",
    "sampled_images = sampled_images.float() / 255.0  # Scale back to [0, 1]\n",
    "sampled_images = (sampled_images - 0.5) / 0.5  # Normalize using the same mean and std as MNIST\n",
    "\n",
    "print(f\"Loaded and normalized {sampled_images.size(0)} sampled images from {sampled_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2926d07a-b0c0-4937-9825-a645a82c659e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the training set: 6361\n",
      "Number of images in the validation set: 1591\n",
      "Number of batches in the training loader: 100\n",
      "Total number of images in the training loader: 6400\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Create labels for the sampled data (0 for generated images)\n",
    "sampled_labels = torch.zeros(sampled_images.size(0), dtype=torch.long)\n",
    "\n",
    "# Create labels for the original MNIST data (1 for original images)\n",
    "omniglot_labels = torch.ones(omniglot_images.size(0), dtype=torch.long)\n",
    "\n",
    "# Combine the images and labels into a single dataset\n",
    "combined_images = torch.cat((sampled_images, omniglot_images), dim=0)\n",
    "combined_labels = torch.cat((sampled_labels, omniglot_labels), dim=0)\n",
    "\n",
    "# Create a permutation of indices\n",
    "indices = torch.randperm(len(combined_images))\n",
    "\n",
    "# Apply permutation to shuffle the dataset\n",
    "shuffled_images = combined_images[indices]\n",
    "shuffled_labels = combined_labels[indices]\n",
    "\n",
    "# Create a TensorDataset with the shuffled data\n",
    "combined_dataset = TensorDataset(shuffled_images, shuffled_labels)\n",
    "\n",
    "# Splitting dataset into training and validation sets\n",
    "train_size = int(0.8 * len(combined_dataset))\n",
    "val_size = len(combined_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(combined_dataset, [train_size, val_size])\n",
    "\n",
    "# Print the number of images in the train and validation sets\n",
    "print(f\"Number of images in the training set: {len(train_dataset)}\")\n",
    "print(f\"Number of images in the validation set: {len(val_dataset)}\")\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Inspect the contents of the train_loader\n",
    "train_batches = 0\n",
    "for batch in train_loader:\n",
    "    train_batches += 1\n",
    "\n",
    "print(f\"Number of batches in the training loader: {train_batches}\")\n",
    "print(f\"Total number of images in the training loader: {train_batches * 64}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ad7090a-faf0-4b02-8b2f-2bdd6ef6093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple neural network\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64*7*7, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 64*7*7)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae72a2c2-5116-40c9-93d6-cac93b9b10a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.3088, Val Loss: 0.2437, Val Accuracy: 90.57%\n",
      "Epoch [2/50], Loss: 0.2460, Val Loss: 0.2896, Val Accuracy: 88.06%\n",
      "Epoch [3/50], Loss: 0.2508, Val Loss: 0.2353, Val Accuracy: 90.89%\n",
      "Epoch [4/50], Loss: 0.2353, Val Loss: 0.2459, Val Accuracy: 90.45%\n",
      "Epoch [5/50], Loss: 0.2299, Val Loss: 0.2365, Val Accuracy: 91.07%\n",
      "Epoch [6/50], Loss: 0.2275, Val Loss: 0.2387, Val Accuracy: 91.33%\n",
      "Epoch [7/50], Loss: 0.2226, Val Loss: 0.2332, Val Accuracy: 91.14%\n",
      "Epoch [8/50], Loss: 0.2136, Val Loss: 0.2660, Val Accuracy: 89.00%\n",
      "Epoch [9/50], Loss: 0.2120, Val Loss: 0.2476, Val Accuracy: 90.13%\n",
      "Epoch [10/50], Loss: 0.1998, Val Loss: 0.2507, Val Accuracy: 90.32%\n",
      "Epoch [11/50], Loss: 0.1936, Val Loss: 0.2612, Val Accuracy: 89.31%\n",
      "Epoch [12/50], Loss: 0.1909, Val Loss: 0.2500, Val Accuracy: 90.26%\n",
      "Epoch [13/50], Loss: 0.1742, Val Loss: 0.2514, Val Accuracy: 90.13%\n",
      "Epoch [14/50], Loss: 0.1570, Val Loss: 0.2600, Val Accuracy: 90.19%\n",
      "Epoch [15/50], Loss: 0.1595, Val Loss: 0.2677, Val Accuracy: 89.88%\n",
      "Epoch [16/50], Loss: 0.1366, Val Loss: 0.2850, Val Accuracy: 89.44%\n",
      "Epoch [17/50], Loss: 0.1302, Val Loss: 0.2973, Val Accuracy: 90.45%\n",
      "Epoch [18/50], Loss: 0.1138, Val Loss: 0.3013, Val Accuracy: 89.88%\n",
      "Epoch [19/50], Loss: 0.1021, Val Loss: 0.3200, Val Accuracy: 89.06%\n",
      "Epoch [20/50], Loss: 0.0983, Val Loss: 0.3026, Val Accuracy: 90.26%\n",
      "Epoch [21/50], Loss: 0.0793, Val Loss: 0.3442, Val Accuracy: 89.82%\n",
      "Epoch [22/50], Loss: 0.0695, Val Loss: 0.3719, Val Accuracy: 89.31%\n",
      "Epoch [23/50], Loss: 0.0666, Val Loss: 0.3679, Val Accuracy: 90.07%\n",
      "Epoch [24/50], Loss: 0.0724, Val Loss: 0.3919, Val Accuracy: 89.31%\n",
      "Epoch [25/50], Loss: 0.0522, Val Loss: 0.4248, Val Accuracy: 89.82%\n",
      "Epoch [26/50], Loss: 0.0430, Val Loss: 0.4715, Val Accuracy: 89.38%\n",
      "Epoch [27/50], Loss: 0.0383, Val Loss: 0.4996, Val Accuracy: 89.00%\n",
      "Epoch [28/50], Loss: 0.0360, Val Loss: 0.4692, Val Accuracy: 89.50%\n",
      "Epoch [29/50], Loss: 0.0364, Val Loss: 0.5269, Val Accuracy: 90.45%\n",
      "Epoch [30/50], Loss: 0.0808, Val Loss: 0.4460, Val Accuracy: 89.19%\n",
      "Epoch [31/50], Loss: 0.0479, Val Loss: 0.5975, Val Accuracy: 89.38%\n",
      "Epoch [32/50], Loss: 0.0450, Val Loss: 0.4922, Val Accuracy: 90.19%\n",
      "Epoch [33/50], Loss: 0.0264, Val Loss: 0.5246, Val Accuracy: 90.32%\n",
      "Epoch [34/50], Loss: 0.0203, Val Loss: 0.5438, Val Accuracy: 90.45%\n",
      "Epoch [35/50], Loss: 0.0177, Val Loss: 0.5606, Val Accuracy: 90.76%\n",
      "Epoch [36/50], Loss: 0.0162, Val Loss: 0.5653, Val Accuracy: 90.38%\n",
      "Epoch [37/50], Loss: 0.0155, Val Loss: 0.6277, Val Accuracy: 90.38%\n",
      "Epoch [38/50], Loss: 0.0138, Val Loss: 0.6223, Val Accuracy: 90.76%\n",
      "Epoch [39/50], Loss: 0.0129, Val Loss: 0.6711, Val Accuracy: 90.19%\n",
      "Epoch [40/50], Loss: 0.0116, Val Loss: 0.6643, Val Accuracy: 90.38%\n",
      "Epoch [41/50], Loss: 0.0109, Val Loss: 0.6979, Val Accuracy: 90.51%\n",
      "Epoch [42/50], Loss: 0.0099, Val Loss: 0.6800, Val Accuracy: 90.76%\n",
      "Epoch [43/50], Loss: 0.0097, Val Loss: 0.6980, Val Accuracy: 89.75%\n",
      "Epoch [44/50], Loss: 0.0088, Val Loss: 0.7508, Val Accuracy: 90.07%\n",
      "Epoch [45/50], Loss: 0.0195, Val Loss: 0.9519, Val Accuracy: 89.13%\n",
      "Epoch [46/50], Loss: 0.1642, Val Loss: 0.4396, Val Accuracy: 89.50%\n",
      "Epoch [47/50], Loss: 0.0420, Val Loss: 0.5179, Val Accuracy: 90.76%\n",
      "Epoch [48/50], Loss: 0.0224, Val Loss: 0.5387, Val Accuracy: 90.13%\n",
      "Epoch [49/50], Loss: 0.0150, Val Loss: 0.5136, Val Accuracy: 90.76%\n",
      "Epoch [50/50], Loss: 0.0112, Val Loss: 0.5691, Val Accuracy: 90.32%\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "# Define the validate function\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return val_loss / len(val_loader), accuracy\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the neural network with validation\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    val_loss, val_accuracy = validate(model, val_loader, criterion, device)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beca2d5-a663-4cd5-816f-838a3785f9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
