{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec619f54-5dc4-4292-8727-066008d1e8ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ec619f54-5dc4-4292-8727-066008d1e8ed",
    "outputId": "5b117faa-8a74-4912-bc05-093f2d4ba270"
   },
   "outputs": [],
   "source": [
    "# Import necessary PyTorch libraries\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    " \n",
    "# Additional libraries for visualization and utilities\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c09ae2f-28f2-4a9a-80e0-5a4cab2902e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    \"\"\"Selects the best available device for PyTorch computations.\n",
    "\n",
    "    Returns:\n",
    "        torch.device: The selected device.\n",
    "    \"\"\"\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "device = get_device()\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "306efd2f-3f3b-418a-ab09-bf1173151f80",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "306efd2f-3f3b-418a-ab09-bf1173151f80",
    "outputId": "807d84ee-b69b-4434-e510-e00e6a21255c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Data loaders created for training and validation.\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Grayscale\n",
    "\n",
    "# Define transformations: Convert to grayscale, resize if needed, and normalize the data\n",
    "transform = Compose([\n",
    "    Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    ToTensor(),\n",
    "    Normalize((0.5,), (0.5,))  # Normalize with single channel\n",
    "])\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Splitting dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "print(\"Data loaders created for training and validation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "TenNg_Ywlv3n",
   "metadata": {
    "id": "TenNg_Ywlv3n"
   },
   "outputs": [],
   "source": [
    "def get_timestep_embedding(timesteps, embedding_dim):\n",
    "    \"\"\"\n",
    "    This matches the implementation in Denoising Diffusion Probabilistic Models:\n",
    "    From Fairseq.\n",
    "    Build sinusoidal embeddings.\n",
    "    This matches the implementation in tensor2tensor, but differs slightly\n",
    "    from the description in Section 3.5 of \"Attention Is All You Need\".\n",
    "    \"\"\"\n",
    "    assert len(timesteps.shape) == 1\n",
    "\n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = math.log(10000) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
    "    emb = emb.to(device=timesteps.device)\n",
    "    emb = timesteps.float()[:, None] * emb[None, :]\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "    if embedding_dim % 2 == 1:  # zero pad\n",
    "        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f08a5ac-95ad-45ae-a36e-f96dbd3f1e99",
   "metadata": {
    "id": "1f08a5ac-95ad-45ae-a36e-f96dbd3f1e99"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dims, output_shape, timestep_dim=128):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.latent_dims = latent_dims\n",
    "        self.output_shape = output_shape\n",
    "        self.timestep_dim = timestep_dim\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, latent_dims[0], kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(latent_dims[0], latent_dims[1], kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(latent_dims[1], latent_dims[2], kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(latent_dims[2], latent_dims[3], kernel_size=3, stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv2d(latent_dims[3], latent_dims[4], kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.timestep_mlp = nn.Sequential(\n",
    "            nn.Linear(timestep_dim, latent_dims[4]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_dims[4], latent_dims[4]),\n",
    "        )\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose2d(latent_dims[4] * 2, latent_dims[3], kernel_size=3, stride=1, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(latent_dims[3], latent_dims[2], kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(latent_dims[2], latent_dims[1], kernel_size=3, stride=1, padding=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(latent_dims[1], latent_dims[0], kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv5 = nn.ConvTranspose2d(latent_dims[0], output_shape[0], kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        timestep_emb = get_timestep_embedding(t, self.timestep_dim)\n",
    "        timestep_emb = self.timestep_mlp(timestep_emb)\n",
    "\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = torch.relu(self.conv5(x))\n",
    "\n",
    "        x = torch.cat([x, timestep_emb[:, :, None, None].repeat(1, 1, x.shape[2], x.shape[3])], dim=1)\n",
    "\n",
    "        x = torch.relu(self.deconv1(x))\n",
    "        x = torch.relu(self.deconv2(x))\n",
    "        x = torch.relu(self.deconv3(x))\n",
    "        x = torch.relu(self.deconv4(x))\n",
    "        x = torch.sigmoid(self.deconv5(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a77391b7-6374-4bb7-acd9-577f51a68706",
   "metadata": {
    "id": "a77391b7-6374-4bb7-acd9-577f51a68706"
   },
   "outputs": [],
   "source": [
    "class EchoModel(nn.Module):\n",
    "    def __init__(self, input_shape, latent_dims, output_shape, T=1000, batch_size=100):\n",
    "        super(EchoModel, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.latent_dims = latent_dims\n",
    "        self.output_shape = output_shape\n",
    "        self.T = T\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # self.encoder = Encoder(input_shape, latent_dims)\n",
    "        self.decoder = Decoder(latent_dims, output_shape)\n",
    "\n",
    "        # Define the noise schedule\n",
    "        self.alpha = self.create_noise_schedule(T)\n",
    "\n",
    "    def create_noise_schedule(self, T):\n",
    "        alpha = torch.linspace(0.9999, 1e-5, T)\n",
    "        return alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Calculate z as a gaussian noise tensor\n",
    "        z = torch.randn(self.batch_size,1, 32, 32).to(device)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Retrieve noise scheduler alpha_T\n",
    "        alpha_T = self.alpha[-1]\n",
    "\n",
    "        # Calculate square root alphas\n",
    "        sqrt_alpha_T = torch.sqrt(alpha_T)\n",
    "        sqrt_one_minus_alpha_T = torch.sqrt(1 - alpha_T)\n",
    "        \n",
    "        # Perform the weighted sum\n",
    "        x_T = sqrt_alpha_T * x + sqrt_one_minus_alpha_T * z\n",
    "\n",
    "        del z\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        # Perform the reconstruction process using Algorithm 2\n",
    "        reconstructed_x = self.reconstruct(x_T)\n",
    "        torch.cuda.empty_cache()\n",
    "        return reconstructed_x\n",
    "\n",
    "    def reconstruct(self, x_T):\n",
    "        x_s = x_T\n",
    "        x_0_hat = x_T\n",
    "        for s in range(self.T-1, 0, -1):\n",
    "            t = torch.tensor([s] * x_T.size(0), dtype=torch.long).to(x_T.device)\n",
    "            sqrt_alpha_s = torch.sqrt(self.alpha[s])\n",
    "            sqrt_one_minus_alpha_s = torch.sqrt(1 - self.alpha[s])\n",
    "\n",
    "            # Estimate the original image using the decoder\n",
    "            x_0_hat = self.decoder(x_s, t)\n",
    "\n",
    "            # Calculate the estimated noise using Eq. (3)\n",
    "            z_hat = (x_s - sqrt_alpha_s * x_0_hat) / sqrt_one_minus_alpha_s\n",
    "\n",
    "            # Calculate D(x_0_hat, s) and D(x_0_hat, s-1) using Eq. (5) and (6)\n",
    "            D_x_0_hat_s = sqrt_alpha_s * x_0_hat + sqrt_one_minus_alpha_s * z_hat\n",
    "            D_x_0_hat_s_minus_1 = torch.sqrt(self.alpha[s-1]) * x_0_hat + torch.sqrt(1 - self.alpha[s-1]) * z_hat\n",
    "\n",
    "            # Update x_s using Eq. (7)\n",
    "            x_s = x_s - D_x_0_hat_s + D_x_0_hat_s_minus_1\n",
    "\n",
    "        return x_0_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c466c51d-3b41-4e11-bdfd-39b3d10ed2c9",
   "metadata": {
    "id": "c466c51d-3b41-4e11-bdfd-39b3d10ed2c9"
   },
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import time  # Importing time to log the duration\n",
    "\n",
    "def validate(model, val_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient computation during validation\n",
    "        for data, _ in val_loader:\n",
    "            data = data.to(device)\n",
    "            reconstructed_x, f_x, sx_matrix = model(data)\n",
    "            reconstruction_loss = nn.functional.mse_loss(reconstructed_x, data)\n",
    "            total_val_loss += reconstruction_loss.item()  # Accumulate the validation loss\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)  # Calculate average loss\n",
    "    return avg_val_loss\n",
    "\n",
    "def train(model, optimizer, train_loader, device, num_epochs, accumulation_steps=2, checkpoint_path=\"checkpoint.pth\"):\n",
    "    model.train()\n",
    "    scaler = GradScaler()  # Initialize the gradient scaler\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        # optimizer.zero_grad()  # Move optimizer.zero_grad() outside the batch loop for gradient accumulation\n",
    "        epoch_start_time = time.time()  # Time tracking for the epoch\n",
    "\n",
    "        print(f\"Starting epoch {epoch+1}/{num_epochs}\")\n",
    "        for batch_idx, (data, _) in enumerate(train_loader):\n",
    "            batch_start_time = time.time()  # Time tracking for the batch\n",
    "            data = data.to(device)\n",
    "\n",
    "            with autocast():  # Enable automatic mixed precision\n",
    "                reconstructed_x = model(data)\n",
    "                total_loss = nn.functional.mse_loss(reconstructed_x, data)\n",
    "                print(f\"total loss: {total_loss}\")\n",
    "            # Log before the backward pass\n",
    "            print(f\"\\tBatch {batch_idx+1}/{len(train_loader)}, Forward pass done, starting backward pass.\")\n",
    "\n",
    "            # Scale the loss, but don't call optimizer.step() yet\n",
    "            if not torch.isnan(total_loss).any():\n",
    "                scaler.scale(total_loss).backward()\n",
    "            else:\n",
    "                print(f\"Warning: NaN detected in total_loss at batch {batch_idx+1}, skipping backward pass.\")\n",
    "\n",
    "\n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                scaler.step(optimizer)  # Only step the optimizer every `accumulation_steps`\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()  # Reset gradients only after accumulation\n",
    "\n",
    "            print(f\"total loss item: {total_loss.item()}\")\n",
    "\n",
    "            # Safe-guarding against NaN for epoch_loss\n",
    "            if not torch.isnan(total_loss).any():\n",
    "                epoch_loss += total_loss.item()\n",
    "            else:\n",
    "                print(f\"NaN detected, not adding to epoch_loss at batch {batch_idx+1}\")\n",
    "            print(f\"Epoch loss: {epoch_loss}\")\n",
    "\n",
    "            # Log after a batch is processed\n",
    "            print(f\"\\tBatch {batch_idx+1}/{len(train_loader)} processed in {time.time() - batch_start_time:.2f} seconds.\")\n",
    "\n",
    "        # Average loss after training for an epoch\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] completed in {time.time() - epoch_start_time:.2f} seconds, Avg Loss: {avg_loss}\")\n",
    "\n",
    "        # Validation phase\n",
    "        avg_val_loss = validate(model, val_loader, device)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] validation completed, Avg Validation Loss: {avg_val_loss}\")\n",
    "        \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "661ef76c-fb21-4fd4-b6e6-436524680219",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "661ef76c-fb21-4fd4-b6e6-436524680219",
    "outputId": "957794b4-bfe9-4884-ac9b-510743791893"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/100\n",
      "total loss: 0.5486463308334351\n",
      "\tBatch 1/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.5486463308334351\n",
      "Epoch loss: 0.5486463308334351\n",
      "\tBatch 1/400 processed in 4.99 seconds.\n",
      "total loss: 0.4671659767627716\n",
      "\tBatch 2/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.4671659767627716\n",
      "Epoch loss: 1.0158123075962067\n",
      "\tBatch 2/400 processed in 5.84 seconds.\n",
      "total loss: 0.42320191860198975\n",
      "\tBatch 3/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.42320191860198975\n",
      "Epoch loss: 1.4390142261981964\n",
      "\tBatch 3/400 processed in 5.79 seconds.\n",
      "total loss: 0.43764883279800415\n",
      "\tBatch 4/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.43764883279800415\n",
      "Epoch loss: 1.8766630589962006\n",
      "\tBatch 4/400 processed in 5.88 seconds.\n",
      "total loss: 0.24957598745822906\n",
      "\tBatch 5/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24957598745822906\n",
      "Epoch loss: 2.1262390464544296\n",
      "\tBatch 5/400 processed in 5.98 seconds.\n",
      "total loss: 0.24690654873847961\n",
      "\tBatch 6/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24690654873847961\n",
      "Epoch loss: 2.3731455951929092\n",
      "\tBatch 6/400 processed in 5.80 seconds.\n",
      "total loss: 0.2517649531364441\n",
      "\tBatch 7/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2517649531364441\n",
      "Epoch loss: 2.6249105483293533\n",
      "\tBatch 7/400 processed in 5.83 seconds.\n",
      "total loss: 0.23336683213710785\n",
      "\tBatch 8/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23336683213710785\n",
      "Epoch loss: 2.858277380466461\n",
      "\tBatch 8/400 processed in 5.86 seconds.\n",
      "total loss: 0.21129901707172394\n",
      "\tBatch 9/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21129901707172394\n",
      "Epoch loss: 3.069576397538185\n",
      "\tBatch 9/400 processed in 5.90 seconds.\n",
      "total loss: 0.21297284960746765\n",
      "\tBatch 10/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21297284960746765\n",
      "Epoch loss: 3.2825492471456528\n",
      "\tBatch 10/400 processed in 5.82 seconds.\n",
      "total loss: 0.2380656749010086\n",
      "\tBatch 11/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2380656749010086\n",
      "Epoch loss: 3.5206149220466614\n",
      "\tBatch 11/400 processed in 5.84 seconds.\n",
      "total loss: 0.2470163106918335\n",
      "\tBatch 12/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2470163106918335\n",
      "Epoch loss: 3.767631232738495\n",
      "\tBatch 12/400 processed in 5.82 seconds.\n",
      "total loss: 0.23435640335083008\n",
      "\tBatch 13/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23435640335083008\n",
      "Epoch loss: 4.001987636089325\n",
      "\tBatch 13/400 processed in 5.83 seconds.\n",
      "total loss: 0.22486886382102966\n",
      "\tBatch 14/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22486886382102966\n",
      "Epoch loss: 4.226856499910355\n",
      "\tBatch 14/400 processed in 6.04 seconds.\n",
      "total loss: 0.23098009824752808\n",
      "\tBatch 15/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23098009824752808\n",
      "Epoch loss: 4.457836598157883\n",
      "\tBatch 15/400 processed in 5.83 seconds.\n",
      "total loss: 0.2295541763305664\n",
      "\tBatch 16/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2295541763305664\n",
      "Epoch loss: 4.687390774488449\n",
      "\tBatch 16/400 processed in 5.80 seconds.\n",
      "total loss: 0.24352246522903442\n",
      "\tBatch 17/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24352246522903442\n",
      "Epoch loss: 4.9309132397174835\n",
      "\tBatch 17/400 processed in 5.81 seconds.\n",
      "total loss: 0.22626669704914093\n",
      "\tBatch 18/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22626669704914093\n",
      "Epoch loss: 5.1571799367666245\n",
      "\tBatch 18/400 processed in 5.82 seconds.\n",
      "total loss: 0.23303775489330292\n",
      "\tBatch 19/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23303775489330292\n",
      "Epoch loss: 5.390217691659927\n",
      "\tBatch 19/400 processed in 5.82 seconds.\n",
      "total loss: 0.22479738295078278\n",
      "\tBatch 20/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22479738295078278\n",
      "Epoch loss: 5.61501507461071\n",
      "\tBatch 20/400 processed in 5.82 seconds.\n",
      "total loss: 0.2416689097881317\n",
      "\tBatch 21/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2416689097881317\n",
      "Epoch loss: 5.856683984398842\n",
      "\tBatch 21/400 processed in 5.85 seconds.\n",
      "total loss: 0.2219434529542923\n",
      "\tBatch 22/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2219434529542923\n",
      "Epoch loss: 6.078627437353134\n",
      "\tBatch 22/400 processed in 5.83 seconds.\n",
      "total loss: 0.2389604151248932\n",
      "\tBatch 23/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2389604151248932\n",
      "Epoch loss: 6.317587852478027\n",
      "\tBatch 23/400 processed in 5.86 seconds.\n",
      "total loss: 0.24929457902908325\n",
      "\tBatch 24/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24929457902908325\n",
      "Epoch loss: 6.566882431507111\n",
      "\tBatch 24/400 processed in 6.03 seconds.\n",
      "total loss: 0.24505431950092316\n",
      "\tBatch 25/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24505431950092316\n",
      "Epoch loss: 6.811936751008034\n",
      "\tBatch 25/400 processed in 5.87 seconds.\n",
      "total loss: 0.23201106488704681\n",
      "\tBatch 26/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23201106488704681\n",
      "Epoch loss: 7.043947815895081\n",
      "\tBatch 26/400 processed in 5.85 seconds.\n",
      "total loss: 0.2172403484582901\n",
      "\tBatch 27/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2172403484582901\n",
      "Epoch loss: 7.261188164353371\n",
      "\tBatch 27/400 processed in 5.85 seconds.\n",
      "total loss: 0.2417708933353424\n",
      "\tBatch 28/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2417708933353424\n",
      "Epoch loss: 7.502959057688713\n",
      "\tBatch 28/400 processed in 5.84 seconds.\n",
      "total loss: 0.23783379793167114\n",
      "\tBatch 29/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23783379793167114\n",
      "Epoch loss: 7.740792855620384\n",
      "\tBatch 29/400 processed in 5.89 seconds.\n",
      "total loss: 0.22610755264759064\n",
      "\tBatch 30/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22610755264759064\n",
      "Epoch loss: 7.966900408267975\n",
      "\tBatch 30/400 processed in 5.93 seconds.\n",
      "total loss: 0.22353678941726685\n",
      "\tBatch 31/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22353678941726685\n",
      "Epoch loss: 8.190437197685242\n",
      "\tBatch 31/400 processed in 5.89 seconds.\n",
      "total loss: 0.24778448045253754\n",
      "\tBatch 32/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24778448045253754\n",
      "Epoch loss: 8.43822167813778\n",
      "\tBatch 32/400 processed in 5.83 seconds.\n",
      "total loss: 0.2329472303390503\n",
      "\tBatch 33/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2329472303390503\n",
      "Epoch loss: 8.67116890847683\n",
      "\tBatch 33/400 processed in 5.76 seconds.\n",
      "total loss: 0.21536487340927124\n",
      "\tBatch 34/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21536487340927124\n",
      "Epoch loss: 8.8865337818861\n",
      "\tBatch 34/400 processed in 5.95 seconds.\n",
      "total loss: 0.21742483973503113\n",
      "\tBatch 35/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21742483973503113\n",
      "Epoch loss: 9.103958621621132\n",
      "\tBatch 35/400 processed in 5.82 seconds.\n",
      "total loss: 0.24496836960315704\n",
      "\tBatch 36/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24496836960315704\n",
      "Epoch loss: 9.348926991224289\n",
      "\tBatch 36/400 processed in 5.78 seconds.\n",
      "total loss: 0.23692691326141357\n",
      "\tBatch 37/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23692691326141357\n",
      "Epoch loss: 9.585853904485703\n",
      "\tBatch 37/400 processed in 5.73 seconds.\n",
      "total loss: 0.22845309972763062\n",
      "\tBatch 38/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22845309972763062\n",
      "Epoch loss: 9.814307004213333\n",
      "\tBatch 38/400 processed in 5.69 seconds.\n",
      "total loss: 0.2234768122434616\n",
      "\tBatch 39/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2234768122434616\n",
      "Epoch loss: 10.037783816456795\n",
      "\tBatch 39/400 processed in 5.60 seconds.\n",
      "total loss: 0.23393778502941132\n",
      "\tBatch 40/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23393778502941132\n",
      "Epoch loss: 10.271721601486206\n",
      "\tBatch 40/400 processed in 5.74 seconds.\n",
      "total loss: 0.22496463358402252\n",
      "\tBatch 41/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22496463358402252\n",
      "Epoch loss: 10.496686235070229\n",
      "\tBatch 41/400 processed in 5.58 seconds.\n",
      "total loss: 0.23873254656791687\n",
      "\tBatch 42/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23873254656791687\n",
      "Epoch loss: 10.735418781638145\n",
      "\tBatch 42/400 processed in 5.61 seconds.\n",
      "total loss: 0.2464233934879303\n",
      "\tBatch 43/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2464233934879303\n",
      "Epoch loss: 10.981842175126076\n",
      "\tBatch 43/400 processed in 5.85 seconds.\n",
      "total loss: 0.22168488800525665\n",
      "\tBatch 44/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22168488800525665\n",
      "Epoch loss: 11.203527063131332\n",
      "\tBatch 44/400 processed in 5.58 seconds.\n",
      "total loss: 0.21306456625461578\n",
      "\tBatch 45/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21306456625461578\n",
      "Epoch loss: 11.416591629385948\n",
      "\tBatch 45/400 processed in 5.52 seconds.\n",
      "total loss: 0.22220847010612488\n",
      "\tBatch 46/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22220847010612488\n",
      "Epoch loss: 11.638800099492073\n",
      "\tBatch 46/400 processed in 5.65 seconds.\n",
      "total loss: 0.24504035711288452\n",
      "\tBatch 47/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24504035711288452\n",
      "Epoch loss: 11.883840456604958\n",
      "\tBatch 47/400 processed in 5.52 seconds.\n",
      "total loss: 0.2545022964477539\n",
      "\tBatch 48/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2545022964477539\n",
      "Epoch loss: 12.138342753052711\n",
      "\tBatch 48/400 processed in 5.59 seconds.\n",
      "total loss: 0.24124190211296082\n",
      "\tBatch 49/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24124190211296082\n",
      "Epoch loss: 12.379584655165672\n",
      "\tBatch 49/400 processed in 5.56 seconds.\n",
      "total loss: 0.2471427172422409\n",
      "\tBatch 50/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2471427172422409\n",
      "Epoch loss: 12.626727372407913\n",
      "\tBatch 50/400 processed in 5.56 seconds.\n",
      "total loss: 0.24475215375423431\n",
      "\tBatch 51/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24475215375423431\n",
      "Epoch loss: 12.871479526162148\n",
      "\tBatch 51/400 processed in 5.89 seconds.\n",
      "total loss: 0.20116233825683594\n",
      "\tBatch 52/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.20116233825683594\n",
      "Epoch loss: 13.072641864418983\n",
      "\tBatch 52/400 processed in 5.61 seconds.\n",
      "total loss: 0.2228594720363617\n",
      "\tBatch 53/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2228594720363617\n",
      "Epoch loss: 13.295501336455345\n",
      "\tBatch 53/400 processed in 5.59 seconds.\n",
      "total loss: 0.23475734889507294\n",
      "\tBatch 54/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23475734889507294\n",
      "Epoch loss: 13.530258685350418\n",
      "\tBatch 54/400 processed in 5.55 seconds.\n",
      "total loss: 0.23289655148983002\n",
      "\tBatch 55/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23289655148983002\n",
      "Epoch loss: 13.763155236840248\n",
      "\tBatch 55/400 processed in 5.59 seconds.\n",
      "total loss: 0.21426665782928467\n",
      "\tBatch 56/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21426665782928467\n",
      "Epoch loss: 13.977421894669533\n",
      "\tBatch 56/400 processed in 5.77 seconds.\n",
      "total loss: 0.24114809930324554\n",
      "\tBatch 57/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24114809930324554\n",
      "Epoch loss: 14.218569993972778\n",
      "\tBatch 57/400 processed in 5.54 seconds.\n",
      "total loss: 0.21227551996707916\n",
      "\tBatch 58/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21227551996707916\n",
      "Epoch loss: 14.430845513939857\n",
      "\tBatch 58/400 processed in 5.60 seconds.\n",
      "total loss: 0.2329620122909546\n",
      "\tBatch 59/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2329620122909546\n",
      "Epoch loss: 14.663807526230812\n",
      "\tBatch 59/400 processed in 5.55 seconds.\n",
      "total loss: 0.2118278592824936\n",
      "\tBatch 60/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2118278592824936\n",
      "Epoch loss: 14.875635385513306\n",
      "\tBatch 60/400 processed in 5.57 seconds.\n",
      "total loss: 0.23876667022705078\n",
      "\tBatch 61/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23876667022705078\n",
      "Epoch loss: 15.114402055740356\n",
      "\tBatch 61/400 processed in 5.57 seconds.\n",
      "total loss: 0.2268095165491104\n",
      "\tBatch 62/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2268095165491104\n",
      "Epoch loss: 15.341211572289467\n",
      "\tBatch 62/400 processed in 5.80 seconds.\n",
      "total loss: 0.21212074160575867\n",
      "\tBatch 63/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21212074160575867\n",
      "Epoch loss: 15.553332313895226\n",
      "\tBatch 63/400 processed in 5.57 seconds.\n",
      "total loss: 0.2275358885526657\n",
      "\tBatch 64/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2275358885526657\n",
      "Epoch loss: 15.780868202447891\n",
      "\tBatch 64/400 processed in 5.59 seconds.\n",
      "total loss: 0.21085913479328156\n",
      "\tBatch 65/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21085913479328156\n",
      "Epoch loss: 15.991727337241173\n",
      "\tBatch 65/400 processed in 5.58 seconds.\n",
      "total loss: 0.22964534163475037\n",
      "\tBatch 66/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22964534163475037\n",
      "Epoch loss: 16.221372678875923\n",
      "\tBatch 66/400 processed in 5.65 seconds.\n",
      "total loss: 0.20103460550308228\n",
      "\tBatch 67/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.20103460550308228\n",
      "Epoch loss: 16.422407284379005\n",
      "\tBatch 67/400 processed in 5.64 seconds.\n",
      "total loss: 0.2248651087284088\n",
      "\tBatch 68/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2248651087284088\n",
      "Epoch loss: 16.647272393107414\n",
      "\tBatch 68/400 processed in 5.55 seconds.\n",
      "total loss: 0.20448581874370575\n",
      "\tBatch 69/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.20448581874370575\n",
      "Epoch loss: 16.85175821185112\n",
      "\tBatch 69/400 processed in 5.57 seconds.\n",
      "total loss: 0.20296616852283478\n",
      "\tBatch 70/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.20296616852283478\n",
      "Epoch loss: 17.054724380373955\n",
      "\tBatch 70/400 processed in 5.71 seconds.\n",
      "total loss: 0.22188006341457367\n",
      "\tBatch 71/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22188006341457367\n",
      "Epoch loss: 17.27660444378853\n",
      "\tBatch 71/400 processed in 5.55 seconds.\n",
      "total loss: 0.21825887262821198\n",
      "\tBatch 72/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21825887262821198\n",
      "Epoch loss: 17.49486331641674\n",
      "\tBatch 72/400 processed in 5.99 seconds.\n",
      "total loss: 0.21942584216594696\n",
      "\tBatch 73/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21942584216594696\n",
      "Epoch loss: 17.714289158582687\n",
      "\tBatch 73/400 processed in 5.59 seconds.\n",
      "total loss: 0.2492036372423172\n",
      "\tBatch 74/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2492036372423172\n",
      "Epoch loss: 17.963492795825005\n",
      "\tBatch 74/400 processed in 5.58 seconds.\n",
      "total loss: 0.23074594140052795\n",
      "\tBatch 75/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23074594140052795\n",
      "Epoch loss: 18.194238737225533\n",
      "\tBatch 75/400 processed in 5.56 seconds.\n",
      "total loss: 0.24470765888690948\n",
      "\tBatch 76/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24470765888690948\n",
      "Epoch loss: 18.438946396112442\n",
      "\tBatch 76/400 processed in 5.57 seconds.\n",
      "total loss: 0.22078298032283783\n",
      "\tBatch 77/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22078298032283783\n",
      "Epoch loss: 18.65972937643528\n",
      "\tBatch 77/400 processed in 5.61 seconds.\n",
      "total loss: 0.22145000100135803\n",
      "\tBatch 78/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22145000100135803\n",
      "Epoch loss: 18.881179377436638\n",
      "\tBatch 78/400 processed in 5.59 seconds.\n",
      "total loss: 0.24009183049201965\n",
      "\tBatch 79/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24009183049201965\n",
      "Epoch loss: 19.121271207928658\n",
      "\tBatch 79/400 processed in 5.55 seconds.\n",
      "total loss: 0.2119053602218628\n",
      "\tBatch 80/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2119053602218628\n",
      "Epoch loss: 19.33317656815052\n",
      "\tBatch 80/400 processed in 5.58 seconds.\n",
      "total loss: 0.23711955547332764\n",
      "\tBatch 81/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23711955547332764\n",
      "Epoch loss: 19.570296123623848\n",
      "\tBatch 81/400 processed in 5.83 seconds.\n",
      "total loss: 0.23667873442173004\n",
      "\tBatch 82/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23667873442173004\n",
      "Epoch loss: 19.806974858045578\n",
      "\tBatch 82/400 processed in 5.62 seconds.\n",
      "total loss: 0.22332759201526642\n",
      "\tBatch 83/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22332759201526642\n",
      "Epoch loss: 20.030302450060844\n",
      "\tBatch 83/400 processed in 5.63 seconds.\n",
      "total loss: 0.21949604153633118\n",
      "\tBatch 84/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21949604153633118\n",
      "Epoch loss: 20.249798491597176\n",
      "\tBatch 84/400 processed in 5.62 seconds.\n",
      "total loss: 0.23683099448680878\n",
      "\tBatch 85/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23683099448680878\n",
      "Epoch loss: 20.486629486083984\n",
      "\tBatch 85/400 processed in 5.61 seconds.\n",
      "total loss: 0.23862506449222565\n",
      "\tBatch 86/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23862506449222565\n",
      "Epoch loss: 20.72525455057621\n",
      "\tBatch 86/400 processed in 5.60 seconds.\n",
      "total loss: 0.2265099287033081\n",
      "\tBatch 87/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2265099287033081\n",
      "Epoch loss: 20.951764479279518\n",
      "\tBatch 87/400 processed in 5.61 seconds.\n",
      "total loss: 0.23113124072551727\n",
      "\tBatch 88/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23113124072551727\n",
      "Epoch loss: 21.182895720005035\n",
      "\tBatch 88/400 processed in 5.69 seconds.\n",
      "total loss: 0.23011012375354767\n",
      "\tBatch 89/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23011012375354767\n",
      "Epoch loss: 21.413005843758583\n",
      "\tBatch 89/400 processed in 5.79 seconds.\n",
      "total loss: 0.2301764190196991\n",
      "\tBatch 90/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2301764190196991\n",
      "Epoch loss: 21.643182262778282\n",
      "\tBatch 90/400 processed in 5.58 seconds.\n",
      "total loss: 0.23105990886688232\n",
      "\tBatch 91/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23105990886688232\n",
      "Epoch loss: 21.874242171645164\n",
      "\tBatch 91/400 processed in 5.59 seconds.\n",
      "total loss: 0.24528424441814423\n",
      "\tBatch 92/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24528424441814423\n",
      "Epoch loss: 22.11952641606331\n",
      "\tBatch 92/400 processed in 5.57 seconds.\n",
      "total loss: 0.22197212278842926\n",
      "\tBatch 93/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22197212278842926\n",
      "Epoch loss: 22.341498538851738\n",
      "\tBatch 93/400 processed in 5.73 seconds.\n",
      "total loss: 0.2150142937898636\n",
      "\tBatch 94/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2150142937898636\n",
      "Epoch loss: 22.5565128326416\n",
      "\tBatch 94/400 processed in 5.58 seconds.\n",
      "total loss: 0.24417434632778168\n",
      "\tBatch 95/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24417434632778168\n",
      "Epoch loss: 22.800687178969383\n",
      "\tBatch 95/400 processed in 5.60 seconds.\n",
      "total loss: 0.23008351027965546\n",
      "\tBatch 96/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23008351027965546\n",
      "Epoch loss: 23.03077068924904\n",
      "\tBatch 96/400 processed in 5.58 seconds.\n",
      "total loss: 0.25397926568984985\n",
      "\tBatch 97/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.25397926568984985\n",
      "Epoch loss: 23.28474995493889\n",
      "\tBatch 97/400 processed in 5.67 seconds.\n",
      "total loss: 0.2252463847398758\n",
      "\tBatch 98/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2252463847398758\n",
      "Epoch loss: 23.509996339678764\n",
      "\tBatch 98/400 processed in 5.69 seconds.\n",
      "total loss: 0.21658110618591309\n",
      "\tBatch 99/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21658110618591309\n",
      "Epoch loss: 23.726577445864677\n",
      "\tBatch 99/400 processed in 5.69 seconds.\n",
      "total loss: 0.22635284066200256\n",
      "\tBatch 100/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22635284066200256\n",
      "Epoch loss: 23.95293028652668\n",
      "\tBatch 100/400 processed in 5.84 seconds.\n",
      "total loss: 0.24870334565639496\n",
      "\tBatch 101/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24870334565639496\n",
      "Epoch loss: 24.201633632183075\n",
      "\tBatch 101/400 processed in 5.72 seconds.\n",
      "total loss: 0.23951344192028046\n",
      "\tBatch 102/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23951344192028046\n",
      "Epoch loss: 24.441147074103355\n",
      "\tBatch 102/400 processed in 5.62 seconds.\n",
      "total loss: 0.22083795070648193\n",
      "\tBatch 103/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22083795070648193\n",
      "Epoch loss: 24.661985024809837\n",
      "\tBatch 103/400 processed in 5.70 seconds.\n",
      "total loss: 0.2276611030101776\n",
      "\tBatch 104/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2276611030101776\n",
      "Epoch loss: 24.889646127820015\n",
      "\tBatch 104/400 processed in 5.88 seconds.\n",
      "total loss: 0.22246286273002625\n",
      "\tBatch 105/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22246286273002625\n",
      "Epoch loss: 25.11210899055004\n",
      "\tBatch 105/400 processed in 5.68 seconds.\n",
      "total loss: 0.23825080692768097\n",
      "\tBatch 106/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23825080692768097\n",
      "Epoch loss: 25.350359797477722\n",
      "\tBatch 106/400 processed in 5.68 seconds.\n",
      "total loss: 0.23642046749591827\n",
      "\tBatch 107/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23642046749591827\n",
      "Epoch loss: 25.58678026497364\n",
      "\tBatch 107/400 processed in 5.66 seconds.\n",
      "total loss: 0.23293903470039368\n",
      "\tBatch 108/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23293903470039368\n",
      "Epoch loss: 25.819719299674034\n",
      "\tBatch 108/400 processed in 5.67 seconds.\n",
      "total loss: 0.23412998020648956\n",
      "\tBatch 109/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23412998020648956\n",
      "Epoch loss: 26.053849279880524\n",
      "\tBatch 109/400 processed in 5.76 seconds.\n",
      "total loss: 0.25408098101615906\n",
      "\tBatch 110/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.25408098101615906\n",
      "Epoch loss: 26.307930260896683\n",
      "\tBatch 110/400 processed in 5.87 seconds.\n",
      "total loss: 0.23477953672409058\n",
      "\tBatch 111/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23477953672409058\n",
      "Epoch loss: 26.542709797620773\n",
      "\tBatch 111/400 processed in 5.70 seconds.\n",
      "total loss: 0.23320797085762024\n",
      "\tBatch 112/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23320797085762024\n",
      "Epoch loss: 26.775917768478394\n",
      "\tBatch 112/400 processed in 5.70 seconds.\n",
      "total loss: 0.2165273278951645\n",
      "\tBatch 113/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2165273278951645\n",
      "Epoch loss: 26.992445096373558\n",
      "\tBatch 113/400 processed in 5.71 seconds.\n",
      "total loss: 0.22815978527069092\n",
      "\tBatch 114/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22815978527069092\n",
      "Epoch loss: 27.22060488164425\n",
      "\tBatch 114/400 processed in 5.83 seconds.\n",
      "total loss: 0.21773271262645721\n",
      "\tBatch 115/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21773271262645721\n",
      "Epoch loss: 27.438337594270706\n",
      "\tBatch 115/400 processed in 5.71 seconds.\n",
      "total loss: 0.25349363684654236\n",
      "\tBatch 116/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.25349363684654236\n",
      "Epoch loss: 27.69183123111725\n",
      "\tBatch 116/400 processed in 5.72 seconds.\n",
      "total loss: 0.24535110592842102\n",
      "\tBatch 117/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24535110592842102\n",
      "Epoch loss: 27.93718233704567\n",
      "\tBatch 117/400 processed in 5.65 seconds.\n",
      "total loss: 0.24514342844486237\n",
      "\tBatch 118/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24514342844486237\n",
      "Epoch loss: 28.182325765490532\n",
      "\tBatch 118/400 processed in 5.73 seconds.\n",
      "total loss: 0.22732779383659363\n",
      "\tBatch 119/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22732779383659363\n",
      "Epoch loss: 28.409653559327126\n",
      "\tBatch 119/400 processed in 5.92 seconds.\n",
      "total loss: 0.2237565815448761\n",
      "\tBatch 120/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2237565815448761\n",
      "Epoch loss: 28.633410140872\n",
      "\tBatch 120/400 processed in 6.91 seconds.\n",
      "total loss: 0.24007876217365265\n",
      "\tBatch 121/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24007876217365265\n",
      "Epoch loss: 28.873488903045654\n",
      "\tBatch 121/400 processed in 6.33 seconds.\n",
      "total loss: 0.2520033121109009\n",
      "\tBatch 122/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2520033121109009\n",
      "Epoch loss: 29.125492215156555\n",
      "\tBatch 122/400 processed in 6.76 seconds.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m loss_weights \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreconstruction\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmi_penalty\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.0\u001b[39m}  \u001b[38;5;66;03m# Adjust the weights as needed\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 32\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_loader, device, num_epochs, loss_weights, accumulation_steps, checkpoint_path)\u001b[0m\n\u001b[1;32m     29\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():  \u001b[38;5;66;03m# Enable automatic mixed precision\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     reconstructed_x \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# reconstruction_loss = nn.functional.l1_loss(reconstructed_x, data)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     reconstruction_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmse_loss(reconstructed_x, data)\n",
      "File \u001b[0;32m~/miniconda3/envs/parthaenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/parthaenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 25\u001b[0m, in \u001b[0;36mEchoModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Calculate z as a gaussian noise tensor\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size,\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 25\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Retrieve noise scheduler alpha_T\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     alpha_T \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/parthaenv/lib/python3.10/site-packages/torch/cuda/memory.py:162\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 162\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the input shape, latent dimensions, and output shape\n",
    "input_shape = (1, 32, 32)  # Shape for grayscale CIFAR-10 (1 channel, 32x32 images)\n",
    "latent_dims = [32, 64, 128, 256, 512]  # Updated latent dimensions\n",
    "output_shape = (1, 32, 32)  # Shape for grayscale CIFAR-10\n",
    "\n",
    "# Create an instance of the EchoModel\n",
    "model = EchoModel(input_shape, latent_dims, output_shape).to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Define the number of epochs and loss weights\n",
    "num_epochs = 100\n",
    "\n",
    "# Train the model\n",
    "trained_model = train(model, optimizer, train_loader, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "qmfrva2uRuvu",
   "metadata": {
    "id": "qmfrva2uRuvu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at epoch 1 to checkpoint-gaussian.pth\n"
     ]
    }
   ],
   "source": [
    "epoch = 1\n",
    "save_checkpoint(epoch, model, optimizer, filename=\"checkpoint-gaussian.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5040ed-080f-48d4-a005-f2e528884474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
