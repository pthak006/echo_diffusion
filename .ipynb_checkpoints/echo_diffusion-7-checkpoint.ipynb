{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec619f54-5dc4-4292-8727-066008d1e8ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ec619f54-5dc4-4292-8727-066008d1e8ed",
    "outputId": "5b117faa-8a74-4912-bc05-093f2d4ba270"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import necessary PyTorch libraries\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    " \n",
    "# Set the device to MPS if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Additional libraries for visualization and utilities\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a353550-e380-4555-8519-25b35c5b1654",
   "metadata": {
    "id": "8a353550-e380-4555-8519-25b35c5b1654"
   },
   "outputs": [],
   "source": [
    "# Import the adapted Echo noise functions\n",
    "from echo import echo_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "306efd2f-3f3b-418a-ab09-bf1173151f80",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "306efd2f-3f3b-418a-ab09-bf1173151f80",
    "outputId": "807d84ee-b69b-4434-e510-e00e6a21255c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Data loaders created for training and validation.\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Grayscale\n",
    "\n",
    "# Define transformations: Convert to grayscale, resize if needed, and normalize the data\n",
    "transform = Compose([\n",
    "    Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    ToTensor(),\n",
    "    Normalize((0.5,), (0.5,))  # Normalize with single channel\n",
    "])\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Splitting dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "print(\"Data loaders created for training and validation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5860ada1-b1cb-45f5-accd-933906b45160",
   "metadata": {
    "id": "5860ada1-b1cb-45f5-accd-933906b45160"
   },
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch import Unet\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_shape, latent_dims):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.latent_dims = latent_dims\n",
    "\n",
    "        self.unet = Unet(\n",
    "            encoder_name=\"resnet18\",\n",
    "            encoder_depth=3,\n",
    "            encoder_weights=None,\n",
    "            decoder_use_batchnorm=True,\n",
    "            decoder_channels=(latent_dims[2], latent_dims[1], latent_dims[0]),\n",
    "            decoder_attention_type=None,\n",
    "            in_channels=input_shape[0],\n",
    "            classes=latent_dims[-1],\n",
    "            activation=None,\n",
    "        )\n",
    "\n",
    "        # Output layers\n",
    "        self.out_mean = nn.Conv2d(latent_dims[-1], input_shape[0], kernel_size=1)\n",
    "        self.out_log_var = nn.Conv2d(latent_dims[-1], input_shape[0], kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.unet(x)\n",
    "        f_x = torch.tanh(self.out_mean(x))\n",
    "        log_var = torch.sigmoid(self.out_log_var(x))\n",
    "        return f_x, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "TenNg_Ywlv3n",
   "metadata": {
    "id": "TenNg_Ywlv3n"
   },
   "outputs": [],
   "source": [
    "def get_timestep_embedding(timesteps, embedding_dim):\n",
    "    \"\"\"\n",
    "    This matches the implementation in Denoising Diffusion Probabilistic Models:\n",
    "    From Fairseq.\n",
    "    Build sinusoidal embeddings.\n",
    "    This matches the implementation in tensor2tensor, but differs slightly\n",
    "    from the description in Section 3.5 of \"Attention Is All You Need\".\n",
    "    \"\"\"\n",
    "    assert len(timesteps.shape) == 1\n",
    "\n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = math.log(10000) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
    "    emb = emb.to(device=timesteps.device)\n",
    "    emb = timesteps.float()[:, None] * emb[None, :]\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "    if embedding_dim % 2 == 1:  # zero pad\n",
    "        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f08a5ac-95ad-45ae-a36e-f96dbd3f1e99",
   "metadata": {
    "id": "1f08a5ac-95ad-45ae-a36e-f96dbd3f1e99"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dims, output_shape, timestep_dim=128):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.latent_dims = latent_dims\n",
    "        self.output_shape = output_shape\n",
    "        self.timestep_dim = timestep_dim\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, latent_dims[0], kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(latent_dims[0], latent_dims[1], kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(latent_dims[1], latent_dims[2], kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(latent_dims[2], latent_dims[3], kernel_size=3, stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv2d(latent_dims[3], latent_dims[4], kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.timestep_mlp = nn.Sequential(\n",
    "            nn.Linear(timestep_dim, latent_dims[4]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_dims[4], latent_dims[4]),\n",
    "        )\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose2d(latent_dims[4] * 2, latent_dims[3], kernel_size=3, stride=1, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(latent_dims[3], latent_dims[2], kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(latent_dims[2], latent_dims[1], kernel_size=3, stride=1, padding=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(latent_dims[1], latent_dims[0], kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv5 = nn.ConvTranspose2d(latent_dims[0], output_shape[0], kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        timestep_emb = get_timestep_embedding(t, self.timestep_dim)\n",
    "        timestep_emb = self.timestep_mlp(timestep_emb)\n",
    "\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = torch.relu(self.conv5(x))\n",
    "\n",
    "        x = torch.cat([x, timestep_emb[:, :, None, None].repeat(1, 1, x.shape[2], x.shape[3])], dim=1)\n",
    "\n",
    "        x = torch.relu(self.deconv1(x))\n",
    "        x = torch.relu(self.deconv2(x))\n",
    "        x = torch.relu(self.deconv3(x))\n",
    "        x = torch.relu(self.deconv4(x))\n",
    "        x = torch.sigmoid(self.deconv5(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a77391b7-6374-4bb7-acd9-577f51a68706",
   "metadata": {
    "id": "a77391b7-6374-4bb7-acd9-577f51a68706"
   },
   "outputs": [],
   "source": [
    "class EchoModel(nn.Module):\n",
    "    def __init__(self, input_shape, latent_dims, output_shape, T=1000, batch_size=100):\n",
    "        super(EchoModel, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.latent_dims = latent_dims\n",
    "        self.output_shape = output_shape\n",
    "        self.T = T\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.encoder = Encoder(input_shape, latent_dims)\n",
    "        self.decoder = Decoder(latent_dims, output_shape)\n",
    "\n",
    "        # Define the noise schedule\n",
    "        self.alpha = self.create_noise_schedule(T)\n",
    "\n",
    "    def create_noise_schedule(self, T):\n",
    "        alpha = torch.linspace(0.9999, 1e-5, T)\n",
    "        return alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Calculate f_x and S_x\n",
    "        f_x, sx_matrix = self.encoder(x)\n",
    "        \n",
    "        #Calculate epsilon in a detached way\n",
    "        epsilon = echo_sample((f_x, sx_matrix)).detach()\n",
    "\n",
    "        #Calculate echo output z \n",
    "        z = f_x + sx_matrix * epsilon\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Retrieve noise scheduler alpha_T\n",
    "        alpha_T = self.alpha[-1]\n",
    "\n",
    "        # Calculate square root alphas\n",
    "        sqrt_alpha_T = torch.sqrt(alpha_T)\n",
    "        sqrt_one_minus_alpha_T = torch.sqrt(1 - alpha_T)\n",
    "        \n",
    "        # Perform the weighted sum\n",
    "        x_T = sqrt_alpha_T * x + sqrt_one_minus_alpha_T * z\n",
    "\n",
    "\n",
    "        # Perform the reconstruction process using Algorithm 2\n",
    "        reconstructed_x = self.reconstruct(x_T, z, f_x, sx_matrix)\n",
    "        return reconstructed_x, f_x, sx_matrix\n",
    "\n",
    "    def reconstruct(self, x_t, z, f_x, sx_matrix):\n",
    "        x_s = x_t\n",
    "        for s in range(self.T-1, 0, -1):\n",
    "            t = torch.tensor([s] * x_t.size(0), dtype=torch.long).to(x_t.device)\n",
    "            sqrt_alpha_s = torch.sqrt(self.alpha[s])\n",
    "            sqrt_one_minus_alpha_s = torch.sqrt(1 - self.alpha[s])\n",
    "\n",
    "            # Estimate the original image using the decoder\n",
    "            x_0_hat = self.decoder(x_s, t)\n",
    "\n",
    "            # Calculate the estimated noise using Eq. (3)\n",
    "            z_hat = (x_s - sqrt_alpha_s * x_0_hat) / sqrt_one_minus_alpha_s\n",
    "\n",
    "            # Calculate D(x_0_hat, s) and D(x_0_hat, s-1) using Eq. (5) and (6)\n",
    "            D_x_0_hat_s = sqrt_alpha_s * x_0_hat + sqrt_one_minus_alpha_s * z_hat\n",
    "            D_x_0_hat_s_minus_1 = torch.sqrt(self.alpha[s-1]) * x_0_hat + torch.sqrt(1 - self.alpha[s-1]) * z_hat\n",
    "\n",
    "            # Update x_s using Eq. (7)\n",
    "            x_s = x_s - D_x_0_hat_s + D_x_0_hat_s_minus_1\n",
    "\n",
    "        return x_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c466c51d-3b41-4e11-bdfd-39b3d10ed2c9",
   "metadata": {
    "id": "c466c51d-3b41-4e11-bdfd-39b3d10ed2c9"
   },
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import time  # Importing time to log the duration\n",
    "\n",
    "import os\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, filename=\"checkpoint.pth\"):\n",
    "    \"\"\"Saves the model and optimizer state at the specified path.\"\"\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, filename)\n",
    "    print(f\"Checkpoint saved at epoch {epoch} to {filename}\")\n",
    "\n",
    "\n",
    "def train(model, optimizer, train_loader, device, num_epochs, loss_weights, accumulation_steps=2, checkpoint_path=\"checkpoint.pth\"):\n",
    "    # torch.autograd.set_detect_anomaly(True)  # Enable anomaly detection\n",
    "    model.train()\n",
    "    scaler = GradScaler()  # Initialize the gradient scaler\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print(f\"Resuming training from epoch {start_epoch}\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        optimizer.zero_grad()  # Move optimizer.zero_grad() outside the batch loop for gradient accumulation\n",
    "        epoch_start_time = time.time()  # Time tracking for the epoch\n",
    "\n",
    "        print(f\"Starting epoch {epoch+1}/{num_epochs}\")\n",
    "        for batch_idx, (data, _) in enumerate(train_loader):\n",
    "            batch_start_time = time.time()  # Time tracking for the batch\n",
    "            data = data.to(device)\n",
    "\n",
    "            with autocast():  # Enable automatic mixed precision\n",
    "                reconstructed_x, f_x, sx_matrix = model(data)\n",
    "                # reconstruction_loss = nn.functional.l1_loss(reconstructed_x, data)\n",
    "                reconstruction_loss = nn.functional.mse_loss(reconstructed_x, data)\n",
    "                # mi_penalty = echo_loss([f_x, sx_matrix])\n",
    "                # total_loss = (loss_weights['reconstruction'] * reconstruction_loss + loss_weights['mi_penalty'] * mi_penalty) / accumulation_steps\n",
    "                total_loss = (loss_weights['reconstruction'] * reconstruction_loss)\n",
    "                print(f\"total loss: {total_loss}\")\n",
    "            # Log before the backward pass\n",
    "            print(f\"\\tBatch {batch_idx+1}/{len(train_loader)}, Forward pass done, starting backward pass.\")\n",
    "\n",
    "            # Scale the loss, but don't call optimizer.step() yet\n",
    "            if not torch.isnan(total_loss).any():\n",
    "                scaler.scale(total_loss).backward()\n",
    "            else:\n",
    "                print(f\"Warning: NaN detected in total_loss at batch {batch_idx+1}, skipping backward pass.\")\n",
    "\n",
    "\n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                scaler.step(optimizer)  # Only step the optimizer every `accumulation_steps`\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()  # Reset gradients only after accumulation\n",
    "\n",
    "            print(f\"total loss item: {total_loss.item()}\")\n",
    "\n",
    "            # Safe-guarding against NaN for epoch_loss\n",
    "            if not torch.isnan(total_loss).any():\n",
    "                epoch_loss += total_loss.item()\n",
    "            else:\n",
    "                print(f\"NaN detected, not adding to epoch_loss at batch {batch_idx+1}\")\n",
    "            print(f\"Epoch loss: {epoch_loss}\")\n",
    "\n",
    "            # Log after a batch is processed\n",
    "            print(f\"\\tBatch {batch_idx+1}/{len(train_loader)} processed in {time.time() - batch_start_time:.2f} seconds.\")\n",
    "\n",
    "        print(f\"Epoch loss accumulated: {epoch_loss}\")\n",
    "        print(f\"Train loader length as of now: {len(train_loader)}\")\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        save_checkpoint(epoch, model, optimizer, checkpoint_path)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] completed in {time.time() - epoch_start_time:.2f} seconds, Avg Loss: {avg_loss}\")\n",
    "        \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "661ef76c-fb21-4fd4-b6e6-436524680219",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "661ef76c-fb21-4fd4-b6e6-436524680219",
    "outputId": "957794b4-bfe9-4884-ac9b-510743791893"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/100\n",
      "total loss: 0.4717535376548767\n",
      "\tBatch 1/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.4717535376548767\n",
      "Epoch loss: 0.4717535376548767\n",
      "\tBatch 1/400 processed in 5.81 seconds.\n",
      "total loss: 0.47471433877944946\n",
      "\tBatch 2/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.47471433877944946\n",
      "Epoch loss: 0.9464678764343262\n",
      "\tBatch 2/400 processed in 6.50 seconds.\n",
      "total loss: 0.424702912569046\n",
      "\tBatch 3/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.424702912569046\n",
      "Epoch loss: 1.3711707890033722\n",
      "\tBatch 3/400 processed in 6.32 seconds.\n",
      "total loss: 0.45115378499031067\n",
      "\tBatch 4/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.45115378499031067\n",
      "Epoch loss: 1.8223245739936829\n",
      "\tBatch 4/400 processed in 6.36 seconds.\n",
      "total loss: 0.28549519181251526\n",
      "\tBatch 5/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.28549519181251526\n",
      "Epoch loss: 2.107819765806198\n",
      "\tBatch 5/400 processed in 6.70 seconds.\n",
      "total loss: 0.2733688950538635\n",
      "\tBatch 6/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2733688950538635\n",
      "Epoch loss: 2.3811886608600616\n",
      "\tBatch 6/400 processed in 6.46 seconds.\n",
      "total loss: 0.20001044869422913\n",
      "\tBatch 7/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.20001044869422913\n",
      "Epoch loss: 2.5811991095542908\n",
      "\tBatch 7/400 processed in 6.43 seconds.\n",
      "total loss: 0.22180064022541046\n",
      "\tBatch 8/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22180064022541046\n",
      "Epoch loss: 2.8029997497797012\n",
      "\tBatch 8/400 processed in 6.72 seconds.\n",
      "total loss: 0.24183289706707\n",
      "\tBatch 9/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24183289706707\n",
      "Epoch loss: 3.0448326468467712\n",
      "\tBatch 9/400 processed in 6.57 seconds.\n",
      "total loss: 0.23357324302196503\n",
      "\tBatch 10/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23357324302196503\n",
      "Epoch loss: 3.2784058898687363\n",
      "\tBatch 10/400 processed in 6.64 seconds.\n",
      "total loss: 0.21704231202602386\n",
      "\tBatch 11/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21704231202602386\n",
      "Epoch loss: 3.49544820189476\n",
      "\tBatch 11/400 processed in 6.85 seconds.\n",
      "total loss: 0.2267192006111145\n",
      "\tBatch 12/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2267192006111145\n",
      "Epoch loss: 3.7221674025058746\n",
      "\tBatch 12/400 processed in 6.60 seconds.\n",
      "total loss: 0.20651671290397644\n",
      "\tBatch 13/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.20651671290397644\n",
      "Epoch loss: 3.928684115409851\n",
      "\tBatch 13/400 processed in 6.50 seconds.\n",
      "total loss: 0.21329084038734436\n",
      "\tBatch 14/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21329084038734436\n",
      "Epoch loss: 4.141974955797195\n",
      "\tBatch 14/400 processed in 6.63 seconds.\n",
      "total loss: 0.21102039515972137\n",
      "\tBatch 15/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21102039515972137\n",
      "Epoch loss: 4.352995350956917\n",
      "\tBatch 15/400 processed in 6.82 seconds.\n",
      "total loss: 0.24080650508403778\n",
      "\tBatch 16/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24080650508403778\n",
      "Epoch loss: 4.593801856040955\n",
      "\tBatch 16/400 processed in 6.56 seconds.\n",
      "total loss: 0.23104675114154816\n",
      "\tBatch 17/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23104675114154816\n",
      "Epoch loss: 4.824848607182503\n",
      "\tBatch 17/400 processed in 6.55 seconds.\n",
      "total loss: 0.22147807478904724\n",
      "\tBatch 18/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22147807478904724\n",
      "Epoch loss: 5.04632668197155\n",
      "\tBatch 18/400 processed in 6.61 seconds.\n",
      "total loss: 0.24233902990818024\n",
      "\tBatch 19/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24233902990818024\n",
      "Epoch loss: 5.28866571187973\n",
      "\tBatch 19/400 processed in 7.06 seconds.\n",
      "total loss: 0.2156817764043808\n",
      "\tBatch 20/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2156817764043808\n",
      "Epoch loss: 5.504347488284111\n",
      "\tBatch 20/400 processed in 6.66 seconds.\n",
      "total loss: 0.2364504039287567\n",
      "\tBatch 21/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2364504039287567\n",
      "Epoch loss: 5.740797892212868\n",
      "\tBatch 21/400 processed in 6.65 seconds.\n",
      "total loss: 0.2170247733592987\n",
      "\tBatch 22/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2170247733592987\n",
      "Epoch loss: 5.957822665572166\n",
      "\tBatch 22/400 processed in 6.69 seconds.\n",
      "total loss: 0.23666785657405853\n",
      "\tBatch 23/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23666785657405853\n",
      "Epoch loss: 6.194490522146225\n",
      "\tBatch 23/400 processed in 7.04 seconds.\n",
      "total loss: 0.2212841808795929\n",
      "\tBatch 24/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2212841808795929\n",
      "Epoch loss: 6.415774703025818\n",
      "\tBatch 24/400 processed in 6.68 seconds.\n",
      "total loss: 0.23265716433525085\n",
      "\tBatch 25/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23265716433525085\n",
      "Epoch loss: 6.648431867361069\n",
      "\tBatch 25/400 processed in 6.62 seconds.\n",
      "total loss: 0.22462375462055206\n",
      "\tBatch 26/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22462375462055206\n",
      "Epoch loss: 6.873055621981621\n",
      "\tBatch 26/400 processed in 6.66 seconds.\n",
      "total loss: 0.21052178740501404\n",
      "\tBatch 27/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21052178740501404\n",
      "Epoch loss: 7.083577409386635\n",
      "\tBatch 27/400 processed in 6.65 seconds.\n",
      "total loss: 0.21827933192253113\n",
      "\tBatch 28/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21827933192253113\n",
      "Epoch loss: 7.301856741309166\n",
      "\tBatch 28/400 processed in 6.81 seconds.\n",
      "total loss: 0.22476573288440704\n",
      "\tBatch 29/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22476573288440704\n",
      "Epoch loss: 7.526622474193573\n",
      "\tBatch 29/400 processed in 6.97 seconds.\n",
      "total loss: 0.2067001610994339\n",
      "\tBatch 30/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2067001610994339\n",
      "Epoch loss: 7.733322635293007\n",
      "\tBatch 30/400 processed in 6.68 seconds.\n",
      "total loss: 0.20716391503810883\n",
      "\tBatch 31/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.20716391503810883\n",
      "Epoch loss: 7.940486550331116\n",
      "\tBatch 31/400 processed in 6.63 seconds.\n",
      "total loss: 0.19928111135959625\n",
      "\tBatch 32/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.19928111135959625\n",
      "Epoch loss: 8.139767661690712\n",
      "\tBatch 32/400 processed in 6.71 seconds.\n",
      "total loss: 0.22206354141235352\n",
      "\tBatch 33/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22206354141235352\n",
      "Epoch loss: 8.361831203103065\n",
      "\tBatch 33/400 processed in 6.62 seconds.\n",
      "total loss: 0.22887055575847626\n",
      "\tBatch 34/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22887055575847626\n",
      "Epoch loss: 8.590701758861542\n",
      "\tBatch 34/400 processed in 6.65 seconds.\n",
      "total loss: 0.21980589628219604\n",
      "\tBatch 35/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21980589628219604\n",
      "Epoch loss: 8.810507655143738\n",
      "\tBatch 35/400 processed in 6.92 seconds.\n",
      "total loss: 0.23187659680843353\n",
      "\tBatch 36/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23187659680843353\n",
      "Epoch loss: 9.042384251952171\n",
      "\tBatch 36/400 processed in 6.65 seconds.\n",
      "total loss: 0.24256224930286407\n",
      "\tBatch 37/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24256224930286407\n",
      "Epoch loss: 9.284946501255035\n",
      "\tBatch 37/400 processed in 6.73 seconds.\n",
      "total loss: 0.2159426510334015\n",
      "\tBatch 38/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2159426510334015\n",
      "Epoch loss: 9.500889152288437\n",
      "\tBatch 38/400 processed in 6.65 seconds.\n",
      "total loss: 0.22634978592395782\n",
      "\tBatch 39/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22634978592395782\n",
      "Epoch loss: 9.727238938212395\n",
      "\tBatch 39/400 processed in 6.63 seconds.\n",
      "total loss: 0.22199958562850952\n",
      "\tBatch 40/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22199958562850952\n",
      "Epoch loss: 9.949238523840904\n",
      "\tBatch 40/400 processed in 6.63 seconds.\n",
      "total loss: 0.2098996341228485\n",
      "\tBatch 41/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2098996341228485\n",
      "Epoch loss: 10.159138157963753\n",
      "\tBatch 41/400 processed in 7.11 seconds.\n",
      "total loss: 0.2303885668516159\n",
      "\tBatch 42/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2303885668516159\n",
      "Epoch loss: 10.389526724815369\n",
      "\tBatch 42/400 processed in 6.71 seconds.\n",
      "total loss: 0.2500535249710083\n",
      "\tBatch 43/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2500535249710083\n",
      "Epoch loss: 10.639580249786377\n",
      "\tBatch 43/400 processed in 6.54 seconds.\n",
      "total loss: 0.21898998320102692\n",
      "\tBatch 44/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21898998320102692\n",
      "Epoch loss: 10.858570232987404\n",
      "\tBatch 44/400 processed in 6.70 seconds.\n",
      "total loss: 0.22160673141479492\n",
      "\tBatch 45/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22160673141479492\n",
      "Epoch loss: 11.080176964402199\n",
      "\tBatch 45/400 processed in 6.54 seconds.\n",
      "total loss: 0.2373271882534027\n",
      "\tBatch 46/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2373271882534027\n",
      "Epoch loss: 11.317504152655602\n",
      "\tBatch 46/400 processed in 6.61 seconds.\n",
      "total loss: 0.22425593435764313\n",
      "\tBatch 47/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22425593435764313\n",
      "Epoch loss: 11.541760087013245\n",
      "\tBatch 47/400 processed in 6.50 seconds.\n",
      "total loss: 0.2266402393579483\n",
      "\tBatch 48/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2266402393579483\n",
      "Epoch loss: 11.768400326371193\n",
      "\tBatch 48/400 processed in 6.62 seconds.\n",
      "total loss: 0.2017415314912796\n",
      "\tBatch 49/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2017415314912796\n",
      "Epoch loss: 11.970141857862473\n",
      "\tBatch 49/400 processed in 6.87 seconds.\n",
      "total loss: 0.19911570847034454\n",
      "\tBatch 50/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.19911570847034454\n",
      "Epoch loss: 12.169257566332817\n",
      "\tBatch 50/400 processed in 6.78 seconds.\n",
      "total loss: 0.19286373257637024\n",
      "\tBatch 51/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.19286373257637024\n",
      "Epoch loss: 12.362121298909187\n",
      "\tBatch 51/400 processed in 6.66 seconds.\n",
      "total loss: 0.2182944118976593\n",
      "\tBatch 52/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2182944118976593\n",
      "Epoch loss: 12.580415710806847\n",
      "\tBatch 52/400 processed in 6.67 seconds.\n",
      "total loss: 0.22903130948543549\n",
      "\tBatch 53/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22903130948543549\n",
      "Epoch loss: 12.809447020292282\n",
      "\tBatch 53/400 processed in 6.59 seconds.\n",
      "total loss: 0.21650321781635284\n",
      "\tBatch 54/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21650321781635284\n",
      "Epoch loss: 13.025950238108635\n",
      "\tBatch 54/400 processed in 6.58 seconds.\n",
      "total loss: 0.22234447300434113\n",
      "\tBatch 55/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22234447300434113\n",
      "Epoch loss: 13.248294711112976\n",
      "\tBatch 55/400 processed in 6.53 seconds.\n",
      "total loss: 0.23181244730949402\n",
      "\tBatch 56/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23181244730949402\n",
      "Epoch loss: 13.48010715842247\n",
      "\tBatch 56/400 processed in 6.53 seconds.\n",
      "total loss: 0.21502430737018585\n",
      "\tBatch 57/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21502430737018585\n",
      "Epoch loss: 13.695131465792656\n",
      "\tBatch 57/400 processed in 6.48 seconds.\n",
      "total loss: 0.22661355137825012\n",
      "\tBatch 58/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22661355137825012\n",
      "Epoch loss: 13.921745017170906\n",
      "\tBatch 58/400 processed in 6.88 seconds.\n",
      "total loss: 0.21082112193107605\n",
      "\tBatch 59/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21082112193107605\n",
      "Epoch loss: 14.132566139101982\n",
      "\tBatch 59/400 processed in 6.64 seconds.\n",
      "total loss: 0.21174243092536926\n",
      "\tBatch 60/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21174243092536926\n",
      "Epoch loss: 14.344308570027351\n",
      "\tBatch 60/400 processed in 6.53 seconds.\n",
      "total loss: 0.21745486557483673\n",
      "\tBatch 61/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21745486557483673\n",
      "Epoch loss: 14.561763435602188\n",
      "\tBatch 61/400 processed in 6.47 seconds.\n",
      "total loss: 0.2547551989555359\n",
      "\tBatch 62/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2547551989555359\n",
      "Epoch loss: 14.816518634557724\n",
      "\tBatch 62/400 processed in 6.53 seconds.\n",
      "total loss: 0.2172655612230301\n",
      "\tBatch 63/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2172655612230301\n",
      "Epoch loss: 15.033784195780754\n",
      "\tBatch 63/400 processed in 6.50 seconds.\n",
      "total loss: 0.22506411373615265\n",
      "\tBatch 64/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22506411373615265\n",
      "Epoch loss: 15.258848309516907\n",
      "\tBatch 64/400 processed in 6.66 seconds.\n",
      "total loss: 0.21338286995887756\n",
      "\tBatch 65/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21338286995887756\n",
      "Epoch loss: 15.472231179475784\n",
      "\tBatch 65/400 processed in 6.53 seconds.\n",
      "total loss: 0.2331112176179886\n",
      "\tBatch 66/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2331112176179886\n",
      "Epoch loss: 15.705342397093773\n",
      "\tBatch 66/400 processed in 6.54 seconds.\n",
      "total loss: 0.23483102023601532\n",
      "\tBatch 67/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23483102023601532\n",
      "Epoch loss: 15.940173417329788\n",
      "\tBatch 67/400 processed in 6.88 seconds.\n",
      "total loss: 0.21837857365608215\n",
      "\tBatch 68/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21837857365608215\n",
      "Epoch loss: 16.15855199098587\n",
      "\tBatch 68/400 processed in 6.68 seconds.\n",
      "total loss: 0.19474636018276215\n",
      "\tBatch 69/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.19474636018276215\n",
      "Epoch loss: 16.353298351168633\n",
      "\tBatch 69/400 processed in 6.47 seconds.\n",
      "total loss: 0.24378924071788788\n",
      "\tBatch 70/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24378924071788788\n",
      "Epoch loss: 16.59708759188652\n",
      "\tBatch 70/400 processed in 6.52 seconds.\n",
      "total loss: 0.2089937925338745\n",
      "\tBatch 71/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2089937925338745\n",
      "Epoch loss: 16.806081384420395\n",
      "\tBatch 71/400 processed in 6.50 seconds.\n",
      "total loss: 0.20123815536499023\n",
      "\tBatch 72/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.20123815536499023\n",
      "Epoch loss: 17.007319539785385\n",
      "\tBatch 72/400 processed in 6.59 seconds.\n",
      "total loss: 0.21445514261722565\n",
      "\tBatch 73/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21445514261722565\n",
      "Epoch loss: 17.22177468240261\n",
      "\tBatch 73/400 processed in 6.69 seconds.\n",
      "total loss: 0.2160746157169342\n",
      "\tBatch 74/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2160746157169342\n",
      "Epoch loss: 17.437849298119545\n",
      "\tBatch 74/400 processed in 6.80 seconds.\n",
      "total loss: 0.2271411120891571\n",
      "\tBatch 75/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2271411120891571\n",
      "Epoch loss: 17.664990410208702\n",
      "\tBatch 75/400 processed in 6.63 seconds.\n",
      "total loss: 0.19456222653388977\n",
      "\tBatch 76/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.19456222653388977\n",
      "Epoch loss: 17.859552636742592\n",
      "\tBatch 76/400 processed in 6.61 seconds.\n",
      "total loss: 0.23578324913978577\n",
      "\tBatch 77/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23578324913978577\n",
      "Epoch loss: 18.095335885882378\n",
      "\tBatch 77/400 processed in 6.77 seconds.\n",
      "total loss: 0.2090786248445511\n",
      "\tBatch 78/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2090786248445511\n",
      "Epoch loss: 18.30441451072693\n",
      "\tBatch 78/400 processed in 6.84 seconds.\n",
      "total loss: 0.21584051847457886\n",
      "\tBatch 79/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21584051847457886\n",
      "Epoch loss: 18.520255029201508\n",
      "\tBatch 79/400 processed in 6.51 seconds.\n",
      "total loss: 0.24152377247810364\n",
      "\tBatch 80/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24152377247810364\n",
      "Epoch loss: 18.76177880167961\n",
      "\tBatch 80/400 processed in 6.59 seconds.\n",
      "total loss: 0.23604407906532288\n",
      "\tBatch 81/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23604407906532288\n",
      "Epoch loss: 18.997822880744934\n",
      "\tBatch 81/400 processed in 6.48 seconds.\n",
      "total loss: 0.2153473049402237\n",
      "\tBatch 82/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2153473049402237\n",
      "Epoch loss: 19.213170185685158\n",
      "\tBatch 82/400 processed in 6.66 seconds.\n",
      "total loss: 0.2104104608297348\n",
      "\tBatch 83/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2104104608297348\n",
      "Epoch loss: 19.423580646514893\n",
      "\tBatch 83/400 processed in 6.52 seconds.\n",
      "total loss: 0.23438400030136108\n",
      "\tBatch 84/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23438400030136108\n",
      "Epoch loss: 19.657964646816254\n",
      "\tBatch 84/400 processed in 6.55 seconds.\n",
      "total loss: 0.229737788438797\n",
      "\tBatch 85/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.229737788438797\n",
      "Epoch loss: 19.88770243525505\n",
      "\tBatch 85/400 processed in 6.52 seconds.\n",
      "total loss: 0.24892251193523407\n",
      "\tBatch 86/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24892251193523407\n",
      "Epoch loss: 20.136624947190285\n",
      "\tBatch 86/400 processed in 6.68 seconds.\n",
      "total loss: 0.2233358770608902\n",
      "\tBatch 87/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2233358770608902\n",
      "Epoch loss: 20.359960824251175\n",
      "\tBatch 87/400 processed in 6.53 seconds.\n",
      "total loss: 0.2221153974533081\n",
      "\tBatch 88/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2221153974533081\n",
      "Epoch loss: 20.582076221704483\n",
      "\tBatch 88/400 processed in 6.56 seconds.\n",
      "total loss: 0.20490996539592743\n",
      "\tBatch 89/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.20490996539592743\n",
      "Epoch loss: 20.78698618710041\n",
      "\tBatch 89/400 processed in 6.50 seconds.\n",
      "total loss: 0.22115039825439453\n",
      "\tBatch 90/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22115039825439453\n",
      "Epoch loss: 21.008136585354805\n",
      "\tBatch 90/400 processed in 6.59 seconds.\n",
      "total loss: 0.18992634117603302\n",
      "\tBatch 91/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.18992634117603302\n",
      "Epoch loss: 21.198062926530838\n",
      "\tBatch 91/400 processed in 7.17 seconds.\n",
      "total loss: 0.23235218226909637\n",
      "\tBatch 92/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23235218226909637\n",
      "Epoch loss: 21.430415108799934\n",
      "\tBatch 92/400 processed in 6.73 seconds.\n",
      "total loss: 0.20505645871162415\n",
      "\tBatch 93/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.20505645871162415\n",
      "Epoch loss: 21.63547156751156\n",
      "\tBatch 93/400 processed in 6.58 seconds.\n",
      "total loss: 0.21600447595119476\n",
      "\tBatch 94/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21600447595119476\n",
      "Epoch loss: 21.851476043462753\n",
      "\tBatch 94/400 processed in 6.70 seconds.\n",
      "total loss: 0.22769562900066376\n",
      "\tBatch 95/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22769562900066376\n",
      "Epoch loss: 22.079171672463417\n",
      "\tBatch 95/400 processed in 6.71 seconds.\n",
      "total loss: 0.2101769596338272\n",
      "\tBatch 96/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2101769596338272\n",
      "Epoch loss: 22.289348632097244\n",
      "\tBatch 96/400 processed in 6.56 seconds.\n",
      "total loss: 0.22123408317565918\n",
      "\tBatch 97/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22123408317565918\n",
      "Epoch loss: 22.510582715272903\n",
      "\tBatch 97/400 processed in 6.64 seconds.\n",
      "total loss: 0.2201463282108307\n",
      "\tBatch 98/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2201463282108307\n",
      "Epoch loss: 22.730729043483734\n",
      "\tBatch 98/400 processed in 6.56 seconds.\n",
      "total loss: 0.20973697304725647\n",
      "\tBatch 99/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.20973697304725647\n",
      "Epoch loss: 22.94046601653099\n",
      "\tBatch 99/400 processed in 6.69 seconds.\n",
      "total loss: 0.19055919349193573\n",
      "\tBatch 100/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.19055919349193573\n",
      "Epoch loss: 23.131025210022926\n",
      "\tBatch 100/400 processed in 6.68 seconds.\n",
      "total loss: 0.2083914875984192\n",
      "\tBatch 101/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2083914875984192\n",
      "Epoch loss: 23.339416697621346\n",
      "\tBatch 101/400 processed in 6.61 seconds.\n",
      "total loss: 0.1967550814151764\n",
      "\tBatch 102/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.1967550814151764\n",
      "Epoch loss: 23.536171779036522\n",
      "\tBatch 102/400 processed in 6.60 seconds.\n",
      "total loss: 0.22051097452640533\n",
      "\tBatch 103/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22051097452640533\n",
      "Epoch loss: 23.756682753562927\n",
      "\tBatch 103/400 processed in 6.59 seconds.\n",
      "total loss: 0.2212313860654831\n",
      "\tBatch 104/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2212313860654831\n",
      "Epoch loss: 23.97791413962841\n",
      "\tBatch 104/400 processed in 6.76 seconds.\n",
      "total loss: 0.24105437099933624\n",
      "\tBatch 105/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24105437099933624\n",
      "Epoch loss: 24.218968510627747\n",
      "\tBatch 105/400 processed in 7.03 seconds.\n",
      "total loss: 0.2207745760679245\n",
      "\tBatch 106/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2207745760679245\n",
      "Epoch loss: 24.43974308669567\n",
      "\tBatch 106/400 processed in 6.67 seconds.\n",
      "total loss: 0.23105785250663757\n",
      "\tBatch 107/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23105785250663757\n",
      "Epoch loss: 24.67080093920231\n",
      "\tBatch 107/400 processed in 6.54 seconds.\n",
      "total loss: 0.22151705622673035\n",
      "\tBatch 108/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22151705622673035\n",
      "Epoch loss: 24.89231799542904\n",
      "\tBatch 108/400 processed in 6.61 seconds.\n",
      "total loss: 0.23933295905590057\n",
      "\tBatch 109/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23933295905590057\n",
      "Epoch loss: 25.13165095448494\n",
      "\tBatch 109/400 processed in 6.60 seconds.\n",
      "total loss: 0.22524985671043396\n",
      "\tBatch 110/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22524985671043396\n",
      "Epoch loss: 25.356900811195374\n",
      "\tBatch 110/400 processed in 6.64 seconds.\n",
      "total loss: 0.21551765501499176\n",
      "\tBatch 111/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21551765501499176\n",
      "Epoch loss: 25.572418466210365\n",
      "\tBatch 111/400 processed in 6.60 seconds.\n",
      "total loss: 0.2225300520658493\n",
      "\tBatch 112/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2225300520658493\n",
      "Epoch loss: 25.794948518276215\n",
      "\tBatch 112/400 processed in 6.58 seconds.\n",
      "total loss: nan\n",
      "\tBatch 113/400, Forward pass done, starting backward pass.\n",
      "Warning: NaN detected in total_loss at batch 113, skipping backward pass.\n",
      "total loss item: nan\n",
      "NaN detected, not adding to epoch_loss at batch 113\n",
      "Epoch loss: 25.794948518276215\n",
      "\tBatch 113/400 processed in 4.35 seconds.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 6.12 MiB is free. Process 3833874 has 79.13 GiB memory in use. Of the allocated memory 74.03 GiB is allocated by PyTorch, and 4.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m loss_weights \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreconstruction\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmi_penalty\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.0\u001b[39m}  \u001b[38;5;66;03m# Adjust the weights as needed\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 39\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_loader, device, num_epochs, loss_weights, accumulation_steps, checkpoint_path)\u001b[0m\n\u001b[1;32m     36\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():  \u001b[38;5;66;03m# Enable automatic mixed precision\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     reconstructed_x, f_x, sx_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# reconstruction_loss = nn.functional.l1_loss(reconstructed_x, data)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     reconstruction_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmse_loss(reconstructed_x, data)\n",
      "File \u001b[0;32m~/miniconda3/envs/parthaenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/parthaenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 44\u001b[0m, in \u001b[0;36mEchoModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m x_T \u001b[38;5;241m=\u001b[39m sqrt_alpha_T \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m+\u001b[39m sqrt_one_minus_alpha_T \u001b[38;5;241m*\u001b[39m z\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Perform the reconstruction process using Algorithm 2\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m reconstructed_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_T\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msx_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reconstructed_x, f_x, sx_matrix\n",
      "Cell \u001b[0;32mIn[7], line 55\u001b[0m, in \u001b[0;36mEchoModel.reconstruct\u001b[0;34m(self, x_t, z, f_x, sx_matrix)\u001b[0m\n\u001b[1;32m     52\u001b[0m sqrt_one_minus_alpha_s \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha[s])\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Estimate the original image using the decoder\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m x_0_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Calculate the estimated noise using Eq. (3)\u001b[39;00m\n\u001b[1;32m     58\u001b[0m z_hat \u001b[38;5;241m=\u001b[39m (x_s \u001b[38;5;241m-\u001b[39m sqrt_alpha_s \u001b[38;5;241m*\u001b[39m x_0_hat) \u001b[38;5;241m/\u001b[39m sqrt_one_minus_alpha_s\n",
      "File \u001b[0;32m~/miniconda3/envs/parthaenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/parthaenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 41\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     39\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeconv2(x))\n\u001b[1;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeconv3(x))\n\u001b[0;32m---> 41\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeconv4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeconv5(x))\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 6.12 MiB is free. Process 3833874 has 79.13 GiB memory in use. Of the allocated memory 74.03 GiB is allocated by PyTorch, and 4.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Define the input shape, latent dimensions, and output shape\n",
    "input_shape = (1, 32, 32)  # Shape for grayscale CIFAR-10 (1 channel, 32x32 images)\n",
    "latent_dims = [32, 64, 128, 256, 512]  # Updated latent dimensions\n",
    "output_shape = (1, 32, 32)  # Shape for grayscale CIFAR-10\n",
    "\n",
    "# Create an instance of the EchoModel\n",
    "model = EchoModel(input_shape, latent_dims, output_shape).to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Define the number of epochs and loss weights\n",
    "num_epochs = 100\n",
    "loss_weights = {'reconstruction': 1.0, 'mi_penalty': 0.0}  # Adjust the weights as needed\n",
    "\n",
    "# Train the model\n",
    "trained_model = train(model, optimizer, train_loader, device, num_epochs, loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qmfrva2uRuvu",
   "metadata": {
    "id": "qmfrva2uRuvu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
