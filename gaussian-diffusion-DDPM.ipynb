{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d14b0ef-fcc9-4652-a5db-9aba8a985cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary PyTorch libraries\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    " \n",
    "# Additional libraries for visualization and utilities\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a0294-5aff-48fd-a9b3-f0e43b747b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    \"\"\"Selects the best available device for PyTorch computations.\n",
    "\n",
    "    Returns:\n",
    "        torch.device: The selected device.\n",
    "    \"\"\"\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "device = get_device()\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fcec4a-bc36-45f4-a902-cefd2ee6a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Grayscale\n",
    "\n",
    "# Define transformations: Convert to grayscale, resize if needed, and normalize the data\n",
    "transform = Compose([\n",
    "    Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    ToTensor(),\n",
    "    Normalize((0.5,), (0.5,))  # Normalize with single channel\n",
    "])\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Splitting dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "print(\"Data loaders created for training and validation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6385d4eb-499a-42e5-9808-f3ea7f17b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestep_embedding(timesteps, embedding_dim):\n",
    "    \"\"\"\n",
    "    This matches the implementation in Denoising Diffusion Probabilistic Models:\n",
    "    From Fairseq.\n",
    "    Build sinusoidal embeddings.\n",
    "    This matches the implementation in tensor2tensor, but differs slightly\n",
    "    from the description in Section 3.5 of \"Attention Is All You Need\".\n",
    "    \"\"\"\n",
    "    assert len(timesteps.shape) == 1\n",
    "\n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = math.log(10000) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
    "    emb = emb.to(device=timesteps.device)\n",
    "    emb = timesteps.float()[:, None] * emb[None, :]\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "    if embedding_dim % 2 == 1:  # zero pad\n",
    "        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13fe561-4f2a-4021-9a64-812e09280c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dims, output_shape, timestep_dim=128):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.latent_dims = latent_dims\n",
    "        self.output_shape = output_shape\n",
    "        self.timestep_dim = timestep_dim\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, latent_dims[0], kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(latent_dims[0], latent_dims[1], kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(latent_dims[1], latent_dims[2], kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(latent_dims[2], latent_dims[3], kernel_size=3, stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv2d(latent_dims[3], latent_dims[4], kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.timestep_mlp = nn.Sequential(\n",
    "            nn.Linear(timestep_dim, latent_dims[4]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_dims[4], latent_dims[4]),\n",
    "        )\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose2d(latent_dims[4] * 2, latent_dims[3], kernel_size=3, stride=1, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(latent_dims[3], latent_dims[2], kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(latent_dims[2], latent_dims[1], kernel_size=3, stride=1, padding=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(latent_dims[1], latent_dims[0], kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv5 = nn.ConvTranspose2d(latent_dims[0], output_shape[0], kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        timestep_emb = get_timestep_embedding(t, self.timestep_dim)\n",
    "        timestep_emb = self.timestep_mlp(timestep_emb)\n",
    "\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = torch.relu(self.conv5(x))\n",
    "\n",
    "        x = torch.cat([x, timestep_emb[:, :, None, None].repeat(1, 1, x.shape[2], x.shape[3])], dim=1)\n",
    "\n",
    "        x = torch.relu(self.deconv1(x))\n",
    "        x = torch.relu(self.deconv2(x))\n",
    "        x = torch.relu(self.deconv3(x))\n",
    "        x = torch.relu(self.deconv4(x))\n",
    "        x = torch.sigmoid(self.deconv5(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa4dc1c-8d3b-4e0c-80e8-57aab342fa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self, input_shape, latent_dims, output_shape, T=1000, batch_size=100):\n",
    "        super(DiffusionModel, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.latent_dims = latent_dims\n",
    "        self.output_shape = output_shape\n",
    "        self.T = T\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.decoder = Decoder(latent_dims, output_shape)\n",
    "\n",
    "        # Define the noise schedule\n",
    "        self.alpha = self.create_noise_schedule(T)\n",
    "\n",
    "    def create_noise_schedule(self, T):\n",
    "        beta_start = 0.0001\n",
    "        beta_end = 0.02\n",
    "        betas = torch.linspace(beta_start, beta_end, T)\n",
    "        alphas = 1 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        return alphas_cumprod\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Calculate the gaussian noise tensor\n",
    "        epsilon = torch.randn(self.batch_size,1, 32, 32).to(device)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        #sample a timestep t\n",
    "        t = np.random.randint(0, self.T)\n",
    "        # Retrieve noise scheduler alpha_T\n",
    "        alpha_t = self.alpha[t]\n",
    "\n",
    "        # Calculate square root alphas\n",
    "        sqrt_alpha_t = torch.sqrt(alpha_t)\n",
    "        sqrt_one_minus_alpha_t = torch.sqrt(1 - alpha_t)\n",
    "        \n",
    "        # Perform the weighted sum\n",
    "        x_t = sqrt_alpha_t * x + sqrt_one_minus_alpha_t * epsilon\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        #Calculate the timestep tensor\n",
    "        t = torch.tensor([t] * x_t.size(0), dtype=torch.long).to(x_t.device)\n",
    "\n",
    "        # Perform the reconstruction process using Algorithm 2\n",
    "        estimated_epsilon = self.decoder(x_t,t)\n",
    "        torch.cuda.empty_cache()\n",
    "        return epsilon, estimated_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184eed0d-0113-41b0-a7f4-6cb2a7a6ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # Importing time to log the duration\n",
    "\n",
    "def validate(model, val_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient computation during validation\n",
    "        for data, _ in val_loader:\n",
    "            data = data.to(device)\n",
    "            epsilon, estimated_epsilon = model(data)\n",
    "            reconstruction_loss = nn.functional.mse_loss(epsilon, estimated_epsilon)\n",
    "            total_val_loss += reconstruction_loss.item()  # Accumulate the validation loss\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)  # Calculate average loss\n",
    "    return avg_val_loss\n",
    "\n",
    "def train(model, optimizer, train_loader, device, num_epochs, accumulation_steps=2, checkpoint_path=\"checkpoint.pth\"):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        epoch_start_time = time.time()  # Time tracking for the epoch\n",
    "\n",
    "        print(f\"Starting epoch {epoch+1}/{num_epochs}\")\n",
    "        for batch_idx, (data, _) in enumerate(train_loader):\n",
    "            batch_start_time = time.time()  # Time tracking for the batch\n",
    "            data = data.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            epsilon, estimated_epsilon = model(data)\n",
    "            total_loss = nn.functional.mse_loss(estimated_epsilon, epsilon)\n",
    "            print(f\"total loss: {total_loss}\")\n",
    "\n",
    "            # Log before the backward pass\n",
    "            print(f\"\\tBatch {batch_idx+1}/{len(train_loader)}, Forward pass done, starting backward pass.\")\n",
    "\n",
    "            # Backward pass\n",
    "            if not torch.isnan(total_loss).any():\n",
    "                total_loss.backward()\n",
    "            else:\n",
    "                print(f\"Warning: NaN detected in total_loss at batch {batch_idx+1}, skipping backward pass.\")\n",
    "\n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                optimizer.step()  # Only step the optimizer every `accumulation_steps`\n",
    "                optimizer.zero_grad()  # Reset gradients only after accumulation\n",
    "\n",
    "            print(f\"total loss item: {total_loss.item()}\")\n",
    "\n",
    "            # Safe-guarding against NaN for epoch_loss\n",
    "            if not torch.isnan(total_loss).any():\n",
    "                epoch_loss += total_loss.item()\n",
    "            else:\n",
    "                print(f\"NaN detected, not adding to epoch_loss at batch {batch_idx+1}\")\n",
    "            print(f\"Epoch loss: {epoch_loss}\")\n",
    "\n",
    "            # Log after a batch is processed\n",
    "            print(f\"\\tBatch {batch_idx+1}/{len(train_loader)} processed in {time.time() - batch_start_time:.2f} seconds.\")\n",
    "\n",
    "        # Average loss after training for an epoch\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] completed in {time.time() - epoch_start_time:.2f} seconds, Avg Loss: {avg_loss}\")\n",
    "\n",
    "        # Validation phase\n",
    "        avg_val_loss = validate(model, val_loader, device)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] validation completed, Avg Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5a1b2a-c0a9-4d56-bf27-dbaba616661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input shape, latent dimensions, and output shape\n",
    "input_shape = (1, 32, 32)  # Shape for grayscale CIFAR-10 (1 channel, 32x32 images)\n",
    "latent_dims = [32, 64, 128, 256, 512]  # Updated latent dimensions\n",
    "output_shape = (1, 32, 32)  # Shape for grayscale CIFAR-10\n",
    "\n",
    "# Create an instance of the EchoModel\n",
    "model = DiffusionModel(input_shape, latent_dims, output_shape).to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Define the number of epochs and loss weights\n",
    "num_epochs = 100\n",
    "\n",
    "# Train the model\n",
    "trained_model = train(model, optimizer, train_loader, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033a0ca4-dd5f-4124-9b87-e3658ce6975a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
