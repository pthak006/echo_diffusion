{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec619f54-5dc4-4292-8727-066008d1e8ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ec619f54-5dc4-4292-8727-066008d1e8ed",
    "outputId": "5b117faa-8a74-4912-bc05-093f2d4ba270"
   },
   "outputs": [],
   "source": [
    "# Import necessary PyTorch libraries\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Additional libraries for visualization and utilities\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8503d243-9f17-4117-a5bd-4e22f31905ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    \"\"\"Selects the best available device for PyTorch computations.\n",
    "\n",
    "    Returns:\n",
    "        torch.device: The selected device.\n",
    "    \"\"\"\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "device = get_device()\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a353550-e380-4555-8519-25b35c5b1654",
   "metadata": {
    "id": "8a353550-e380-4555-8519-25b35c5b1654"
   },
   "outputs": [],
   "source": [
    "# Import the adapted Echo noise functions\n",
    "from echo import echo_sample,echo_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "306efd2f-3f3b-418a-ab09-bf1173151f80",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "306efd2f-3f3b-418a-ab09-bf1173151f80",
    "outputId": "807d84ee-b69b-4434-e510-e00e6a21255c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Data loaders created for training and validation.\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Grayscale\n",
    "\n",
    "# Define transformations: Convert to grayscale, resize if needed, and normalize the data\n",
    "transform = Compose([\n",
    "    Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    ToTensor(),\n",
    "    Normalize((0.5,), (0.5,))  # Normalize with single channel\n",
    "])\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Splitting dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "print(\"Data loaders created for training and validation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5860ada1-b1cb-45f5-accd-933906b45160",
   "metadata": {
    "id": "5860ada1-b1cb-45f5-accd-933906b45160"
   },
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch import Unet\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_shape, latent_dims):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.latent_dims = latent_dims\n",
    "\n",
    "        self.unet = Unet(\n",
    "            encoder_name=\"resnet18\",\n",
    "            encoder_depth=3,\n",
    "            encoder_weights=None,\n",
    "            decoder_use_batchnorm=True,\n",
    "            decoder_channels=(latent_dims[2], latent_dims[1], latent_dims[0]),\n",
    "            decoder_attention_type=None,\n",
    "            in_channels=input_shape[0],\n",
    "            classes=latent_dims[-1],\n",
    "            activation=None,\n",
    "        )\n",
    "\n",
    "        # Output layers\n",
    "        self.out_mean = nn.Conv2d(latent_dims[-1], input_shape[0], kernel_size=1)\n",
    "        self.out_log_var = nn.Conv2d(latent_dims[-1], input_shape[0], kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.unet(x)\n",
    "        f_x = torch.tanh(self.out_mean(x))\n",
    "        log_var = torch.sigmoid(self.out_log_var(x))\n",
    "        return f_x, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "TenNg_Ywlv3n",
   "metadata": {
    "id": "TenNg_Ywlv3n"
   },
   "outputs": [],
   "source": [
    "def get_timestep_embedding(timesteps, embedding_dim):\n",
    "    \"\"\"\n",
    "    This matches the implementation in Denoising Diffusion Probabilistic Models:\n",
    "    From Fairseq.\n",
    "    Build sinusoidal embeddings.\n",
    "    This matches the implementation in tensor2tensor, but differs slightly\n",
    "    from the description in Section 3.5 of \"Attention Is All You Need\".\n",
    "    \"\"\"\n",
    "    assert len(timesteps.shape) == 1\n",
    "\n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = math.log(10000) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
    "    emb = emb.to(device=timesteps.device)\n",
    "    emb = timesteps.float()[:, None] * emb[None, :]\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "    if embedding_dim % 2 == 1:  # zero pad\n",
    "        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f08a5ac-95ad-45ae-a36e-f96dbd3f1e99",
   "metadata": {
    "id": "1f08a5ac-95ad-45ae-a36e-f96dbd3f1e99"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dims, output_shape, timestep_dim=128):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.latent_dims = latent_dims\n",
    "        self.output_shape = output_shape\n",
    "        self.timestep_dim = timestep_dim\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, latent_dims[0], kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(latent_dims[0], latent_dims[1], kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(latent_dims[1], latent_dims[2], kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(latent_dims[2], latent_dims[3], kernel_size=3, stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv2d(latent_dims[3], latent_dims[4], kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.timestep_mlp = nn.Sequential(\n",
    "            nn.Linear(timestep_dim, latent_dims[4]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_dims[4], latent_dims[4]),\n",
    "        )\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose2d(latent_dims[4] * 2, latent_dims[3], kernel_size=3, stride=1, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(latent_dims[3], latent_dims[2], kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(latent_dims[2], latent_dims[1], kernel_size=3, stride=1, padding=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(latent_dims[1], latent_dims[0], kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv5 = nn.ConvTranspose2d(latent_dims[0], output_shape[0], kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        timestep_emb = get_timestep_embedding(t, self.timestep_dim)\n",
    "        timestep_emb = self.timestep_mlp(timestep_emb)\n",
    "\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = torch.relu(self.conv5(x))\n",
    "\n",
    "        x = torch.cat([x, timestep_emb[:, :, None, None].repeat(1, 1, x.shape[2], x.shape[3])], dim=1)\n",
    "\n",
    "        x = torch.relu(self.deconv1(x))\n",
    "        x = torch.relu(self.deconv2(x))\n",
    "        x = torch.relu(self.deconv3(x))\n",
    "        x = torch.relu(self.deconv4(x))\n",
    "        x = torch.sigmoid(self.deconv5(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a77391b7-6374-4bb7-acd9-577f51a68706",
   "metadata": {
    "id": "a77391b7-6374-4bb7-acd9-577f51a68706"
   },
   "outputs": [],
   "source": [
    "class EchoModel(nn.Module):\n",
    "    def __init__(self, input_shape, latent_dims, output_shape, T=1000, batch_size=100):\n",
    "        super(EchoModel, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.latent_dims = latent_dims\n",
    "        self.output_shape = output_shape\n",
    "        self.T = T\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.encoder = Encoder(input_shape, latent_dims)\n",
    "        self.decoder = Decoder(latent_dims, output_shape)\n",
    "\n",
    "        # Define the noise schedule\n",
    "        self.alpha = self.create_noise_schedule(T)\n",
    "\n",
    "    def create_noise_schedule(self, T):\n",
    "        alpha = torch.linspace(0.9999, 1e-5, T)\n",
    "        return alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Calculate f_x and S_x\n",
    "        f_x, sx_matrix = self.encoder(x)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        #Calculate epsilon in a detached way\n",
    "        epsilon = echo_sample((f_x, sx_matrix)).detach()\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        #Calculate echo output z \n",
    "        z = f_x + sx_matrix * epsilon\n",
    "\n",
    "        del epsilon\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Retrieve noise scheduler alpha_T\n",
    "        alpha_T = self.alpha[-1]\n",
    "\n",
    "        # Calculate square root alphas\n",
    "        sqrt_alpha_T = torch.sqrt(alpha_T)\n",
    "        sqrt_one_minus_alpha_T = torch.sqrt(1 - alpha_T)\n",
    "        \n",
    "        # Perform the weighted sum\n",
    "        x_T = sqrt_alpha_T * x + sqrt_one_minus_alpha_T * z\n",
    "\n",
    "\n",
    "        # Perform the reconstruction process using Algorithm 2\n",
    "        reconstructed_x = self.reconstruct(x_T)\n",
    "        torch.cuda.empty_cache()\n",
    "        return reconstructed_x, f_x, sx_matrix\n",
    "\n",
    "    def reconstruct(self, x_T):\n",
    "        x_s = x_T\n",
    "        x_0_hat = x_T\n",
    "        for s in range(self.T-1, 0, -1):\n",
    "            t = torch.tensor([s] * x_T.size(0), dtype=torch.long).to(x_T.device)\n",
    "            sqrt_alpha_s = torch.sqrt(self.alpha[s])\n",
    "            sqrt_one_minus_alpha_s = torch.sqrt(1 - self.alpha[s])\n",
    "\n",
    "            # Estimate the original image using the decoder\n",
    "            x_0_hat = self.decoder(x_s, t)\n",
    "\n",
    "            # Calculate the estimated noise using Eq. (3)\n",
    "            z_hat = (x_s - sqrt_alpha_s * x_0_hat) / sqrt_one_minus_alpha_s\n",
    "\n",
    "            # Calculate D(x_0_hat, s) and D(x_0_hat, s-1) using Eq. (5) and (6)\n",
    "            D_x_0_hat_s = sqrt_alpha_s * x_0_hat + sqrt_one_minus_alpha_s * z_hat\n",
    "            D_x_0_hat_s_minus_1 = torch.sqrt(self.alpha[s-1]) * x_0_hat + torch.sqrt(1 - self.alpha[s-1]) * z_hat\n",
    "\n",
    "            # Update x_s using Eq. (7)\n",
    "            x_s = x_s - D_x_0_hat_s + D_x_0_hat_s_minus_1\n",
    "\n",
    "        return x_0_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c466c51d-3b41-4e11-bdfd-39b3d10ed2c9",
   "metadata": {
    "id": "c466c51d-3b41-4e11-bdfd-39b3d10ed2c9"
   },
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import time  # Importing time to log the duration\n",
    "\n",
    "def validate(model, val_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient computation during validation\n",
    "        for data, _ in val_loader:\n",
    "            data = data.to(device)\n",
    "            reconstructed_x, f_x, sx_matrix = model(data)\n",
    "            reconstruction_loss = nn.functional.mse_loss(reconstructed_x, data)\n",
    "            total_val_loss += reconstruction_loss.item()  # Accumulate the validation loss\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)  # Calculate average loss\n",
    "    return avg_val_loss\n",
    "\n",
    "def train(model, optimizer, train_loader, device, num_epochs, loss_weights, accumulation_steps=2, checkpoint_path=\"checkpoint.pth\"):\n",
    "    model.train()\n",
    "    scaler = GradScaler()  # Initialize the gradient scaler\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        epoch_start_time = time.time()  # Time tracking for the epoch\n",
    "\n",
    "        print(f\"Starting epoch {epoch+1}/{num_epochs}\")\n",
    "        for batch_idx, (data, _) in enumerate(train_loader):\n",
    "            batch_start_time = time.time()  # Time tracking for the batch\n",
    "            data = data.to(device)\n",
    "\n",
    "            with autocast():  # Enable automatic mixed precision\n",
    "                reconstructed_x, f_x, S_x  = model(data)\n",
    "                reconstruction_loss = nn.functional.mse_loss(reconstructed_x, data)\n",
    "                mi = echo_loss(S_x)\n",
    "                mi_penalty = (1.0/mi)\n",
    "                total_loss = loss_weights['reconstruction'] * reconstruction_loss + loss_weights['mi_penalty'] * mi_penalty\n",
    "                print(f\"total loss: {total_loss}\")\n",
    "                \n",
    "            # Log before the backward pass\n",
    "            print(f\"\\tBatch {batch_idx+1}/{len(train_loader)}, Forward pass done, starting backward pass.\")\n",
    "\n",
    "            # Scale the loss, but don't call optimizer.step() yet\n",
    "            if not torch.isnan(total_loss).any():\n",
    "                scaler.scale(total_loss).backward()\n",
    "            else:\n",
    "                print(f\"Warning: NaN detected in total_loss at batch {batch_idx+1}, skipping backward pass.\")\n",
    "\n",
    "\n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                scaler.step(optimizer)  # Only step the optimizer every `accumulation_steps`\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)  # Reset gradients only after accumulation\n",
    "\n",
    "\n",
    "            # Safe-guarding against NaN for epoch_loss\n",
    "            if not torch.isnan(total_loss).any():\n",
    "                epoch_loss += total_loss.item()\n",
    "            else:\n",
    "                print(f\"NaN detected, not adding to epoch_loss at batch {batch_idx+1}\")\n",
    "            print(f\"Epoch loss: {epoch_loss}\")\n",
    "\n",
    "            # Log after a batch is processed\n",
    "            print(f\"\\tBatch {batch_idx+1}/{len(train_loader)} processed in {time.time() - batch_start_time:.2f} seconds.\")\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] completed in {time.time() - epoch_start_time:.2f} seconds, Avg Loss: {avg_loss}\")\n",
    "        \n",
    "        # Validation phase\n",
    "        avg_val_loss = validate(model, val_loader, device)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] validation completed, Avg Validation Loss: {avg_val_loss}\")\n",
    "        \n",
    "        \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "661ef76c-fb21-4fd4-b6e6-436524680219",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "661ef76c-fb21-4fd4-b6e6-436524680219",
    "outputId": "957794b4-bfe9-4884-ac9b-510743791893"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/100\n",
      "total loss: 1.2109489440917969\n",
      "\tBatch 1/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 1.2109489440917969\n",
      "\tBatch 1/400 processed in 5.01 seconds.\n",
      "total loss: 1.2469508647918701\n",
      "\tBatch 2/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 2.457899808883667\n",
      "\tBatch 2/400 processed in 5.63 seconds.\n",
      "total loss: 1.2204582691192627\n",
      "\tBatch 3/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 3.6783580780029297\n",
      "\tBatch 3/400 processed in 5.55 seconds.\n",
      "total loss: 1.2118003368377686\n",
      "\tBatch 4/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 4.890158414840698\n",
      "\tBatch 4/400 processed in 5.74 seconds.\n",
      "total loss: 1.2649998664855957\n",
      "\tBatch 5/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 6.155158281326294\n",
      "\tBatch 5/400 processed in 5.58 seconds.\n",
      "total loss: 1.2525962591171265\n",
      "\tBatch 6/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 7.40775454044342\n",
      "\tBatch 6/400 processed in 5.70 seconds.\n",
      "total loss: 1.2067641019821167\n",
      "\tBatch 7/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 8.614518642425537\n",
      "\tBatch 7/400 processed in 5.64 seconds.\n",
      "total loss: 1.17180335521698\n",
      "\tBatch 8/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 9.786321997642517\n",
      "\tBatch 8/400 processed in 5.61 seconds.\n",
      "total loss: 1.0101215839385986\n",
      "\tBatch 9/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 10.796443581581116\n",
      "\tBatch 9/400 processed in 5.59 seconds.\n",
      "total loss: 1.0052170753479004\n",
      "\tBatch 10/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 11.801660656929016\n",
      "\tBatch 10/400 processed in 5.76 seconds.\n",
      "total loss: 0.8898669481277466\n",
      "\tBatch 11/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 12.691527605056763\n",
      "\tBatch 11/400 processed in 5.62 seconds.\n",
      "total loss: 0.8706545233726501\n",
      "\tBatch 12/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 13.562182128429413\n",
      "\tBatch 12/400 processed in 5.61 seconds.\n",
      "total loss: 0.8721389770507812\n",
      "\tBatch 13/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 14.434321105480194\n",
      "\tBatch 13/400 processed in 5.64 seconds.\n",
      "total loss: 0.8856887817382812\n",
      "\tBatch 14/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 15.320009887218475\n",
      "\tBatch 14/400 processed in 5.78 seconds.\n",
      "total loss: 0.8721311092376709\n",
      "\tBatch 15/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 16.192140996456146\n",
      "\tBatch 15/400 processed in 5.61 seconds.\n",
      "total loss: 0.8907608985900879\n",
      "\tBatch 16/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 17.082901895046234\n",
      "\tBatch 16/400 processed in 5.62 seconds.\n",
      "total loss: 0.8691269159317017\n",
      "\tBatch 17/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 17.952028810977936\n",
      "\tBatch 17/400 processed in 5.60 seconds.\n",
      "total loss: 0.9108796119689941\n",
      "\tBatch 18/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 18.86290842294693\n",
      "\tBatch 18/400 processed in 5.77 seconds.\n",
      "total loss: 0.8685028553009033\n",
      "\tBatch 19/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 19.731411278247833\n",
      "\tBatch 19/400 processed in 5.65 seconds.\n",
      "total loss: 0.8833784461021423\n",
      "\tBatch 20/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 20.614789724349976\n",
      "\tBatch 20/400 processed in 5.65 seconds.\n",
      "total loss: 0.8746280670166016\n",
      "\tBatch 21/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 21.489417791366577\n",
      "\tBatch 21/400 processed in 5.63 seconds.\n",
      "total loss: 0.8767311573028564\n",
      "\tBatch 22/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 22.366148948669434\n",
      "\tBatch 22/400 processed in 5.70 seconds.\n",
      "total loss: 0.8743330240249634\n",
      "\tBatch 23/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 23.240481972694397\n",
      "\tBatch 23/400 processed in 5.83 seconds.\n",
      "total loss: 0.868589460849762\n",
      "\tBatch 24/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 24.10907143354416\n",
      "\tBatch 24/400 processed in 5.69 seconds.\n",
      "total loss: 0.8607923984527588\n",
      "\tBatch 25/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 24.969863831996918\n",
      "\tBatch 25/400 processed in 5.68 seconds.\n",
      "total loss: 0.8703696131706238\n",
      "\tBatch 26/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 25.84023344516754\n",
      "\tBatch 26/400 processed in 5.69 seconds.\n",
      "total loss: 0.833503246307373\n",
      "\tBatch 27/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 26.673736691474915\n",
      "\tBatch 27/400 processed in 5.78 seconds.\n",
      "total loss: 0.8342764973640442\n",
      "\tBatch 28/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 27.50801318883896\n",
      "\tBatch 28/400 processed in 5.68 seconds.\n",
      "total loss: 0.8435887098312378\n",
      "\tBatch 29/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 28.351601898670197\n",
      "\tBatch 29/400 processed in 5.75 seconds.\n",
      "total loss: 0.8534946441650391\n",
      "\tBatch 30/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 29.205096542835236\n",
      "\tBatch 30/400 processed in 5.69 seconds.\n",
      "total loss: 0.8419058322906494\n",
      "\tBatch 31/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 30.047002375125885\n",
      "\tBatch 31/400 processed in 5.68 seconds.\n",
      "total loss: 0.8483114838600159\n",
      "\tBatch 32/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 30.8953138589859\n",
      "\tBatch 32/400 processed in 5.71 seconds.\n",
      "total loss: 0.8444719314575195\n",
      "\tBatch 33/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 31.73978579044342\n",
      "\tBatch 33/400 processed in 5.70 seconds.\n",
      "total loss: 0.8404698967933655\n",
      "\tBatch 34/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 32.580255687236786\n",
      "\tBatch 34/400 processed in 5.70 seconds.\n",
      "total loss: 0.8206568956375122\n",
      "\tBatch 35/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 33.4009125828743\n",
      "\tBatch 35/400 processed in 5.78 seconds.\n",
      "total loss: 0.8300331234931946\n",
      "\tBatch 36/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 34.23094570636749\n",
      "\tBatch 36/400 processed in 5.71 seconds.\n",
      "total loss: 0.8223553895950317\n",
      "\tBatch 37/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 35.053301095962524\n",
      "\tBatch 37/400 processed in 5.69 seconds.\n",
      "total loss: 0.8461339473724365\n",
      "\tBatch 38/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 35.89943504333496\n",
      "\tBatch 38/400 processed in 5.71 seconds.\n",
      "total loss: 0.7956353425979614\n",
      "\tBatch 39/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 36.69507038593292\n",
      "\tBatch 39/400 processed in 5.69 seconds.\n",
      "total loss: 0.8136104345321655\n",
      "\tBatch 40/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 37.50868082046509\n",
      "\tBatch 40/400 processed in 5.72 seconds.\n",
      "total loss: 0.8062382936477661\n",
      "\tBatch 41/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 38.314919114112854\n",
      "\tBatch 41/400 processed in 5.69 seconds.\n",
      "total loss: 0.7887533903121948\n",
      "\tBatch 42/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 39.10367250442505\n",
      "\tBatch 42/400 processed in 5.81 seconds.\n",
      "total loss: 0.8030472993850708\n",
      "\tBatch 43/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 39.90671980381012\n",
      "\tBatch 43/400 processed in 5.74 seconds.\n",
      "total loss: 0.7910432815551758\n",
      "\tBatch 44/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 40.697763085365295\n",
      "\tBatch 44/400 processed in 5.73 seconds.\n",
      "total loss: 0.7766467332839966\n",
      "\tBatch 45/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 41.47440981864929\n",
      "\tBatch 45/400 processed in 5.72 seconds.\n",
      "total loss: 0.7911827564239502\n",
      "\tBatch 46/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 42.26559257507324\n",
      "\tBatch 46/400 processed in 5.73 seconds.\n",
      "total loss: 0.7980052828788757\n",
      "\tBatch 47/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 43.06359785795212\n",
      "\tBatch 47/400 processed in 5.71 seconds.\n",
      "total loss: 0.7773041725158691\n",
      "\tBatch 48/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 43.84090203046799\n",
      "\tBatch 48/400 processed in 5.80 seconds.\n",
      "total loss: 0.7792977094650269\n",
      "\tBatch 49/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 44.620199739933014\n",
      "\tBatch 49/400 processed in 5.94 seconds.\n",
      "total loss: 0.7622278332710266\n",
      "\tBatch 50/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 45.38242757320404\n",
      "\tBatch 50/400 processed in 5.73 seconds.\n",
      "total loss: 0.7340337634086609\n",
      "\tBatch 51/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 46.1164613366127\n",
      "\tBatch 51/400 processed in 5.81 seconds.\n",
      "total loss: 0.7518270611763\n",
      "\tBatch 52/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 46.868288397789\n",
      "\tBatch 52/400 processed in 5.74 seconds.\n",
      "total loss: 0.771864652633667\n",
      "\tBatch 53/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 47.64015305042267\n",
      "\tBatch 53/400 processed in 5.82 seconds.\n",
      "total loss: 0.7619345188140869\n",
      "\tBatch 54/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 48.402087569236755\n",
      "\tBatch 54/400 processed in 5.73 seconds.\n",
      "total loss: 0.7229565382003784\n",
      "\tBatch 55/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 49.125044107437134\n",
      "\tBatch 55/400 processed in 5.70 seconds.\n",
      "total loss: 0.7577713131904602\n",
      "\tBatch 56/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 49.882815420627594\n",
      "\tBatch 56/400 processed in 5.71 seconds.\n",
      "total loss: 0.7489206194877625\n",
      "\tBatch 57/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 50.631736040115356\n",
      "\tBatch 57/400 processed in 5.69 seconds.\n",
      "total loss: 0.773162305355072\n",
      "\tBatch 58/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 51.40489834547043\n",
      "\tBatch 58/400 processed in 5.82 seconds.\n",
      "total loss: 0.7240647673606873\n",
      "\tBatch 59/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 52.128963112831116\n",
      "\tBatch 59/400 processed in 5.70 seconds.\n",
      "total loss: 0.7546862363815308\n",
      "\tBatch 60/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 52.88364934921265\n",
      "\tBatch 60/400 processed in 5.72 seconds.\n",
      "total loss: 0.7389661073684692\n",
      "\tBatch 61/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 53.622615456581116\n",
      "\tBatch 61/400 processed in 5.70 seconds.\n",
      "total loss: 0.7195560336112976\n",
      "\tBatch 62/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 54.34217149019241\n",
      "\tBatch 62/400 processed in 5.72 seconds.\n",
      "total loss: 0.7377143502235413\n",
      "\tBatch 63/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 55.079885840415955\n",
      "\tBatch 63/400 processed in 5.72 seconds.\n",
      "total loss: 0.7253849506378174\n",
      "\tBatch 64/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 55.80527079105377\n",
      "\tBatch 64/400 processed in 5.70 seconds.\n",
      "total loss: 0.722680389881134\n",
      "\tBatch 65/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 56.527951180934906\n",
      "\tBatch 65/400 processed in 5.63 seconds.\n",
      "total loss: 0.6869090795516968\n",
      "\tBatch 66/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 57.2148602604866\n",
      "\tBatch 66/400 processed in 5.68 seconds.\n",
      "total loss: 0.7030696272850037\n",
      "\tBatch 67/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 57.917929887771606\n",
      "\tBatch 67/400 processed in 5.71 seconds.\n",
      "total loss: 0.7057217955589294\n",
      "\tBatch 68/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 58.623651683330536\n",
      "\tBatch 68/400 processed in 5.98 seconds.\n",
      "total loss: 0.6864303946495056\n",
      "\tBatch 69/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 59.31008207798004\n",
      "\tBatch 69/400 processed in 5.71 seconds.\n",
      "total loss: 0.6868690252304077\n",
      "\tBatch 70/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 59.99695110321045\n",
      "\tBatch 70/400 processed in 5.73 seconds.\n",
      "total loss: 0.6975518465042114\n",
      "\tBatch 71/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 60.69450294971466\n",
      "\tBatch 71/400 processed in 5.70 seconds.\n",
      "total loss: 0.6943631172180176\n",
      "\tBatch 72/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 61.38886606693268\n",
      "\tBatch 72/400 processed in 5.73 seconds.\n",
      "total loss: 0.6709610819816589\n",
      "\tBatch 73/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 62.05982714891434\n",
      "\tBatch 73/400 processed in 5.72 seconds.\n",
      "total loss: 0.6722000241279602\n",
      "\tBatch 74/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 62.7320271730423\n",
      "\tBatch 74/400 processed in 5.83 seconds.\n",
      "total loss: 0.7014083862304688\n",
      "\tBatch 75/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 63.433435559272766\n",
      "\tBatch 75/400 processed in 5.71 seconds.\n",
      "total loss: 0.682620108127594\n",
      "\tBatch 76/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 64.11605566740036\n",
      "\tBatch 76/400 processed in 5.72 seconds.\n",
      "total loss: 0.6938683390617371\n",
      "\tBatch 77/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 64.8099240064621\n",
      "\tBatch 77/400 processed in 5.73 seconds.\n",
      "total loss: 0.6902361512184143\n",
      "\tBatch 78/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 65.50016015768051\n",
      "\tBatch 78/400 processed in 5.75 seconds.\n",
      "total loss: 0.6476689577102661\n",
      "\tBatch 79/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 66.14782911539078\n",
      "\tBatch 79/400 processed in 6.15 seconds.\n",
      "total loss: 0.6580897569656372\n",
      "\tBatch 80/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 66.80591887235641\n",
      "\tBatch 80/400 processed in 5.76 seconds.\n",
      "total loss: 0.6767003536224365\n",
      "\tBatch 81/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 67.48261922597885\n",
      "\tBatch 81/400 processed in 5.74 seconds.\n",
      "total loss: 0.6698830127716064\n",
      "\tBatch 82/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 68.15250223875046\n",
      "\tBatch 82/400 processed in 5.70 seconds.\n",
      "total loss: 0.6600139141082764\n",
      "\tBatch 83/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 68.81251615285873\n",
      "\tBatch 83/400 processed in 5.68 seconds.\n",
      "total loss: 0.6669689416885376\n",
      "\tBatch 84/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 69.47948509454727\n",
      "\tBatch 84/400 processed in 5.71 seconds.\n",
      "total loss: 0.6535211205482483\n",
      "\tBatch 85/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 70.13300621509552\n",
      "\tBatch 85/400 processed in 5.72 seconds.\n",
      "total loss: 0.6494223475456238\n",
      "\tBatch 86/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 70.78242856264114\n",
      "\tBatch 86/400 processed in 5.74 seconds.\n",
      "total loss: 0.6438350677490234\n",
      "\tBatch 87/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 71.42626363039017\n",
      "\tBatch 87/400 processed in 5.71 seconds.\n",
      "total loss: 0.6564551591873169\n",
      "\tBatch 88/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 72.08271878957748\n",
      "\tBatch 88/400 processed in 5.63 seconds.\n",
      "total loss: 0.6742960810661316\n",
      "\tBatch 89/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 72.75701487064362\n",
      "\tBatch 89/400 processed in 5.67 seconds.\n",
      "total loss: 0.6582406759262085\n",
      "\tBatch 90/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 73.41525554656982\n",
      "\tBatch 90/400 processed in 5.74 seconds.\n",
      "total loss: 0.657498836517334\n",
      "\tBatch 91/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 74.07275438308716\n",
      "\tBatch 91/400 processed in 5.64 seconds.\n",
      "total loss: 0.6757031679153442\n",
      "\tBatch 92/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 74.7484575510025\n",
      "\tBatch 92/400 processed in 5.85 seconds.\n",
      "total loss: 0.6490340232849121\n",
      "\tBatch 93/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 75.39749157428741\n",
      "\tBatch 93/400 processed in 5.72 seconds.\n",
      "total loss: 0.6629550457000732\n",
      "\tBatch 94/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 76.06044661998749\n",
      "\tBatch 94/400 processed in 5.72 seconds.\n",
      "total loss: 0.6155085563659668\n",
      "\tBatch 95/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 76.67595517635345\n",
      "\tBatch 95/400 processed in 5.72 seconds.\n",
      "total loss: 0.6221861243247986\n",
      "\tBatch 96/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 77.29814130067825\n",
      "\tBatch 96/400 processed in 5.74 seconds.\n",
      "total loss: 0.6621907353401184\n",
      "\tBatch 97/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 77.96033203601837\n",
      "\tBatch 97/400 processed in 5.71 seconds.\n",
      "total loss: 0.6318202018737793\n",
      "\tBatch 98/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 78.59215223789215\n",
      "\tBatch 98/400 processed in 5.72 seconds.\n",
      "total loss: 0.6422919034957886\n",
      "\tBatch 99/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 79.23444414138794\n",
      "\tBatch 99/400 processed in 5.71 seconds.\n",
      "total loss: 0.6613313555717468\n",
      "\tBatch 100/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 79.89577549695969\n",
      "\tBatch 100/400 processed in 5.85 seconds.\n",
      "total loss: 0.6208652853965759\n",
      "\tBatch 101/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 80.51664078235626\n",
      "\tBatch 101/400 processed in 5.70 seconds.\n",
      "total loss: 0.6508750915527344\n",
      "\tBatch 102/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 81.167515873909\n",
      "\tBatch 102/400 processed in 5.73 seconds.\n",
      "total loss: 0.6473610997200012\n",
      "\tBatch 103/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 81.814876973629\n",
      "\tBatch 103/400 processed in 5.73 seconds.\n",
      "total loss: 0.6544054746627808\n",
      "\tBatch 104/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 82.46928244829178\n",
      "\tBatch 104/400 processed in 5.75 seconds.\n",
      "total loss: 0.6415783166885376\n",
      "\tBatch 105/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 83.11086076498032\n",
      "\tBatch 105/400 processed in 5.84 seconds.\n",
      "total loss: 0.6427781581878662\n",
      "\tBatch 106/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 83.75363892316818\n",
      "\tBatch 106/400 processed in 5.92 seconds.\n",
      "total loss: 0.6430591940879822\n",
      "\tBatch 107/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 84.39669811725616\n",
      "\tBatch 107/400 processed in 5.71 seconds.\n",
      "total loss: 0.6208131909370422\n",
      "\tBatch 108/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 85.0175113081932\n",
      "\tBatch 108/400 processed in 5.72 seconds.\n",
      "total loss: 0.6292234659194946\n",
      "\tBatch 109/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 85.6467347741127\n",
      "\tBatch 109/400 processed in 5.71 seconds.\n",
      "total loss: 0.6415599584579468\n",
      "\tBatch 110/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 86.28829473257065\n",
      "\tBatch 110/400 processed in 5.73 seconds.\n",
      "total loss: 0.6294457316398621\n",
      "\tBatch 111/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 86.91774046421051\n",
      "\tBatch 111/400 processed in 5.71 seconds.\n",
      "total loss: 0.6660698056221008\n",
      "\tBatch 112/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 87.58381026983261\n",
      "\tBatch 112/400 processed in 5.70 seconds.\n",
      "total loss: 0.6286779642105103\n",
      "\tBatch 113/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 88.21248823404312\n",
      "\tBatch 113/400 processed in 5.65 seconds.\n",
      "total loss: 0.6175258159637451\n",
      "\tBatch 114/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 88.83001405000687\n",
      "\tBatch 114/400 processed in 5.78 seconds.\n",
      "total loss: 0.629684567451477\n",
      "\tBatch 115/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 89.45969861745834\n",
      "\tBatch 115/400 processed in 5.74 seconds.\n",
      "total loss: 0.6155256628990173\n",
      "\tBatch 116/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 90.07522428035736\n",
      "\tBatch 116/400 processed in 5.65 seconds.\n",
      "total loss: 0.6207894086837769\n",
      "\tBatch 117/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 90.69601368904114\n",
      "\tBatch 117/400 processed in 5.64 seconds.\n",
      "total loss: 0.6184390783309937\n",
      "\tBatch 118/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 91.31445276737213\n",
      "\tBatch 118/400 processed in 5.71 seconds.\n",
      "total loss: 0.6410934329032898\n",
      "\tBatch 119/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 91.95554620027542\n",
      "\tBatch 119/400 processed in 5.65 seconds.\n",
      "total loss: 0.6242036819458008\n",
      "\tBatch 120/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 92.57974988222122\n",
      "\tBatch 120/400 processed in 5.66 seconds.\n",
      "total loss: 0.6311029195785522\n",
      "\tBatch 121/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 93.21085280179977\n",
      "\tBatch 121/400 processed in 5.69 seconds.\n",
      "total loss: 0.6475197076797485\n",
      "\tBatch 122/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 93.85837250947952\n",
      "\tBatch 122/400 processed in 6.07 seconds.\n",
      "total loss: 0.6133809089660645\n",
      "\tBatch 123/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 94.47175341844559\n",
      "\tBatch 123/400 processed in 5.67 seconds.\n",
      "total loss: 0.6205925941467285\n",
      "\tBatch 124/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 95.09234601259232\n",
      "\tBatch 124/400 processed in 5.64 seconds.\n",
      "total loss: 0.6209757328033447\n",
      "\tBatch 125/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 95.71332174539566\n",
      "\tBatch 125/400 processed in 5.64 seconds.\n",
      "total loss: 0.6408566236495972\n",
      "\tBatch 126/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 96.35417836904526\n",
      "\tBatch 126/400 processed in 5.81 seconds.\n",
      "total loss: 0.6124463677406311\n",
      "\tBatch 127/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 96.96662473678589\n",
      "\tBatch 127/400 processed in 5.63 seconds.\n",
      "total loss: 0.6334960460662842\n",
      "\tBatch 128/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 97.60012078285217\n",
      "\tBatch 128/400 processed in 5.64 seconds.\n",
      "total loss: 0.6405720710754395\n",
      "\tBatch 129/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 98.24069285392761\n",
      "\tBatch 129/400 processed in 5.65 seconds.\n",
      "total loss: 0.6192591190338135\n",
      "\tBatch 130/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 98.85995197296143\n",
      "\tBatch 130/400 processed in 5.68 seconds.\n",
      "total loss: 0.604918360710144\n",
      "\tBatch 131/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 99.46487033367157\n",
      "\tBatch 131/400 processed in 5.63 seconds.\n",
      "total loss: 0.6008710265159607\n",
      "\tBatch 132/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 100.06574136018753\n",
      "\tBatch 132/400 processed in 5.68 seconds.\n",
      "total loss: 0.6100542545318604\n",
      "\tBatch 133/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 100.67579561471939\n",
      "\tBatch 133/400 processed in 5.64 seconds.\n",
      "total loss: 0.626401424407959\n",
      "\tBatch 134/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 101.30219703912735\n",
      "\tBatch 134/400 processed in 5.69 seconds.\n",
      "total loss: 0.6239443421363831\n",
      "\tBatch 135/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 101.92614138126373\n",
      "\tBatch 135/400 processed in 5.67 seconds.\n",
      "total loss: 0.6193825006484985\n",
      "\tBatch 136/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 102.54552388191223\n",
      "\tBatch 136/400 processed in 5.76 seconds.\n",
      "total loss: 0.6029531955718994\n",
      "\tBatch 137/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 103.14847707748413\n",
      "\tBatch 137/400 processed in 5.75 seconds.\n",
      "total loss: 0.5936234593391418\n",
      "\tBatch 138/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 103.74210053682327\n",
      "\tBatch 138/400 processed in 5.78 seconds.\n",
      "total loss: 0.5928698778152466\n",
      "\tBatch 139/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 104.33497041463852\n",
      "\tBatch 139/400 processed in 5.79 seconds.\n",
      "total loss: 0.5975171327590942\n",
      "\tBatch 140/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 104.93248754739761\n",
      "\tBatch 140/400 processed in 6.25 seconds.\n",
      "total loss: 0.606646716594696\n",
      "\tBatch 141/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 105.53913426399231\n",
      "\tBatch 141/400 processed in 5.77 seconds.\n",
      "total loss: 0.586609959602356\n",
      "\tBatch 142/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 106.12574422359467\n",
      "\tBatch 142/400 processed in 5.85 seconds.\n",
      "total loss: 0.5866535902023315\n",
      "\tBatch 143/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 106.712397813797\n",
      "\tBatch 143/400 processed in 5.78 seconds.\n",
      "total loss: 0.5998925566673279\n",
      "\tBatch 144/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 107.31229037046432\n",
      "\tBatch 144/400 processed in 5.80 seconds.\n",
      "total loss: 0.5990892648696899\n",
      "\tBatch 145/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 107.91137963533401\n",
      "\tBatch 145/400 processed in 5.79 seconds.\n",
      "total loss: 0.5992504358291626\n",
      "\tBatch 146/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 108.51063007116318\n",
      "\tBatch 146/400 processed in 5.83 seconds.\n",
      "total loss: 0.599644660949707\n",
      "\tBatch 147/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 109.11027473211288\n",
      "\tBatch 147/400 processed in 5.79 seconds.\n",
      "total loss: 0.5837692618370056\n",
      "\tBatch 148/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 109.69404399394989\n",
      "\tBatch 148/400 processed in 5.72 seconds.\n",
      "total loss: 0.5985140204429626\n",
      "\tBatch 149/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 110.29255801439285\n",
      "\tBatch 149/400 processed in 5.75 seconds.\n",
      "total loss: 0.5957421064376831\n",
      "\tBatch 150/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 110.88830012083054\n",
      "\tBatch 150/400 processed in 5.76 seconds.\n",
      "total loss: 0.5900105834007263\n",
      "\tBatch 151/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 111.47831070423126\n",
      "\tBatch 151/400 processed in 5.77 seconds.\n",
      "total loss: 0.5714552998542786\n",
      "\tBatch 152/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 112.04976600408554\n",
      "\tBatch 152/400 processed in 5.85 seconds.\n",
      "total loss: 0.5695909261703491\n",
      "\tBatch 153/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 112.61935693025589\n",
      "\tBatch 153/400 processed in 5.71 seconds.\n",
      "total loss: 0.5898737907409668\n",
      "\tBatch 154/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 113.20923072099686\n",
      "\tBatch 154/400 processed in 5.78 seconds.\n",
      "total loss: 0.5560798645019531\n",
      "\tBatch 155/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 113.76531058549881\n",
      "\tBatch 155/400 processed in 5.89 seconds.\n",
      "total loss: 0.5760682821273804\n",
      "\tBatch 156/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 114.34137886762619\n",
      "\tBatch 156/400 processed in 5.77 seconds.\n",
      "total loss: 0.5692527890205383\n",
      "\tBatch 157/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 114.91063165664673\n",
      "\tBatch 157/400 processed in 5.83 seconds.\n",
      "total loss: 0.5759407877922058\n",
      "\tBatch 158/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 115.48657244443893\n",
      "\tBatch 158/400 processed in 5.73 seconds.\n",
      "total loss: 0.5809900164604187\n",
      "\tBatch 159/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 116.06756246089935\n",
      "\tBatch 159/400 processed in 5.69 seconds.\n",
      "total loss: 0.5804723501205444\n",
      "\tBatch 160/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 116.6480348110199\n",
      "\tBatch 160/400 processed in 5.74 seconds.\n",
      "total loss: 0.5581620931625366\n",
      "\tBatch 161/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 117.20619690418243\n",
      "\tBatch 161/400 processed in 5.97 seconds.\n",
      "total loss: 0.5788315534591675\n",
      "\tBatch 162/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 117.7850284576416\n",
      "\tBatch 162/400 processed in 5.80 seconds.\n",
      "total loss: 0.5875166654586792\n",
      "\tBatch 163/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 118.37254512310028\n",
      "\tBatch 163/400 processed in 5.75 seconds.\n",
      "total loss: 0.5726901292800903\n",
      "\tBatch 164/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 118.94523525238037\n",
      "\tBatch 164/400 processed in 5.77 seconds.\n",
      "total loss: 0.5822793245315552\n",
      "\tBatch 165/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 119.52751457691193\n",
      "\tBatch 165/400 processed in 5.75 seconds.\n",
      "total loss: 0.602756679058075\n",
      "\tBatch 166/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 120.13027125597\n",
      "\tBatch 166/400 processed in 5.78 seconds.\n",
      "total loss: 0.5730603337287903\n",
      "\tBatch 167/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 120.70333158969879\n",
      "\tBatch 167/400 processed in 5.74 seconds.\n",
      "total loss: 0.553897500038147\n",
      "\tBatch 168/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 121.25722908973694\n",
      "\tBatch 168/400 processed in 5.77 seconds.\n",
      "total loss: 0.5756452083587646\n",
      "\tBatch 169/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 121.8328742980957\n",
      "\tBatch 169/400 processed in 5.74 seconds.\n",
      "total loss: 0.5521831512451172\n",
      "\tBatch 170/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 122.38505744934082\n",
      "\tBatch 170/400 processed in 5.80 seconds.\n",
      "total loss: 0.5769644975662231\n",
      "\tBatch 171/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 122.96202194690704\n",
      "\tBatch 171/400 processed in 5.74 seconds.\n",
      "total loss: 0.5635548830032349\n",
      "\tBatch 172/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 123.52557682991028\n",
      "\tBatch 172/400 processed in 5.77 seconds.\n",
      "total loss: 0.5686452388763428\n",
      "\tBatch 173/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 124.09422206878662\n",
      "\tBatch 173/400 processed in 5.83 seconds.\n",
      "total loss: 0.5739534497261047\n",
      "\tBatch 174/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 124.66817551851273\n",
      "\tBatch 174/400 processed in 5.78 seconds.\n",
      "total loss: 0.5526516437530518\n",
      "\tBatch 175/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 125.22082716226578\n",
      "\tBatch 175/400 processed in 5.74 seconds.\n",
      "total loss: 0.5446404814720154\n",
      "\tBatch 176/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 125.7654676437378\n",
      "\tBatch 176/400 processed in 5.78 seconds.\n",
      "total loss: 0.5673351883888245\n",
      "\tBatch 177/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 126.33280283212662\n",
      "\tBatch 177/400 processed in 5.75 seconds.\n",
      "total loss: 0.5721844434738159\n",
      "\tBatch 178/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 126.90498727560043\n",
      "\tBatch 178/400 processed in 5.82 seconds.\n",
      "total loss: 0.5539010763168335\n",
      "\tBatch 179/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 127.45888835191727\n",
      "\tBatch 179/400 processed in 5.76 seconds.\n",
      "total loss: 0.5563246011734009\n",
      "\tBatch 180/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 128.01521295309067\n",
      "\tBatch 180/400 processed in 5.78 seconds.\n",
      "total loss: 0.5690580606460571\n",
      "\tBatch 181/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 128.58427101373672\n",
      "\tBatch 181/400 processed in 5.75 seconds.\n",
      "total loss: 0.568670392036438\n",
      "\tBatch 182/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 129.15294140577316\n",
      "\tBatch 182/400 processed in 5.75 seconds.\n",
      "total loss: 0.5481247901916504\n",
      "\tBatch 183/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 129.7010661959648\n",
      "\tBatch 183/400 processed in 5.89 seconds.\n",
      "total loss: 0.5598307847976685\n",
      "\tBatch 184/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 130.26089698076248\n",
      "\tBatch 184/400 processed in 6.00 seconds.\n",
      "total loss: 0.5399855375289917\n",
      "\tBatch 185/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 130.80088251829147\n",
      "\tBatch 185/400 processed in 5.74 seconds.\n",
      "total loss: 0.5595226287841797\n",
      "\tBatch 186/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 131.36040514707565\n",
      "\tBatch 186/400 processed in 5.78 seconds.\n",
      "total loss: 0.5338699817657471\n",
      "\tBatch 187/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 131.8942751288414\n",
      "\tBatch 187/400 processed in 5.75 seconds.\n",
      "total loss: 0.535783052444458\n",
      "\tBatch 188/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 132.43005818128586\n",
      "\tBatch 188/400 processed in 5.91 seconds.\n",
      "total loss: 0.5453345775604248\n",
      "\tBatch 189/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 132.97539275884628\n",
      "\tBatch 189/400 processed in 5.75 seconds.\n",
      "total loss: 0.5541414618492126\n",
      "\tBatch 190/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 133.5295342206955\n",
      "\tBatch 190/400 processed in 5.79 seconds.\n",
      "total loss: 0.516832709312439\n",
      "\tBatch 191/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 134.04636693000793\n",
      "\tBatch 191/400 processed in 5.76 seconds.\n",
      "total loss: 0.5188570618629456\n",
      "\tBatch 192/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 134.56522399187088\n",
      "\tBatch 192/400 processed in 5.79 seconds.\n",
      "total loss: 0.5357705950737\n",
      "\tBatch 193/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 135.10099458694458\n",
      "\tBatch 193/400 processed in 5.76 seconds.\n",
      "total loss: 0.5409137606620789\n",
      "\tBatch 194/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 135.64190834760666\n",
      "\tBatch 194/400 processed in 5.77 seconds.\n",
      "total loss: 0.5485642552375793\n",
      "\tBatch 195/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 136.19047260284424\n",
      "\tBatch 195/400 processed in 5.75 seconds.\n",
      "total loss: 0.5185489654541016\n",
      "\tBatch 196/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 136.70902156829834\n",
      "\tBatch 196/400 processed in 5.79 seconds.\n",
      "total loss: 0.538749098777771\n",
      "\tBatch 197/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 137.2477706670761\n",
      "\tBatch 197/400 processed in 5.76 seconds.\n",
      "total loss: 0.5262028574943542\n",
      "\tBatch 198/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 137.77397352457047\n",
      "\tBatch 198/400 processed in 5.77 seconds.\n",
      "total loss: 0.5226956605911255\n",
      "\tBatch 199/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 138.2966691851616\n",
      "\tBatch 199/400 processed in 5.75 seconds.\n",
      "total loss: 0.5359459519386292\n",
      "\tBatch 200/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 138.83261513710022\n",
      "\tBatch 200/400 processed in 5.76 seconds.\n",
      "total loss: 0.524146318435669\n",
      "\tBatch 201/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 139.3567614555359\n",
      "\tBatch 201/400 processed in 5.75 seconds.\n",
      "total loss: 0.5497854351997375\n",
      "\tBatch 202/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 139.90654689073563\n",
      "\tBatch 202/400 processed in 5.78 seconds.\n",
      "total loss: 0.5163429975509644\n",
      "\tBatch 203/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 140.4228898882866\n",
      "\tBatch 203/400 processed in 5.76 seconds.\n",
      "total loss: 0.5170367956161499\n",
      "\tBatch 204/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 140.93992668390274\n",
      "\tBatch 204/400 processed in 5.84 seconds.\n",
      "total loss: 0.5184481739997864\n",
      "\tBatch 205/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 141.45837485790253\n",
      "\tBatch 205/400 processed in 5.76 seconds.\n",
      "total loss: 0.52174973487854\n",
      "\tBatch 206/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 141.98012459278107\n",
      "\tBatch 206/400 processed in 5.78 seconds.\n",
      "total loss: 0.5261904001235962\n",
      "\tBatch 207/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 142.50631499290466\n",
      "\tBatch 207/400 processed in 5.78 seconds.\n",
      "total loss: 0.5352717638015747\n",
      "\tBatch 208/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 143.04158675670624\n",
      "\tBatch 208/400 processed in 5.78 seconds.\n",
      "total loss: 0.5072751045227051\n",
      "\tBatch 209/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 143.54886186122894\n",
      "\tBatch 209/400 processed in 5.80 seconds.\n",
      "total loss: 0.51439368724823\n",
      "\tBatch 210/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 144.06325554847717\n",
      "\tBatch 210/400 processed in 6.33 seconds.\n",
      "total loss: 0.525980532169342\n",
      "\tBatch 211/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 144.58923608064651\n",
      "\tBatch 211/400 processed in 5.77 seconds.\n",
      "total loss: 0.5357337594032288\n",
      "\tBatch 212/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 145.12496984004974\n",
      "\tBatch 212/400 processed in 5.79 seconds.\n",
      "total loss: 0.4933115839958191\n",
      "\tBatch 213/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 145.61828142404556\n",
      "\tBatch 213/400 processed in 5.83 seconds.\n",
      "total loss: 0.5242351293563843\n",
      "\tBatch 214/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 146.14251655340195\n",
      "\tBatch 214/400 processed in 5.83 seconds.\n",
      "total loss: 0.5100072622299194\n",
      "\tBatch 215/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 146.65252381563187\n",
      "\tBatch 215/400 processed in 5.76 seconds.\n",
      "total loss: 0.5011610984802246\n",
      "\tBatch 216/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 147.1536849141121\n",
      "\tBatch 216/400 processed in 5.77 seconds.\n",
      "total loss: 0.4938983619213104\n",
      "\tBatch 217/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 147.6475832760334\n",
      "\tBatch 217/400 processed in 5.75 seconds.\n",
      "total loss: 0.5232129096984863\n",
      "\tBatch 218/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 148.1707961857319\n",
      "\tBatch 218/400 processed in 5.76 seconds.\n",
      "total loss: 0.5281777381896973\n",
      "\tBatch 219/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 148.69897392392159\n",
      "\tBatch 219/400 processed in 5.88 seconds.\n",
      "total loss: 0.5076133608818054\n",
      "\tBatch 220/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 149.2065872848034\n",
      "\tBatch 220/400 processed in 5.89 seconds.\n",
      "total loss: 0.5253990292549133\n",
      "\tBatch 221/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 149.7319863140583\n",
      "\tBatch 221/400 processed in 5.75 seconds.\n",
      "total loss: 0.4999251961708069\n",
      "\tBatch 222/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 150.2319115102291\n",
      "\tBatch 222/400 processed in 5.77 seconds.\n",
      "total loss: 0.5022463798522949\n",
      "\tBatch 223/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 150.7341578900814\n",
      "\tBatch 223/400 processed in 5.74 seconds.\n",
      "total loss: 0.5153285264968872\n",
      "\tBatch 224/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 151.2494864165783\n",
      "\tBatch 224/400 processed in 5.88 seconds.\n",
      "total loss: 0.49691516160964966\n",
      "\tBatch 225/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 151.74640157818794\n",
      "\tBatch 225/400 processed in 5.74 seconds.\n",
      "total loss: 0.5189176797866821\n",
      "\tBatch 226/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 152.26531925797462\n",
      "\tBatch 226/400 processed in 5.78 seconds.\n",
      "total loss: 0.5155068635940552\n",
      "\tBatch 227/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 152.78082612156868\n",
      "\tBatch 227/400 processed in 5.75 seconds.\n",
      "total loss: 0.47627395391464233\n",
      "\tBatch 228/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 153.25710007548332\n",
      "\tBatch 228/400 processed in 5.77 seconds.\n",
      "total loss: 0.4943239092826843\n",
      "\tBatch 229/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 153.751423984766\n",
      "\tBatch 229/400 processed in 5.75 seconds.\n",
      "total loss: 0.5255707502365112\n",
      "\tBatch 230/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 154.27699473500252\n",
      "\tBatch 230/400 processed in 5.78 seconds.\n",
      "total loss: 0.5211873054504395\n",
      "\tBatch 231/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 154.79818204045296\n",
      "\tBatch 231/400 processed in 5.76 seconds.\n",
      "total loss: 0.5183290243148804\n",
      "\tBatch 232/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 155.31651106476784\n",
      "\tBatch 232/400 processed in 5.79 seconds.\n",
      "total loss: 0.5050373077392578\n",
      "\tBatch 233/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 155.8215483725071\n",
      "\tBatch 233/400 processed in 5.76 seconds.\n",
      "total loss: 0.49649834632873535\n",
      "\tBatch 234/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 156.31804671883583\n",
      "\tBatch 234/400 processed in 5.78 seconds.\n",
      "total loss: 0.5194244980812073\n",
      "\tBatch 235/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 156.83747121691704\n",
      "\tBatch 235/400 processed in 5.76 seconds.\n",
      "total loss: 0.4843572676181793\n",
      "\tBatch 236/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 157.32182848453522\n",
      "\tBatch 236/400 processed in 5.78 seconds.\n",
      "total loss: 0.504663348197937\n",
      "\tBatch 237/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 157.82649183273315\n",
      "\tBatch 237/400 processed in 5.76 seconds.\n",
      "total loss: 0.48279619216918945\n",
      "\tBatch 238/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 158.30928802490234\n",
      "\tBatch 238/400 processed in 5.94 seconds.\n",
      "total loss: 0.49094170331954956\n",
      "\tBatch 239/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 158.8002297282219\n",
      "\tBatch 239/400 processed in 5.75 seconds.\n",
      "total loss: 0.48954588174819946\n",
      "\tBatch 240/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 159.2897756099701\n",
      "\tBatch 240/400 processed in 6.09 seconds.\n",
      "total loss: 0.49989014863967896\n",
      "\tBatch 241/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 159.78966575860977\n",
      "\tBatch 241/400 processed in 5.73 seconds.\n",
      "total loss: 0.4803199768066406\n",
      "\tBatch 242/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 160.2699857354164\n",
      "\tBatch 242/400 processed in 5.75 seconds.\n",
      "total loss: 0.5094709992408752\n",
      "\tBatch 243/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 160.7794567346573\n",
      "\tBatch 243/400 processed in 5.73 seconds.\n",
      "total loss: 0.5367351174354553\n",
      "\tBatch 244/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 161.31619185209274\n",
      "\tBatch 244/400 processed in 5.79 seconds.\n",
      "total loss: 0.4947831630706787\n",
      "\tBatch 245/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 161.81097501516342\n",
      "\tBatch 245/400 processed in 5.82 seconds.\n",
      "total loss: 0.49960654973983765\n",
      "\tBatch 246/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 162.31058156490326\n",
      "\tBatch 246/400 processed in 5.77 seconds.\n",
      "total loss: 0.47811877727508545\n",
      "\tBatch 247/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 162.78870034217834\n",
      "\tBatch 247/400 processed in 5.75 seconds.\n",
      "total loss: 0.4721831679344177\n",
      "\tBatch 248/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 163.26088351011276\n",
      "\tBatch 248/400 processed in 5.76 seconds.\n",
      "total loss: 0.47577351331710815\n",
      "\tBatch 249/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 163.73665702342987\n",
      "\tBatch 249/400 processed in 5.73 seconds.\n",
      "total loss: 0.4874013662338257\n",
      "\tBatch 250/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 164.2240583896637\n",
      "\tBatch 250/400 processed in 5.88 seconds.\n",
      "total loss: 0.4867321848869324\n",
      "\tBatch 251/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 164.71079057455063\n",
      "\tBatch 251/400 processed in 5.73 seconds.\n",
      "total loss: 0.47796210646629333\n",
      "\tBatch 252/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 165.18875268101692\n",
      "\tBatch 252/400 processed in 5.75 seconds.\n",
      "total loss: 0.4891054332256317\n",
      "\tBatch 253/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 165.67785811424255\n",
      "\tBatch 253/400 processed in 5.73 seconds.\n",
      "total loss: 0.4901452660560608\n",
      "\tBatch 254/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 166.16800338029861\n",
      "\tBatch 254/400 processed in 5.76 seconds.\n",
      "total loss: 0.5133881568908691\n",
      "\tBatch 255/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 166.68139153718948\n",
      "\tBatch 255/400 processed in 5.86 seconds.\n",
      "total loss: 0.47803813219070435\n",
      "\tBatch 256/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 167.1594296693802\n",
      "\tBatch 256/400 processed in 5.76 seconds.\n",
      "total loss: 0.46513932943344116\n",
      "\tBatch 257/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 167.62456899881363\n",
      "\tBatch 257/400 processed in 5.73 seconds.\n",
      "total loss: 0.47887614369392395\n",
      "\tBatch 258/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 168.10344514250755\n",
      "\tBatch 258/400 processed in 5.77 seconds.\n",
      "total loss: 0.4767402410507202\n",
      "\tBatch 259/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 168.58018538355827\n",
      "\tBatch 259/400 processed in 5.74 seconds.\n",
      "total loss: 0.4899623990058899\n",
      "\tBatch 260/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 169.07014778256416\n",
      "\tBatch 260/400 processed in 5.76 seconds.\n",
      "total loss: 0.48441383242607117\n",
      "\tBatch 261/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 169.55456161499023\n",
      "\tBatch 261/400 processed in 5.74 seconds.\n",
      "total loss: 0.4770441949367523\n",
      "\tBatch 262/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 170.031605809927\n",
      "\tBatch 262/400 processed in 5.77 seconds.\n",
      "total loss: 0.4599987864494324\n",
      "\tBatch 263/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 170.49160459637642\n",
      "\tBatch 263/400 processed in 5.73 seconds.\n",
      "total loss: 0.47991952300071716\n",
      "\tBatch 264/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 170.97152411937714\n",
      "\tBatch 264/400 processed in 5.74 seconds.\n",
      "total loss: 0.4910929799079895\n",
      "\tBatch 265/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 171.46261709928513\n",
      "\tBatch 265/400 processed in 5.73 seconds.\n",
      "total loss: 0.45836085081100464\n",
      "\tBatch 266/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 171.92097795009613\n",
      "\tBatch 266/400 processed in 5.74 seconds.\n",
      "total loss: 0.4655810296535492\n",
      "\tBatch 267/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 172.38655897974968\n",
      "\tBatch 267/400 processed in 5.73 seconds.\n",
      "total loss: 0.49668166041374207\n",
      "\tBatch 268/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 172.88324064016342\n",
      "\tBatch 268/400 processed in 5.76 seconds.\n",
      "total loss: 0.47928255796432495\n",
      "\tBatch 269/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 173.36252319812775\n",
      "\tBatch 269/400 processed in 5.73 seconds.\n",
      "total loss: 0.48350846767425537\n",
      "\tBatch 270/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 173.846031665802\n",
      "\tBatch 270/400 processed in 5.75 seconds.\n",
      "total loss: 0.46827632188796997\n",
      "\tBatch 271/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 174.31430798768997\n",
      "\tBatch 271/400 processed in 5.74 seconds.\n",
      "total loss: 0.4741581678390503\n",
      "\tBatch 272/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 174.78846615552902\n",
      "\tBatch 272/400 processed in 5.74 seconds.\n",
      "total loss: 0.4615948796272278\n",
      "\tBatch 273/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 175.25006103515625\n",
      "\tBatch 273/400 processed in 6.00 seconds.\n",
      "total loss: 0.4657939076423645\n",
      "\tBatch 274/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 175.71585494279861\n",
      "\tBatch 274/400 processed in 5.73 seconds.\n",
      "total loss: 0.4825094938278198\n",
      "\tBatch 275/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 176.19836443662643\n",
      "\tBatch 275/400 processed in 5.72 seconds.\n",
      "total loss: 0.45750778913497925\n",
      "\tBatch 276/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 176.6558722257614\n",
      "\tBatch 276/400 processed in 5.86 seconds.\n",
      "total loss: 0.476107120513916\n",
      "\tBatch 277/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 177.13197934627533\n",
      "\tBatch 277/400 processed in 5.71 seconds.\n",
      "total loss: 0.4707062840461731\n",
      "\tBatch 278/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 177.6026856303215\n",
      "\tBatch 278/400 processed in 5.72 seconds.\n",
      "total loss: 0.4596121311187744\n",
      "\tBatch 279/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 178.06229776144028\n",
      "\tBatch 279/400 processed in 5.71 seconds.\n",
      "total loss: 0.48946094512939453\n",
      "\tBatch 280/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 178.55175870656967\n",
      "\tBatch 280/400 processed in 5.72 seconds.\n",
      "total loss: 0.46573060750961304\n",
      "\tBatch 281/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 179.01748931407928\n",
      "\tBatch 281/400 processed in 5.82 seconds.\n",
      "total loss: 0.4704745411872864\n",
      "\tBatch 282/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 179.48796385526657\n",
      "\tBatch 282/400 processed in 5.72 seconds.\n",
      "total loss: 0.4873654246330261\n",
      "\tBatch 283/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 179.9753292798996\n",
      "\tBatch 283/400 processed in 5.70 seconds.\n",
      "total loss: 0.46388983726501465\n",
      "\tBatch 284/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 180.4392191171646\n",
      "\tBatch 284/400 processed in 5.75 seconds.\n",
      "total loss: 0.4503251612186432\n",
      "\tBatch 285/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 180.88954427838326\n",
      "\tBatch 285/400 processed in 5.72 seconds.\n",
      "total loss: 0.4879921078681946\n",
      "\tBatch 286/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 181.37753638625145\n",
      "\tBatch 286/400 processed in 5.75 seconds.\n",
      "total loss: 0.46248042583465576\n",
      "\tBatch 287/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 181.8400168120861\n",
      "\tBatch 287/400 processed in 5.73 seconds.\n",
      "total loss: 0.45513153076171875\n",
      "\tBatch 288/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 182.29514834284782\n",
      "\tBatch 288/400 processed in 5.72 seconds.\n",
      "total loss: 0.4550313651561737\n",
      "\tBatch 289/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 182.750179708004\n",
      "\tBatch 289/400 processed in 5.71 seconds.\n",
      "total loss: 0.46368998289108276\n",
      "\tBatch 290/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 183.21386969089508\n",
      "\tBatch 290/400 processed in 5.74 seconds.\n",
      "total loss: 0.47391122579574585\n",
      "\tBatch 291/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 183.68778091669083\n",
      "\tBatch 291/400 processed in 5.70 seconds.\n",
      "total loss: 0.4735470414161682\n",
      "\tBatch 292/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 184.161327958107\n",
      "\tBatch 292/400 processed in 5.73 seconds.\n",
      "total loss: 0.4841119647026062\n",
      "\tBatch 293/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 184.6454399228096\n",
      "\tBatch 293/400 processed in 5.70 seconds.\n",
      "total loss: 0.4795478582382202\n",
      "\tBatch 294/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 185.12498778104782\n",
      "\tBatch 294/400 processed in 5.73 seconds.\n",
      "total loss: 0.4623739719390869\n",
      "\tBatch 295/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 185.5873617529869\n",
      "\tBatch 295/400 processed in 5.72 seconds.\n",
      "total loss: 0.46912479400634766\n",
      "\tBatch 296/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 186.05648654699326\n",
      "\tBatch 296/400 processed in 5.74 seconds.\n",
      "total loss: 0.473273366689682\n",
      "\tBatch 297/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 186.52975991368294\n",
      "\tBatch 297/400 processed in 5.76 seconds.\n",
      "total loss: 0.461416095495224\n",
      "\tBatch 298/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 186.99117600917816\n",
      "\tBatch 298/400 processed in 5.73 seconds.\n",
      "total loss: 0.4797341227531433\n",
      "\tBatch 299/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 187.4709101319313\n",
      "\tBatch 299/400 processed in 5.72 seconds.\n",
      "total loss: 0.45933040976524353\n",
      "\tBatch 300/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 187.93024054169655\n",
      "\tBatch 300/400 processed in 5.73 seconds.\n",
      "total loss: 0.45378202199935913\n",
      "\tBatch 301/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 188.3840225636959\n",
      "\tBatch 301/400 processed in 5.72 seconds.\n",
      "total loss: 0.48294907808303833\n",
      "\tBatch 302/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 188.86697164177895\n",
      "\tBatch 302/400 processed in 5.87 seconds.\n",
      "total loss: 0.49733471870422363\n",
      "\tBatch 303/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 189.36430636048317\n",
      "\tBatch 303/400 processed in 5.71 seconds.\n",
      "total loss: 0.4653375744819641\n",
      "\tBatch 304/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 189.82964393496513\n",
      "\tBatch 304/400 processed in 5.73 seconds.\n",
      "total loss: 0.45706552267074585\n",
      "\tBatch 305/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 190.28670945763588\n",
      "\tBatch 305/400 processed in 5.71 seconds.\n",
      "total loss: 0.45666074752807617\n",
      "\tBatch 306/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 190.74337020516396\n",
      "\tBatch 306/400 processed in 5.73 seconds.\n",
      "total loss: 0.4674370288848877\n",
      "\tBatch 307/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 191.21080723404884\n",
      "\tBatch 307/400 processed in 5.86 seconds.\n",
      "total loss: 0.4511416554450989\n",
      "\tBatch 308/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 191.66194888949394\n",
      "\tBatch 308/400 processed in 5.73 seconds.\n",
      "total loss: 0.46532732248306274\n",
      "\tBatch 309/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 192.127276211977\n",
      "\tBatch 309/400 processed in 5.72 seconds.\n",
      "total loss: 0.49944984912872314\n",
      "\tBatch 310/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 192.62672606110573\n",
      "\tBatch 310/400 processed in 6.20 seconds.\n",
      "total loss: 0.4798510670661926\n",
      "\tBatch 311/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 193.10657712817192\n",
      "\tBatch 311/400 processed in 5.71 seconds.\n",
      "total loss: 0.48711705207824707\n",
      "\tBatch 312/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 193.59369418025017\n",
      "\tBatch 312/400 processed in 5.74 seconds.\n",
      "total loss: 0.44630157947540283\n",
      "\tBatch 313/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 194.03999575972557\n",
      "\tBatch 313/400 processed in 5.71 seconds.\n",
      "total loss: 0.4713937044143677\n",
      "\tBatch 314/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 194.51138946413994\n",
      "\tBatch 314/400 processed in 5.74 seconds.\n",
      "total loss: 0.4589785635471344\n",
      "\tBatch 315/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 194.97036802768707\n",
      "\tBatch 315/400 processed in 5.71 seconds.\n",
      "total loss: 0.4875759482383728\n",
      "\tBatch 316/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 195.45794397592545\n",
      "\tBatch 316/400 processed in 5.74 seconds.\n",
      "total loss: 0.48298215866088867\n",
      "\tBatch 317/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 195.94092613458633\n",
      "\tBatch 317/400 processed in 5.71 seconds.\n",
      "total loss: 0.46004539728164673\n",
      "\tBatch 318/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 196.40097153186798\n",
      "\tBatch 318/400 processed in 5.75 seconds.\n",
      "total loss: 0.462064266204834\n",
      "\tBatch 319/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 196.86303579807281\n",
      "\tBatch 319/400 processed in 5.74 seconds.\n",
      "total loss: 0.460297167301178\n",
      "\tBatch 320/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 197.323332965374\n",
      "\tBatch 320/400 processed in 5.76 seconds.\n",
      "total loss: 0.454342782497406\n",
      "\tBatch 321/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 197.7776757478714\n",
      "\tBatch 321/400 processed in 5.73 seconds.\n",
      "total loss: 0.4616572856903076\n",
      "\tBatch 322/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 198.2393330335617\n",
      "\tBatch 322/400 processed in 5.75 seconds.\n",
      "total loss: 0.47603654861450195\n",
      "\tBatch 323/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 198.7153695821762\n",
      "\tBatch 323/400 processed in 5.74 seconds.\n",
      "total loss: 0.46202248334884644\n",
      "\tBatch 324/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 199.17739206552505\n",
      "\tBatch 324/400 processed in 5.75 seconds.\n",
      "total loss: 0.4596140682697296\n",
      "\tBatch 325/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 199.63700613379478\n",
      "\tBatch 325/400 processed in 5.74 seconds.\n",
      "total loss: 0.455969899892807\n",
      "\tBatch 326/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 200.0929760336876\n",
      "\tBatch 326/400 processed in 5.75 seconds.\n",
      "total loss: 0.4817054271697998\n",
      "\tBatch 327/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 200.5746814608574\n",
      "\tBatch 327/400 processed in 5.73 seconds.\n",
      "total loss: 0.4781414866447449\n",
      "\tBatch 328/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 201.05282294750214\n",
      "\tBatch 328/400 processed in 5.87 seconds.\n",
      "total loss: 0.46056288480758667\n",
      "\tBatch 329/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 201.51338583230972\n",
      "\tBatch 329/400 processed in 5.74 seconds.\n",
      "total loss: 0.4629652202129364\n",
      "\tBatch 330/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 201.97635105252266\n",
      "\tBatch 330/400 processed in 5.72 seconds.\n",
      "total loss: 0.4487926959991455\n",
      "\tBatch 331/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 202.4251437485218\n",
      "\tBatch 331/400 processed in 5.62 seconds.\n",
      "total loss: 0.45423197746276855\n",
      "\tBatch 332/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 202.87937572598457\n",
      "\tBatch 332/400 processed in 5.66 seconds.\n",
      "total loss: 0.4743679165840149\n",
      "\tBatch 333/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 203.3537436425686\n",
      "\tBatch 333/400 processed in 5.83 seconds.\n",
      "total loss: 0.4558942914009094\n",
      "\tBatch 334/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 203.8096379339695\n",
      "\tBatch 334/400 processed in 5.74 seconds.\n",
      "total loss: 0.44716697931289673\n",
      "\tBatch 335/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 204.2568049132824\n",
      "\tBatch 335/400 processed in 5.74 seconds.\n",
      "total loss: 0.45524466037750244\n",
      "\tBatch 336/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 204.7120495736599\n",
      "\tBatch 336/400 processed in 5.75 seconds.\n",
      "total loss: 0.46308010816574097\n",
      "\tBatch 337/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 205.17512968182564\n",
      "\tBatch 337/400 processed in 5.71 seconds.\n",
      "total loss: 0.45633411407470703\n",
      "\tBatch 338/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 205.63146379590034\n",
      "\tBatch 338/400 processed in 5.76 seconds.\n",
      "total loss: 0.4634733200073242\n",
      "\tBatch 339/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 206.09493711590767\n",
      "\tBatch 339/400 processed in 5.66 seconds.\n",
      "total loss: 0.44273972511291504\n",
      "\tBatch 340/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 206.53767684102058\n",
      "\tBatch 340/400 processed in 5.69 seconds.\n",
      "total loss: 0.4481639862060547\n",
      "\tBatch 341/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 206.98584082722664\n",
      "\tBatch 341/400 processed in 5.72 seconds.\n",
      "total loss: 0.46842896938323975\n",
      "\tBatch 342/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 207.45426979660988\n",
      "\tBatch 342/400 processed in 5.75 seconds.\n",
      "total loss: 0.47051310539245605\n",
      "\tBatch 343/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 207.92478290200233\n",
      "\tBatch 343/400 processed in 5.72 seconds.\n",
      "total loss: 0.45415228605270386\n",
      "\tBatch 344/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 208.37893518805504\n",
      "\tBatch 344/400 processed in 5.75 seconds.\n",
      "total loss: 0.4486851096153259\n",
      "\tBatch 345/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 208.82762029767036\n",
      "\tBatch 345/400 processed in 5.73 seconds.\n",
      "total loss: 0.448675274848938\n",
      "\tBatch 346/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 209.2762955725193\n",
      "\tBatch 346/400 processed in 5.75 seconds.\n",
      "total loss: 0.46372276544570923\n",
      "\tBatch 347/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 209.740018337965\n",
      "\tBatch 347/400 processed in 5.73 seconds.\n",
      "total loss: 0.4500885009765625\n",
      "\tBatch 348/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 210.19010683894157\n",
      "\tBatch 348/400 processed in 5.73 seconds.\n",
      "total loss: 0.4363320469856262\n",
      "\tBatch 349/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 210.6264388859272\n",
      "\tBatch 349/400 processed in 5.76 seconds.\n",
      "total loss: 0.4603947699069977\n",
      "\tBatch 350/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 211.0868336558342\n",
      "\tBatch 350/400 processed in 5.71 seconds.\n",
      "total loss: 0.4589412212371826\n",
      "\tBatch 351/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 211.54577487707138\n",
      "\tBatch 351/400 processed in 5.70 seconds.\n",
      "total loss: 0.46172887086868286\n",
      "\tBatch 352/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 212.00750374794006\n",
      "\tBatch 352/400 processed in 5.71 seconds.\n",
      "total loss: 0.47191888093948364\n",
      "\tBatch 353/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 212.47942262887955\n",
      "\tBatch 353/400 processed in 6.15 seconds.\n",
      "total loss: 0.456637978553772\n",
      "\tBatch 354/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 212.93606060743332\n",
      "\tBatch 354/400 processed in 5.87 seconds.\n",
      "total loss: 0.4552225172519684\n",
      "\tBatch 355/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 213.3912831246853\n",
      "\tBatch 355/400 processed in 5.72 seconds.\n",
      "total loss: 0.46841949224472046\n",
      "\tBatch 356/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 213.85970261693\n",
      "\tBatch 356/400 processed in 5.73 seconds.\n",
      "total loss: 0.46650955080986023\n",
      "\tBatch 357/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 214.32621216773987\n",
      "\tBatch 357/400 processed in 5.70 seconds.\n",
      "total loss: 0.45451948046684265\n",
      "\tBatch 358/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 214.7807316482067\n",
      "\tBatch 358/400 processed in 5.70 seconds.\n",
      "total loss: 0.4427662491798401\n",
      "\tBatch 359/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 215.22349789738655\n",
      "\tBatch 359/400 processed in 5.84 seconds.\n",
      "total loss: 0.4745948612689972\n",
      "\tBatch 360/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 215.69809275865555\n",
      "\tBatch 360/400 processed in 5.74 seconds.\n",
      "total loss: 0.4676673412322998\n",
      "\tBatch 361/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 216.16576009988785\n",
      "\tBatch 361/400 processed in 5.72 seconds.\n",
      "total loss: 0.47501954436302185\n",
      "\tBatch 362/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 216.64077964425087\n",
      "\tBatch 362/400 processed in 5.74 seconds.\n",
      "total loss: 0.46364909410476685\n",
      "\tBatch 363/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 217.10442873835564\n",
      "\tBatch 363/400 processed in 5.72 seconds.\n",
      "total loss: 0.463787317276001\n",
      "\tBatch 364/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 217.56821605563164\n",
      "\tBatch 364/400 processed in 5.74 seconds.\n",
      "total loss: 0.4663507342338562\n",
      "\tBatch 365/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 218.0345667898655\n",
      "\tBatch 365/400 processed in 5.72 seconds.\n",
      "total loss: 0.46303433179855347\n",
      "\tBatch 366/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 218.49760112166405\n",
      "\tBatch 366/400 processed in 5.75 seconds.\n",
      "total loss: 0.4626152515411377\n",
      "\tBatch 367/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 218.96021637320518\n",
      "\tBatch 367/400 processed in 5.74 seconds.\n",
      "total loss: 0.4893209934234619\n",
      "\tBatch 368/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 219.44953736662865\n",
      "\tBatch 368/400 processed in 5.74 seconds.\n",
      "total loss: 0.47000694274902344\n",
      "\tBatch 369/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 219.91954430937767\n",
      "\tBatch 369/400 processed in 5.73 seconds.\n",
      "total loss: 0.4740600883960724\n",
      "\tBatch 370/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 220.39360439777374\n",
      "\tBatch 370/400 processed in 5.75 seconds.\n",
      "total loss: 0.46195125579833984\n",
      "\tBatch 371/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 220.85555565357208\n",
      "\tBatch 371/400 processed in 5.73 seconds.\n",
      "total loss: 0.4519081711769104\n",
      "\tBatch 372/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 221.307463824749\n",
      "\tBatch 372/400 processed in 5.74 seconds.\n",
      "total loss: 0.474811851978302\n",
      "\tBatch 373/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 221.7822756767273\n",
      "\tBatch 373/400 processed in 5.73 seconds.\n",
      "total loss: 0.47585999965667725\n",
      "\tBatch 374/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 222.25813567638397\n",
      "\tBatch 374/400 processed in 5.77 seconds.\n",
      "total loss: 0.48674285411834717\n",
      "\tBatch 375/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 222.74487853050232\n",
      "\tBatch 375/400 processed in 5.75 seconds.\n",
      "total loss: 0.4649083614349365\n",
      "\tBatch 376/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 223.20978689193726\n",
      "\tBatch 376/400 processed in 5.75 seconds.\n",
      "total loss: 0.5027632713317871\n",
      "\tBatch 377/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 223.71255016326904\n",
      "\tBatch 377/400 processed in 5.73 seconds.\n",
      "total loss: 0.46548715233802795\n",
      "\tBatch 378/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 224.17803731560707\n",
      "\tBatch 378/400 processed in 5.74 seconds.\n",
      "total loss: 0.489038348197937\n",
      "\tBatch 379/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 224.667075663805\n",
      "\tBatch 379/400 processed in 5.72 seconds.\n",
      "total loss: 0.47483551502227783\n",
      "\tBatch 380/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 225.14191117882729\n",
      "\tBatch 380/400 processed in 5.86 seconds.\n",
      "total loss: 0.4883192181587219\n",
      "\tBatch 381/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 225.630230396986\n",
      "\tBatch 381/400 processed in 5.72 seconds.\n",
      "total loss: 0.49221599102020264\n",
      "\tBatch 382/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 226.1224463880062\n",
      "\tBatch 382/400 processed in 5.73 seconds.\n",
      "total loss: 0.4987422227859497\n",
      "\tBatch 383/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 226.62118861079216\n",
      "\tBatch 383/400 processed in 5.68 seconds.\n",
      "total loss: 0.4571453034877777\n",
      "\tBatch 384/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 227.07833391427994\n",
      "\tBatch 384/400 processed in 5.72 seconds.\n",
      "total loss: 0.4993221163749695\n",
      "\tBatch 385/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 227.5776560306549\n",
      "\tBatch 385/400 processed in 5.86 seconds.\n",
      "total loss: 0.4729999303817749\n",
      "\tBatch 386/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 228.05065596103668\n",
      "\tBatch 386/400 processed in 5.73 seconds.\n",
      "total loss: 0.4758472442626953\n",
      "\tBatch 387/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 228.52650320529938\n",
      "\tBatch 387/400 processed in 5.71 seconds.\n",
      "total loss: 0.5005246996879578\n",
      "\tBatch 388/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 229.02702790498734\n",
      "\tBatch 388/400 processed in 5.73 seconds.\n",
      "total loss: 0.4955485761165619\n",
      "\tBatch 389/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 229.5225764811039\n",
      "\tBatch 389/400 processed in 5.72 seconds.\n",
      "total loss: 0.4938417673110962\n",
      "\tBatch 390/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 230.016418248415\n",
      "\tBatch 390/400 processed in 5.75 seconds.\n",
      "total loss: 0.5131567716598511\n",
      "\tBatch 391/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 230.52957502007484\n",
      "\tBatch 391/400 processed in 5.73 seconds.\n",
      "total loss: 0.49041223526000977\n",
      "\tBatch 392/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 231.01998725533485\n",
      "\tBatch 392/400 processed in 5.76 seconds.\n",
      "total loss: 0.4817778468132019\n",
      "\tBatch 393/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 231.50176510214806\n",
      "\tBatch 393/400 processed in 5.72 seconds.\n",
      "total loss: 0.4993453621864319\n",
      "\tBatch 394/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 232.0011104643345\n",
      "\tBatch 394/400 processed in 5.75 seconds.\n",
      "total loss: 0.4808272123336792\n",
      "\tBatch 395/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 232.48193767666817\n",
      "\tBatch 395/400 processed in 5.73 seconds.\n",
      "total loss: 0.45461103320121765\n",
      "\tBatch 396/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 232.93654870986938\n",
      "\tBatch 396/400 processed in 5.77 seconds.\n",
      "total loss: 0.4738399088382721\n",
      "\tBatch 397/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 233.41038861870766\n",
      "\tBatch 397/400 processed in 5.74 seconds.\n",
      "total loss: 0.4759352207183838\n",
      "\tBatch 398/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 233.88632383942604\n",
      "\tBatch 398/400 processed in 5.77 seconds.\n",
      "total loss: 0.48109951615333557\n",
      "\tBatch 399/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 234.36742335557938\n",
      "\tBatch 399/400 processed in 5.75 seconds.\n",
      "total loss: 0.5051989555358887\n",
      "\tBatch 400/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 234.87262231111526\n",
      "\tBatch 400/400 processed in 6.71 seconds.\n",
      "Epoch [1/100] completed in 2309.61 seconds, Avg Loss: 0.5871815557777882\n",
      "Epoch [1/100] validation completed, Avg Validation Loss: 0.23013286158442497\n",
      "Starting epoch 2/100\n",
      "total loss: 0.23255352675914764\n",
      "\tBatch 1/400, Forward pass done, starting backward pass.\n",
      "Epoch loss: 0.23255352675914764\n",
      "\tBatch 1/400 processed in 4.97 seconds.\n",
      "total loss: nan\n",
      "\tBatch 2/400, Forward pass done, starting backward pass.\n",
      "Warning: NaN detected in total_loss at batch 2, skipping backward pass.\n",
      "NaN detected, not adding to epoch_loss at batch 2\n",
      "Epoch loss: 0.23255352675914764\n",
      "\tBatch 2/400 processed in 3.45 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/parthaenv/lib/python3.10/site-packages/torch/nn/modules/conv.py:952: UserWarning: Plan failed with an OutOfMemoryError: CUDA out of memory. Tried to allocate 22.00 MiB. GPU  (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:924.)\n",
      "  return F.conv_transpose2d(\n",
      "/root/miniconda3/envs/parthaenv/lib/python3.10/site-packages/torch/nn/modules/conv.py:952: UserWarning: Plan failed with an OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU  (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:924.)\n",
      "  return F.conv_transpose2d(\n",
      "/root/miniconda3/envs/parthaenv/lib/python3.10/site-packages/torch/nn/modules/conv.py:952: UserWarning: Plan failed with an OutOfMemoryError: CUDA out of memory. Tried to allocate 42.00 MiB. GPU  (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:924.)\n",
      "  return F.conv_transpose2d(\n",
      "/root/miniconda3/envs/parthaenv/lib/python3.10/site-packages/torch/nn/modules/conv.py:952: UserWarning: Plan failed with an OutOfMemoryError: CUDA out of memory. Tried to allocate 116.00 MiB. GPU  (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:924.)\n",
      "  return F.conv_transpose2d(\n",
      "/root/miniconda3/envs/parthaenv/lib/python3.10/site-packages/torch/nn/modules/conv.py:952: UserWarning: Plan failed with an OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU  (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:924.)\n",
      "  return F.conv_transpose2d(\n",
      "/root/miniconda3/envs/parthaenv/lib/python3.10/site-packages/torch/nn/modules/conv.py:952: UserWarning: Plan failed with an OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU  (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:924.)\n",
      "  return F.conv_transpose2d(\n",
      "/root/miniconda3/envs/parthaenv/lib/python3.10/site-packages/torch/nn/modules/conv.py:952: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv_transpose2d(\n",
      "/root/miniconda3/envs/parthaenv/lib/python3.10/site-packages/torch/nn/modules/conv.py:952: UserWarning: Plan failed with an OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU  (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:924.)\n",
      "  return F.conv_transpose2d(\n",
      "/root/miniconda3/envs/parthaenv/lib/python3.10/site-packages/torch/nn/modules/conv.py:952: UserWarning: Plan failed with an OutOfMemoryError: CUDA out of memory. Tried to allocate 296.00 MiB. GPU  (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:924.)\n",
      "  return F.conv_transpose2d(\n",
      "/root/miniconda3/envs/parthaenv/lib/python3.10/site-packages/torch/nn/modules/conv.py:952: UserWarning: Plan failed with an OutOfMemoryError: CUDA out of memory. Tried to allocate 362.00 MiB. GPU  (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:924.)\n",
      "  return F.conv_transpose2d(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m loss_weights \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreconstruction\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmi_penalty\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m50.0\u001b[39m}  \u001b[38;5;66;03m# Adjust the weights as needed\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_loader, device, num_epochs, loss_weights, accumulation_steps, checkpoint_path)\u001b[0m\n\u001b[1;32m     28\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():  \u001b[38;5;66;03m# Enable automatic mixed precision\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     reconstructed_x, f_x, S_x  \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     reconstruction_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmse_loss(reconstructed_x, data)\n\u001b[1;32m     33\u001b[0m     mi \u001b[38;5;241m=\u001b[39m echo_loss(S_x)\n",
      "File \u001b[0;32m~/miniconda3/envs/parthaenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/parthaenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 50\u001b[0m, in \u001b[0;36mEchoModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m x_T \u001b[38;5;241m=\u001b[39m sqrt_alpha_T \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m+\u001b[39m sqrt_one_minus_alpha_T \u001b[38;5;241m*\u001b[39m z\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Perform the reconstruction process using Algorithm 2\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m reconstructed_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_T\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reconstructed_x, f_x, sx_matrix\n",
      "Cell \u001b[0;32mIn[8], line 63\u001b[0m, in \u001b[0;36mEchoModel.reconstruct\u001b[0;34m(self, x_T)\u001b[0m\n\u001b[1;32m     60\u001b[0m sqrt_one_minus_alpha_s \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha[s])\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Estimate the original image using the decoder\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m x_0_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Calculate the estimated noise using Eq. (3)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m z_hat \u001b[38;5;241m=\u001b[39m (x_s \u001b[38;5;241m-\u001b[39m sqrt_alpha_s \u001b[38;5;241m*\u001b[39m x_0_hat) \u001b[38;5;241m/\u001b[39m sqrt_one_minus_alpha_s\n",
      "File \u001b[0;32m~/miniconda3/envs/parthaenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/parthaenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 39\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, timestep_emb[:, :, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m])], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeconv1(x))\n\u001b[0;32m---> 39\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeconv3(x))\n\u001b[1;32m     41\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeconv4(x))\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "# Define the input shape, latent dimensions, and output shape\n",
    "input_shape = (1, 32, 32)  # Shape for grayscale CIFAR-10 (1 channel, 32x32 images)\n",
    "latent_dims = [32, 64, 128, 256, 512]  # Updated latent dimensions\n",
    "output_shape = (1, 32, 32)  # Shape for grayscale CIFAR-10\n",
    "\n",
    "# Create an instance of the EchoModel\n",
    "model = EchoModel(input_shape, latent_dims, output_shape).to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Define the number of epochs and loss weights\n",
    "num_epochs = 100\n",
    "loss_weights = {'reconstruction': 1.0, 'mi_penalty': 50.0}  # Adjust the weights as needed\n",
    "\n",
    "# Train the model\n",
    "trained_model = train(model, optimizer, train_loader, device, num_epochs, loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qmfrva2uRuvu",
   "metadata": {
    "id": "qmfrva2uRuvu"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, filename=\"checkpoint.pth\"):\n",
    "    \"\"\"Saves the model and optimizer state at the specified path.\"\"\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, filename)\n",
    "    print(f\"Checkpoint saved at epoch {epoch} to {filename}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
