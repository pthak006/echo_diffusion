{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec619f54-5dc4-4292-8727-066008d1e8ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ec619f54-5dc4-4292-8727-066008d1e8ed",
    "outputId": "5b117faa-8a74-4912-bc05-093f2d4ba270"
   },
   "outputs": [],
   "source": [
    "# Import necessary PyTorch libraries\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    " \n",
    "# Additional libraries for visualization and utilities\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c09ae2f-28f2-4a9a-80e0-5a4cab2902e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    \"\"\"Selects the best available device for PyTorch computations.\n",
    "\n",
    "    Returns:\n",
    "        torch.device: The selected device.\n",
    "    \"\"\"\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "device = get_device()\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "306efd2f-3f3b-418a-ab09-bf1173151f80",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "306efd2f-3f3b-418a-ab09-bf1173151f80",
    "outputId": "807d84ee-b69b-4434-e510-e00e6a21255c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Data loaders created for training and validation.\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Grayscale\n",
    "\n",
    "# Define transformations: Convert to grayscale, resize if needed, and normalize the data\n",
    "transform = Compose([\n",
    "    Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    ToTensor(),\n",
    "    Normalize((0.5,), (0.5,))  # Normalize with single channel\n",
    "])\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Splitting dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "print(\"Data loaders created for training and validation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "TenNg_Ywlv3n",
   "metadata": {
    "id": "TenNg_Ywlv3n"
   },
   "outputs": [],
   "source": [
    "def get_timestep_embedding(timesteps, embedding_dim):\n",
    "    \"\"\"\n",
    "    This matches the implementation in Denoising Diffusion Probabilistic Models:\n",
    "    From Fairseq.\n",
    "    Build sinusoidal embeddings.\n",
    "    This matches the implementation in tensor2tensor, but differs slightly\n",
    "    from the description in Section 3.5 of \"Attention Is All You Need\".\n",
    "    \"\"\"\n",
    "    assert len(timesteps.shape) == 1\n",
    "\n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = math.log(10000) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
    "    emb = emb.to(device=timesteps.device)\n",
    "    emb = timesteps.float()[:, None] * emb[None, :]\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "    if embedding_dim % 2 == 1:  # zero pad\n",
    "        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f08a5ac-95ad-45ae-a36e-f96dbd3f1e99",
   "metadata": {
    "id": "1f08a5ac-95ad-45ae-a36e-f96dbd3f1e99"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dims, output_shape, timestep_dim=128):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.latent_dims = latent_dims\n",
    "        self.output_shape = output_shape\n",
    "        self.timestep_dim = timestep_dim\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, latent_dims[0], kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(latent_dims[0], latent_dims[1], kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(latent_dims[1], latent_dims[2], kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(latent_dims[2], latent_dims[3], kernel_size=3, stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv2d(latent_dims[3], latent_dims[4], kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.timestep_mlp = nn.Sequential(\n",
    "            nn.Linear(timestep_dim, latent_dims[4]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_dims[4], latent_dims[4]),\n",
    "        )\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose2d(latent_dims[4] * 2, latent_dims[3], kernel_size=3, stride=1, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(latent_dims[3], latent_dims[2], kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(latent_dims[2], latent_dims[1], kernel_size=3, stride=1, padding=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(latent_dims[1], latent_dims[0], kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv5 = nn.ConvTranspose2d(latent_dims[0], output_shape[0], kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        timestep_emb = get_timestep_embedding(t, self.timestep_dim)\n",
    "        timestep_emb = self.timestep_mlp(timestep_emb)\n",
    "\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = torch.relu(self.conv5(x))\n",
    "\n",
    "        x = torch.cat([x, timestep_emb[:, :, None, None].repeat(1, 1, x.shape[2], x.shape[3])], dim=1)\n",
    "\n",
    "        x = torch.relu(self.deconv1(x))\n",
    "        x = torch.relu(self.deconv2(x))\n",
    "        x = torch.relu(self.deconv3(x))\n",
    "        x = torch.relu(self.deconv4(x))\n",
    "        x = torch.sigmoid(self.deconv5(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a77391b7-6374-4bb7-acd9-577f51a68706",
   "metadata": {
    "id": "a77391b7-6374-4bb7-acd9-577f51a68706"
   },
   "outputs": [],
   "source": [
    "class EchoModel(nn.Module):\n",
    "    def __init__(self, input_shape, latent_dims, output_shape, T=1000, batch_size=100):\n",
    "        super(EchoModel, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.latent_dims = latent_dims\n",
    "        self.output_shape = output_shape\n",
    "        self.T = T\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # self.encoder = Encoder(input_shape, latent_dims)\n",
    "        self.decoder = Decoder(latent_dims, output_shape)\n",
    "\n",
    "        # Define the noise schedule\n",
    "        self.alpha = self.create_noise_schedule(T)\n",
    "\n",
    "    def create_noise_schedule(self, T):\n",
    "        alpha = torch.linspace(0.9999, 1e-5, T)\n",
    "        return alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Calculate z as a gaussian noise tensor\n",
    "        z = torch.randn(self.batch_size,1, 32, 32).to(device)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Retrieve noise scheduler alpha_T\n",
    "        alpha_T = self.alpha[-1]\n",
    "\n",
    "        # Calculate square root alphas\n",
    "        sqrt_alpha_T = torch.sqrt(alpha_T)\n",
    "        sqrt_one_minus_alpha_T = torch.sqrt(1 - alpha_T)\n",
    "        \n",
    "        # Perform the weighted sum\n",
    "        x_T = sqrt_alpha_T * x + sqrt_one_minus_alpha_T * z\n",
    "\n",
    "        del z\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        # Perform the reconstruction process using Algorithm 2\n",
    "        reconstructed_x = self.reconstruct(x_T)\n",
    "        torch.cuda.empty_cache()\n",
    "        return reconstructed_x\n",
    "\n",
    "    def reconstruct(self, x_T):\n",
    "        x_s = x_T\n",
    "        x_0_hat = x_T\n",
    "        for s in range(self.T-1, 0, -1):\n",
    "            t = torch.tensor([s] * x_T.size(0), dtype=torch.long).to(x_T.device)\n",
    "            sqrt_alpha_s = torch.sqrt(self.alpha[s])\n",
    "            sqrt_one_minus_alpha_s = torch.sqrt(1 - self.alpha[s])\n",
    "\n",
    "            # Estimate the original image using the decoder\n",
    "            x_0_hat = self.decoder(x_s, t)\n",
    "\n",
    "            # Calculate the estimated noise using Eq. (3)\n",
    "            z_hat = (x_s - sqrt_alpha_s * x_0_hat) / sqrt_one_minus_alpha_s\n",
    "\n",
    "            # Calculate D(x_0_hat, s) and D(x_0_hat, s-1) using Eq. (5) and (6)\n",
    "            D_x_0_hat_s = sqrt_alpha_s * x_0_hat + sqrt_one_minus_alpha_s * z_hat\n",
    "            D_x_0_hat_s_minus_1 = torch.sqrt(self.alpha[s-1]) * x_0_hat + torch.sqrt(1 - self.alpha[s-1]) * z_hat\n",
    "\n",
    "            # Update x_s using Eq. (7)\n",
    "            x_s = x_s - D_x_0_hat_s + D_x_0_hat_s_minus_1\n",
    "\n",
    "        return x_0_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c466c51d-3b41-4e11-bdfd-39b3d10ed2c9",
   "metadata": {
    "id": "c466c51d-3b41-4e11-bdfd-39b3d10ed2c9"
   },
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import time  # Importing time to log the duration\n",
    "\n",
    "\n",
    "\n",
    "def train(model, optimizer, train_loader, device, num_epochs, accumulation_steps=2, checkpoint_path=\"checkpoint.pth\"):\n",
    "    model.train()\n",
    "    scaler = GradScaler()  # Initialize the gradient scaler\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        # optimizer.zero_grad()  # Move optimizer.zero_grad() outside the batch loop for gradient accumulation\n",
    "        epoch_start_time = time.time()  # Time tracking for the epoch\n",
    "\n",
    "        print(f\"Starting epoch {epoch+1}/{num_epochs}\")\n",
    "        for batch_idx, (data, _) in enumerate(train_loader):\n",
    "            batch_start_time = time.time()  # Time tracking for the batch\n",
    "            data = data.to(device)\n",
    "\n",
    "            with autocast():  # Enable automatic mixed precision\n",
    "                reconstructed_x = model(data)\n",
    "                total_loss = nn.functional.mse_loss(reconstructed_x, data)\n",
    "                print(f\"total loss: {total_loss}\")\n",
    "            # Log before the backward pass\n",
    "            print(f\"\\tBatch {batch_idx+1}/{len(train_loader)}, Forward pass done, starting backward pass.\")\n",
    "\n",
    "            # Scale the loss, but don't call optimizer.step() yet\n",
    "            if not torch.isnan(total_loss).any():\n",
    "                scaler.scale(total_loss).backward()\n",
    "            else:\n",
    "                print(f\"Warning: NaN detected in total_loss at batch {batch_idx+1}, skipping backward pass.\")\n",
    "\n",
    "\n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                scaler.step(optimizer)  # Only step the optimizer every `accumulation_steps`\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()  # Reset gradients only after accumulation\n",
    "\n",
    "            print(f\"total loss item: {total_loss.item()}\")\n",
    "\n",
    "            # Safe-guarding against NaN for epoch_loss\n",
    "            if not torch.isnan(total_loss).any():\n",
    "                epoch_loss += total_loss.item()\n",
    "            else:\n",
    "                print(f\"NaN detected, not adding to epoch_loss at batch {batch_idx+1}\")\n",
    "            print(f\"Epoch loss: {epoch_loss}\")\n",
    "\n",
    "            # Log after a batch is processed\n",
    "            print(f\"\\tBatch {batch_idx+1}/{len(train_loader)} processed in {time.time() - batch_start_time:.2f} seconds.\")\n",
    "\n",
    "        # Average loss after training for an epoch\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] completed in {time.time() - epoch_start_time:.2f} seconds, Avg Loss: {avg_loss}\")\n",
    "\n",
    "        # Validation phase\n",
    "        avg_val_loss = validate(model, val_loader, device)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] validation completed, Avg Validation Loss: {avg_val_loss}\")\n",
    "        \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "661ef76c-fb21-4fd4-b6e6-436524680219",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "661ef76c-fb21-4fd4-b6e6-436524680219",
    "outputId": "957794b4-bfe9-4884-ac9b-510743791893"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/100\n",
      "total loss: 0.5047832131385803\n",
      "\tBatch 1/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.5047832131385803\n",
      "Epoch loss: 0.5047832131385803\n",
      "\tBatch 1/400 processed in 4.62 seconds.\n",
      "total loss: 0.5407262444496155\n",
      "\tBatch 2/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.5407262444496155\n",
      "Epoch loss: 1.0455094575881958\n",
      "\tBatch 2/400 processed in 5.68 seconds.\n",
      "total loss: 0.4463938772678375\n",
      "\tBatch 3/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.4463938772678375\n",
      "Epoch loss: 1.4919033348560333\n",
      "\tBatch 3/400 processed in 5.58 seconds.\n",
      "total loss: 0.511258065700531\n",
      "\tBatch 4/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.511258065700531\n",
      "Epoch loss: 2.0031614005565643\n",
      "\tBatch 4/400 processed in 5.69 seconds.\n",
      "total loss: 0.33101943135261536\n",
      "\tBatch 5/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.33101943135261536\n",
      "Epoch loss: 2.3341808319091797\n",
      "\tBatch 5/400 processed in 5.61 seconds.\n",
      "total loss: 0.2961098849773407\n",
      "\tBatch 6/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2961098849773407\n",
      "Epoch loss: 2.6302907168865204\n",
      "\tBatch 6/400 processed in 5.57 seconds.\n",
      "total loss: 0.22438670694828033\n",
      "\tBatch 7/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22438670694828033\n",
      "Epoch loss: 2.8546774238348007\n",
      "\tBatch 7/400 processed in 5.56 seconds.\n",
      "total loss: 0.24680739641189575\n",
      "\tBatch 8/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24680739641189575\n",
      "Epoch loss: 3.1014848202466965\n",
      "\tBatch 8/400 processed in 5.57 seconds.\n",
      "total loss: 0.2453782558441162\n",
      "\tBatch 9/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2453782558441162\n",
      "Epoch loss: 3.3468630760908127\n",
      "\tBatch 9/400 processed in 5.59 seconds.\n",
      "total loss: 0.22012148797512054\n",
      "\tBatch 10/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22012148797512054\n",
      "Epoch loss: 3.5669845640659332\n",
      "\tBatch 10/400 processed in 5.58 seconds.\n",
      "total loss: 0.22461695969104767\n",
      "\tBatch 11/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22461695969104767\n",
      "Epoch loss: 3.791601523756981\n",
      "\tBatch 11/400 processed in 5.69 seconds.\n",
      "total loss: 0.24501211941242218\n",
      "\tBatch 12/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24501211941242218\n",
      "Epoch loss: 4.036613643169403\n",
      "\tBatch 12/400 processed in 5.64 seconds.\n",
      "total loss: 0.2393861711025238\n",
      "\tBatch 13/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2393861711025238\n",
      "Epoch loss: 4.275999814271927\n",
      "\tBatch 13/400 processed in 5.55 seconds.\n",
      "total loss: 0.22817949950695038\n",
      "\tBatch 14/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22817949950695038\n",
      "Epoch loss: 4.504179313778877\n",
      "\tBatch 14/400 processed in 5.60 seconds.\n",
      "total loss: 0.22751420736312866\n",
      "\tBatch 15/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22751420736312866\n",
      "Epoch loss: 4.731693521142006\n",
      "\tBatch 15/400 processed in 5.65 seconds.\n",
      "total loss: 0.22147682309150696\n",
      "\tBatch 16/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22147682309150696\n",
      "Epoch loss: 4.953170344233513\n",
      "\tBatch 16/400 processed in 5.79 seconds.\n",
      "total loss: 0.22462928295135498\n",
      "\tBatch 17/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22462928295135498\n",
      "Epoch loss: 5.177799627184868\n",
      "\tBatch 17/400 processed in 5.61 seconds.\n",
      "total loss: 0.2406211793422699\n",
      "\tBatch 18/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2406211793422699\n",
      "Epoch loss: 5.418420806527138\n",
      "\tBatch 18/400 processed in 5.58 seconds.\n",
      "total loss: 0.24901710450649261\n",
      "\tBatch 19/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24901710450649261\n",
      "Epoch loss: 5.66743791103363\n",
      "\tBatch 19/400 processed in 5.66 seconds.\n",
      "total loss: 0.2509933114051819\n",
      "\tBatch 20/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2509933114051819\n",
      "Epoch loss: 5.918431222438812\n",
      "\tBatch 20/400 processed in 5.65 seconds.\n",
      "total loss: 0.2048691064119339\n",
      "\tBatch 21/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2048691064119339\n",
      "Epoch loss: 6.123300328850746\n",
      "\tBatch 21/400 processed in 5.59 seconds.\n",
      "total loss: 0.24656565487384796\n",
      "\tBatch 22/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24656565487384796\n",
      "Epoch loss: 6.369865983724594\n",
      "\tBatch 22/400 processed in 5.59 seconds.\n",
      "total loss: 0.22816890478134155\n",
      "\tBatch 23/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22816890478134155\n",
      "Epoch loss: 6.598034888505936\n",
      "\tBatch 23/400 processed in 5.63 seconds.\n",
      "total loss: 0.25876787304878235\n",
      "\tBatch 24/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.25876787304878235\n",
      "Epoch loss: 6.856802761554718\n",
      "\tBatch 24/400 processed in 5.58 seconds.\n",
      "total loss: 0.23407931625843048\n",
      "\tBatch 25/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23407931625843048\n",
      "Epoch loss: 7.0908820778131485\n",
      "\tBatch 25/400 processed in 5.62 seconds.\n",
      "total loss: 0.2271319180727005\n",
      "\tBatch 26/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2271319180727005\n",
      "Epoch loss: 7.318013995885849\n",
      "\tBatch 26/400 processed in 5.60 seconds.\n",
      "total loss: 0.22285768389701843\n",
      "\tBatch 27/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22285768389701843\n",
      "Epoch loss: 7.540871679782867\n",
      "\tBatch 27/400 processed in 5.72 seconds.\n",
      "total loss: 0.22700755298137665\n",
      "\tBatch 28/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22700755298137665\n",
      "Epoch loss: 7.767879232764244\n",
      "\tBatch 28/400 processed in 5.58 seconds.\n",
      "total loss: 0.22881434857845306\n",
      "\tBatch 29/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22881434857845306\n",
      "Epoch loss: 7.996693581342697\n",
      "\tBatch 29/400 processed in 5.56 seconds.\n",
      "total loss: 0.23807136714458466\n",
      "\tBatch 30/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23807136714458466\n",
      "Epoch loss: 8.234764948487282\n",
      "\tBatch 30/400 processed in 7.47 seconds.\n",
      "total loss: 0.2205711305141449\n",
      "\tBatch 31/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2205711305141449\n",
      "Epoch loss: 8.455336079001427\n",
      "\tBatch 31/400 processed in 5.61 seconds.\n",
      "total loss: 0.21683937311172485\n",
      "\tBatch 32/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21683937311172485\n",
      "Epoch loss: 8.672175452113152\n",
      "\tBatch 32/400 processed in 5.81 seconds.\n",
      "total loss: 0.22649310529232025\n",
      "\tBatch 33/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22649310529232025\n",
      "Epoch loss: 8.898668557405472\n",
      "\tBatch 33/400 processed in 5.60 seconds.\n",
      "total loss: 0.263470321893692\n",
      "\tBatch 34/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.263470321893692\n",
      "Epoch loss: 9.162138879299164\n",
      "\tBatch 34/400 processed in 5.72 seconds.\n",
      "total loss: 0.2112392783164978\n",
      "\tBatch 35/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2112392783164978\n",
      "Epoch loss: 9.373378157615662\n",
      "\tBatch 35/400 processed in 5.60 seconds.\n",
      "total loss: 0.23894263803958893\n",
      "\tBatch 36/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23894263803958893\n",
      "Epoch loss: 9.61232079565525\n",
      "\tBatch 36/400 processed in 5.59 seconds.\n",
      "total loss: 0.2187255471944809\n",
      "\tBatch 37/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2187255471944809\n",
      "Epoch loss: 9.831046342849731\n",
      "\tBatch 37/400 processed in 5.67 seconds.\n",
      "total loss: 0.2327508181333542\n",
      "\tBatch 38/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2327508181333542\n",
      "Epoch loss: 10.063797160983086\n",
      "\tBatch 38/400 processed in 5.60 seconds.\n",
      "total loss: 0.2064124196767807\n",
      "\tBatch 39/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2064124196767807\n",
      "Epoch loss: 10.270209580659866\n",
      "\tBatch 39/400 processed in 5.63 seconds.\n",
      "total loss: 0.22395987808704376\n",
      "\tBatch 40/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22395987808704376\n",
      "Epoch loss: 10.49416945874691\n",
      "\tBatch 40/400 processed in 5.66 seconds.\n",
      "total loss: 0.23354408144950867\n",
      "\tBatch 41/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23354408144950867\n",
      "Epoch loss: 10.727713540196419\n",
      "\tBatch 41/400 processed in 5.71 seconds.\n",
      "total loss: 0.20128299295902252\n",
      "\tBatch 42/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.20128299295902252\n",
      "Epoch loss: 10.928996533155441\n",
      "\tBatch 42/400 processed in 5.83 seconds.\n",
      "total loss: 0.22747795283794403\n",
      "\tBatch 43/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22747795283794403\n",
      "Epoch loss: 11.156474485993385\n",
      "\tBatch 43/400 processed in 5.59 seconds.\n",
      "total loss: 0.2496957778930664\n",
      "\tBatch 44/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2496957778930664\n",
      "Epoch loss: 11.406170263886452\n",
      "\tBatch 44/400 processed in 5.62 seconds.\n",
      "total loss: 0.22833816707134247\n",
      "\tBatch 45/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22833816707134247\n",
      "Epoch loss: 11.634508430957794\n",
      "\tBatch 45/400 processed in 5.57 seconds.\n",
      "total loss: 0.24666884541511536\n",
      "\tBatch 46/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24666884541511536\n",
      "Epoch loss: 11.88117727637291\n",
      "\tBatch 46/400 processed in 5.59 seconds.\n",
      "total loss: 0.25277602672576904\n",
      "\tBatch 47/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.25277602672576904\n",
      "Epoch loss: 12.133953303098679\n",
      "\tBatch 47/400 processed in 5.60 seconds.\n",
      "total loss: 0.23844750225543976\n",
      "\tBatch 48/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23844750225543976\n",
      "Epoch loss: 12.372400805354118\n",
      "\tBatch 48/400 processed in 5.58 seconds.\n",
      "total loss: 0.2379045933485031\n",
      "\tBatch 49/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2379045933485031\n",
      "Epoch loss: 12.610305398702621\n",
      "\tBatch 49/400 processed in 5.57 seconds.\n",
      "total loss: 0.22612710297107697\n",
      "\tBatch 50/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22612710297107697\n",
      "Epoch loss: 12.836432501673698\n",
      "\tBatch 50/400 processed in 5.71 seconds.\n",
      "total loss: 0.2179282307624817\n",
      "\tBatch 51/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2179282307624817\n",
      "Epoch loss: 13.05436073243618\n",
      "\tBatch 51/400 processed in 5.58 seconds.\n",
      "total loss: 0.26189014315605164\n",
      "\tBatch 52/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.26189014315605164\n",
      "Epoch loss: 13.316250875592232\n",
      "\tBatch 52/400 processed in 5.57 seconds.\n",
      "total loss: 0.215688094496727\n",
      "\tBatch 53/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.215688094496727\n",
      "Epoch loss: 13.531938970088959\n",
      "\tBatch 53/400 processed in 5.67 seconds.\n",
      "total loss: 0.2216758280992508\n",
      "\tBatch 54/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2216758280992508\n",
      "Epoch loss: 13.75361479818821\n",
      "\tBatch 54/400 processed in 5.59 seconds.\n",
      "total loss: 0.21390166878700256\n",
      "\tBatch 55/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21390166878700256\n",
      "Epoch loss: 13.967516466975212\n",
      "\tBatch 55/400 processed in 5.58 seconds.\n",
      "total loss: 0.2221704125404358\n",
      "\tBatch 56/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2221704125404358\n",
      "Epoch loss: 14.189686879515648\n",
      "\tBatch 56/400 processed in 5.58 seconds.\n",
      "total loss: 0.23200175166130066\n",
      "\tBatch 57/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23200175166130066\n",
      "Epoch loss: 14.421688631176949\n",
      "\tBatch 57/400 processed in 5.59 seconds.\n",
      "total loss: 0.21403348445892334\n",
      "\tBatch 58/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21403348445892334\n",
      "Epoch loss: 14.635722115635872\n",
      "\tBatch 58/400 processed in 5.86 seconds.\n",
      "total loss: 0.2281855195760727\n",
      "\tBatch 59/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2281855195760727\n",
      "Epoch loss: 14.863907635211945\n",
      "\tBatch 59/400 processed in 5.59 seconds.\n",
      "total loss: 0.22878114879131317\n",
      "\tBatch 60/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22878114879131317\n",
      "Epoch loss: 15.092688784003258\n",
      "\tBatch 60/400 processed in 5.56 seconds.\n",
      "total loss: 0.23631934821605682\n",
      "\tBatch 61/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23631934821605682\n",
      "Epoch loss: 15.329008132219315\n",
      "\tBatch 61/400 processed in 5.58 seconds.\n",
      "total loss: 0.21773174405097961\n",
      "\tBatch 62/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21773174405097961\n",
      "Epoch loss: 15.546739876270294\n",
      "\tBatch 62/400 processed in 5.60 seconds.\n",
      "total loss: 0.229531928896904\n",
      "\tBatch 63/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.229531928896904\n",
      "Epoch loss: 15.776271805167198\n",
      "\tBatch 63/400 processed in 5.57 seconds.\n",
      "total loss: 0.23298047482967377\n",
      "\tBatch 64/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23298047482967377\n",
      "Epoch loss: 16.009252279996872\n",
      "\tBatch 64/400 processed in 5.57 seconds.\n",
      "total loss: 0.22376884520053864\n",
      "\tBatch 65/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22376884520053864\n",
      "Epoch loss: 16.23302112519741\n",
      "\tBatch 65/400 processed in 5.60 seconds.\n",
      "total loss: 0.21949870884418488\n",
      "\tBatch 66/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21949870884418488\n",
      "Epoch loss: 16.452519834041595\n",
      "\tBatch 66/400 processed in 5.73 seconds.\n",
      "total loss: 0.22175472974777222\n",
      "\tBatch 67/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22175472974777222\n",
      "Epoch loss: 16.674274563789368\n",
      "\tBatch 67/400 processed in 5.62 seconds.\n",
      "total loss: 0.2201378047466278\n",
      "\tBatch 68/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2201378047466278\n",
      "Epoch loss: 16.894412368535995\n",
      "\tBatch 68/400 processed in 5.65 seconds.\n",
      "total loss: 0.23622488975524902\n",
      "\tBatch 69/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23622488975524902\n",
      "Epoch loss: 17.130637258291245\n",
      "\tBatch 69/400 processed in 5.67 seconds.\n",
      "total loss: 0.2223293036222458\n",
      "\tBatch 70/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2223293036222458\n",
      "Epoch loss: 17.35296656191349\n",
      "\tBatch 70/400 processed in 5.61 seconds.\n",
      "total loss: 0.25928014516830444\n",
      "\tBatch 71/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.25928014516830444\n",
      "Epoch loss: 17.612246707081795\n",
      "\tBatch 71/400 processed in 5.61 seconds.\n",
      "total loss: 0.24059619009494781\n",
      "\tBatch 72/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24059619009494781\n",
      "Epoch loss: 17.852842897176743\n",
      "\tBatch 72/400 processed in 5.58 seconds.\n",
      "total loss: 0.2200382500886917\n",
      "\tBatch 73/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2200382500886917\n",
      "Epoch loss: 18.072881147265434\n",
      "\tBatch 73/400 processed in 5.73 seconds.\n",
      "total loss: 0.2272239476442337\n",
      "\tBatch 74/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2272239476442337\n",
      "Epoch loss: 18.300105094909668\n",
      "\tBatch 74/400 processed in 5.75 seconds.\n",
      "total loss: 0.20454056560993195\n",
      "\tBatch 75/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.20454056560993195\n",
      "Epoch loss: 18.5046456605196\n",
      "\tBatch 75/400 processed in 5.57 seconds.\n",
      "total loss: 0.23566576838493347\n",
      "\tBatch 76/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23566576838493347\n",
      "Epoch loss: 18.740311428904533\n",
      "\tBatch 76/400 processed in 5.62 seconds.\n",
      "total loss: 0.24996419250965118\n",
      "\tBatch 77/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24996419250965118\n",
      "Epoch loss: 18.990275621414185\n",
      "\tBatch 77/400 processed in 5.58 seconds.\n",
      "total loss: 0.23316101729869843\n",
      "\tBatch 78/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23316101729869843\n",
      "Epoch loss: 19.223436638712883\n",
      "\tBatch 78/400 processed in 5.70 seconds.\n",
      "total loss: 0.22143325209617615\n",
      "\tBatch 79/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22143325209617615\n",
      "Epoch loss: 19.44486989080906\n",
      "\tBatch 79/400 processed in 5.60 seconds.\n",
      "total loss: 0.2477564960718155\n",
      "\tBatch 80/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2477564960718155\n",
      "Epoch loss: 19.692626386880875\n",
      "\tBatch 80/400 processed in 5.69 seconds.\n",
      "total loss: 0.2318541705608368\n",
      "\tBatch 81/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2318541705608368\n",
      "Epoch loss: 19.92448055744171\n",
      "\tBatch 81/400 processed in 5.59 seconds.\n",
      "total loss: 0.2123139351606369\n",
      "\tBatch 82/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2123139351606369\n",
      "Epoch loss: 20.13679449260235\n",
      "\tBatch 82/400 processed in 5.58 seconds.\n",
      "total loss: 0.24740348756313324\n",
      "\tBatch 83/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24740348756313324\n",
      "Epoch loss: 20.38419798016548\n",
      "\tBatch 83/400 processed in 5.83 seconds.\n",
      "total loss: 0.2560020685195923\n",
      "\tBatch 84/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2560020685195923\n",
      "Epoch loss: 20.640200048685074\n",
      "\tBatch 84/400 processed in 5.66 seconds.\n",
      "total loss: 0.23678699135780334\n",
      "\tBatch 85/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23678699135780334\n",
      "Epoch loss: 20.876987040042877\n",
      "\tBatch 85/400 processed in 5.71 seconds.\n",
      "total loss: 0.2504960894584656\n",
      "\tBatch 86/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2504960894584656\n",
      "Epoch loss: 21.127483129501343\n",
      "\tBatch 86/400 processed in 5.60 seconds.\n",
      "total loss: 0.22354218363761902\n",
      "\tBatch 87/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22354218363761902\n",
      "Epoch loss: 21.351025313138962\n",
      "\tBatch 87/400 processed in 5.56 seconds.\n",
      "total loss: 0.2183755785226822\n",
      "\tBatch 88/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2183755785226822\n",
      "Epoch loss: 21.569400891661644\n",
      "\tBatch 88/400 processed in 5.68 seconds.\n",
      "total loss: 0.2480897158384323\n",
      "\tBatch 89/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2480897158384323\n",
      "Epoch loss: 21.817490607500076\n",
      "\tBatch 89/400 processed in 5.62 seconds.\n",
      "total loss: 0.25627586245536804\n",
      "\tBatch 90/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.25627586245536804\n",
      "Epoch loss: 22.073766469955444\n",
      "\tBatch 90/400 processed in 5.75 seconds.\n",
      "total loss: 0.24385306239128113\n",
      "\tBatch 91/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24385306239128113\n",
      "Epoch loss: 22.317619532346725\n",
      "\tBatch 91/400 processed in 5.60 seconds.\n",
      "total loss: 0.23817019164562225\n",
      "\tBatch 92/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23817019164562225\n",
      "Epoch loss: 22.555789723992348\n",
      "\tBatch 92/400 processed in 5.56 seconds.\n",
      "total loss: 0.22151648998260498\n",
      "\tBatch 93/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22151648998260498\n",
      "Epoch loss: 22.777306213974953\n",
      "\tBatch 93/400 processed in 5.56 seconds.\n",
      "total loss: 0.2366655319929123\n",
      "\tBatch 94/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2366655319929123\n",
      "Epoch loss: 23.013971745967865\n",
      "\tBatch 94/400 processed in 5.67 seconds.\n",
      "total loss: 0.2324601709842682\n",
      "\tBatch 95/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2324601709842682\n",
      "Epoch loss: 23.246431916952133\n",
      "\tBatch 95/400 processed in 6.10 seconds.\n",
      "total loss: 0.22668661177158356\n",
      "\tBatch 96/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22668661177158356\n",
      "Epoch loss: 23.473118528723717\n",
      "\tBatch 96/400 processed in 5.53 seconds.\n",
      "total loss: 0.2480488270521164\n",
      "\tBatch 97/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2480488270521164\n",
      "Epoch loss: 23.721167355775833\n",
      "\tBatch 97/400 processed in 5.59 seconds.\n",
      "total loss: 0.23499374091625214\n",
      "\tBatch 98/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23499374091625214\n",
      "Epoch loss: 23.956161096692085\n",
      "\tBatch 98/400 processed in 5.63 seconds.\n",
      "total loss: 0.2365109920501709\n",
      "\tBatch 99/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2365109920501709\n",
      "Epoch loss: 24.192672088742256\n",
      "\tBatch 99/400 processed in 5.61 seconds.\n",
      "total loss: 0.23612023890018463\n",
      "\tBatch 100/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23612023890018463\n",
      "Epoch loss: 24.42879232764244\n",
      "\tBatch 100/400 processed in 5.56 seconds.\n",
      "total loss: 0.23199333250522614\n",
      "\tBatch 101/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23199333250522614\n",
      "Epoch loss: 24.660785660147667\n",
      "\tBatch 101/400 processed in 5.58 seconds.\n",
      "total loss: 0.21261198818683624\n",
      "\tBatch 102/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21261198818683624\n",
      "Epoch loss: 24.873397648334503\n",
      "\tBatch 102/400 processed in 5.68 seconds.\n",
      "total loss: 0.2577190101146698\n",
      "\tBatch 103/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2577190101146698\n",
      "Epoch loss: 25.131116658449173\n",
      "\tBatch 103/400 processed in 5.58 seconds.\n",
      "total loss: 0.22822287678718567\n",
      "\tBatch 104/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22822287678718567\n",
      "Epoch loss: 25.35933953523636\n",
      "\tBatch 104/400 processed in 5.57 seconds.\n",
      "total loss: 0.1942211091518402\n",
      "\tBatch 105/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.1942211091518402\n",
      "Epoch loss: 25.5535606443882\n",
      "\tBatch 105/400 processed in 5.60 seconds.\n",
      "total loss: 0.2162037044763565\n",
      "\tBatch 106/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2162037044763565\n",
      "Epoch loss: 25.769764348864555\n",
      "\tBatch 106/400 processed in 5.63 seconds.\n",
      "total loss: 0.20466025173664093\n",
      "\tBatch 107/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.20466025173664093\n",
      "Epoch loss: 25.974424600601196\n",
      "\tBatch 107/400 processed in 5.63 seconds.\n",
      "total loss: 0.239122673869133\n",
      "\tBatch 108/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.239122673869133\n",
      "Epoch loss: 26.21354727447033\n",
      "\tBatch 108/400 processed in 5.58 seconds.\n",
      "total loss: 0.21805395185947418\n",
      "\tBatch 109/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21805395185947418\n",
      "Epoch loss: 26.431601226329803\n",
      "\tBatch 109/400 processed in 5.78 seconds.\n",
      "total loss: 0.23340491950511932\n",
      "\tBatch 110/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23340491950511932\n",
      "Epoch loss: 26.665006145834923\n",
      "\tBatch 110/400 processed in 5.69 seconds.\n",
      "total loss: 0.22933520376682281\n",
      "\tBatch 111/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22933520376682281\n",
      "Epoch loss: 26.894341349601746\n",
      "\tBatch 111/400 processed in 6.05 seconds.\n",
      "total loss: 0.2055720090866089\n",
      "\tBatch 112/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2055720090866089\n",
      "Epoch loss: 27.099913358688354\n",
      "\tBatch 112/400 processed in 5.71 seconds.\n",
      "total loss: 0.24078308045864105\n",
      "\tBatch 113/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24078308045864105\n",
      "Epoch loss: 27.340696439146996\n",
      "\tBatch 113/400 processed in 5.66 seconds.\n",
      "total loss: 0.24372638761997223\n",
      "\tBatch 114/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24372638761997223\n",
      "Epoch loss: 27.584422826766968\n",
      "\tBatch 114/400 processed in 5.64 seconds.\n",
      "total loss: 0.24050645530223846\n",
      "\tBatch 115/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24050645530223846\n",
      "Epoch loss: 27.824929282069206\n",
      "\tBatch 115/400 processed in 5.64 seconds.\n",
      "total loss: 0.2515255808830261\n",
      "\tBatch 116/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2515255808830261\n",
      "Epoch loss: 28.076454862952232\n",
      "\tBatch 116/400 processed in 5.69 seconds.\n",
      "total loss: 0.22709330916404724\n",
      "\tBatch 117/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22709330916404724\n",
      "Epoch loss: 28.30354817211628\n",
      "\tBatch 117/400 processed in 5.68 seconds.\n",
      "total loss: 0.2364787608385086\n",
      "\tBatch 118/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2364787608385086\n",
      "Epoch loss: 28.540026932954788\n",
      "\tBatch 118/400 processed in 5.69 seconds.\n",
      "total loss: 0.2186240702867508\n",
      "\tBatch 119/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2186240702867508\n",
      "Epoch loss: 28.75865100324154\n",
      "\tBatch 119/400 processed in 5.69 seconds.\n",
      "total loss: 0.22474151849746704\n",
      "\tBatch 120/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22474151849746704\n",
      "Epoch loss: 28.983392521739006\n",
      "\tBatch 120/400 processed in 5.64 seconds.\n",
      "total loss: 0.2378506064414978\n",
      "\tBatch 121/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2378506064414978\n",
      "Epoch loss: 29.221243128180504\n",
      "\tBatch 121/400 processed in 5.68 seconds.\n",
      "total loss: 0.2554754614830017\n",
      "\tBatch 122/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2554754614830017\n",
      "Epoch loss: 29.476718589663506\n",
      "\tBatch 122/400 processed in 5.68 seconds.\n",
      "total loss: 0.22516173124313354\n",
      "\tBatch 123/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22516173124313354\n",
      "Epoch loss: 29.70188032090664\n",
      "\tBatch 123/400 processed in 5.64 seconds.\n",
      "total loss: 0.2271655797958374\n",
      "\tBatch 124/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2271655797958374\n",
      "Epoch loss: 29.929045900702477\n",
      "\tBatch 124/400 processed in 5.64 seconds.\n",
      "total loss: 0.24973498284816742\n",
      "\tBatch 125/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24973498284816742\n",
      "Epoch loss: 30.178780883550644\n",
      "\tBatch 125/400 processed in 5.71 seconds.\n",
      "total loss: 0.22583335638046265\n",
      "\tBatch 126/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22583335638046265\n",
      "Epoch loss: 30.404614239931107\n",
      "\tBatch 126/400 processed in 5.68 seconds.\n",
      "total loss: 0.2242845743894577\n",
      "\tBatch 127/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2242845743894577\n",
      "Epoch loss: 30.628898814320564\n",
      "\tBatch 127/400 processed in 5.83 seconds.\n",
      "total loss: 0.2266017496585846\n",
      "\tBatch 128/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2266017496585846\n",
      "Epoch loss: 30.85550056397915\n",
      "\tBatch 128/400 processed in 5.62 seconds.\n",
      "total loss: 0.23034286499023438\n",
      "\tBatch 129/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23034286499023438\n",
      "Epoch loss: 31.085843428969383\n",
      "\tBatch 129/400 processed in 5.70 seconds.\n",
      "total loss: 0.2109106183052063\n",
      "\tBatch 130/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2109106183052063\n",
      "Epoch loss: 31.29675404727459\n",
      "\tBatch 130/400 processed in 5.68 seconds.\n",
      "total loss: 0.22254016995429993\n",
      "\tBatch 131/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22254016995429993\n",
      "Epoch loss: 31.51929421722889\n",
      "\tBatch 131/400 processed in 5.70 seconds.\n",
      "total loss: 0.2432861328125\n",
      "\tBatch 132/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2432861328125\n",
      "Epoch loss: 31.76258035004139\n",
      "\tBatch 132/400 processed in 5.84 seconds.\n",
      "total loss: 0.22586986422538757\n",
      "\tBatch 133/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22586986422538757\n",
      "Epoch loss: 31.988450214266777\n",
      "\tBatch 133/400 processed in 5.69 seconds.\n",
      "total loss: 0.22852759063243866\n",
      "\tBatch 134/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22852759063243866\n",
      "Epoch loss: 32.216977804899216\n",
      "\tBatch 134/400 processed in 5.68 seconds.\n",
      "total loss: 0.2105405181646347\n",
      "\tBatch 135/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2105405181646347\n",
      "Epoch loss: 32.42751832306385\n",
      "\tBatch 135/400 processed in 5.60 seconds.\n",
      "total loss: 0.21796001493930817\n",
      "\tBatch 136/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21796001493930817\n",
      "Epoch loss: 32.64547833800316\n",
      "\tBatch 136/400 processed in 5.59 seconds.\n",
      "total loss: 0.23997634649276733\n",
      "\tBatch 137/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23997634649276733\n",
      "Epoch loss: 32.885454684495926\n",
      "\tBatch 137/400 processed in 5.64 seconds.\n",
      "total loss: 0.23668311536312103\n",
      "\tBatch 138/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23668311536312103\n",
      "Epoch loss: 33.12213779985905\n",
      "\tBatch 138/400 processed in 5.66 seconds.\n",
      "total loss: 0.22690051794052124\n",
      "\tBatch 139/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22690051794052124\n",
      "Epoch loss: 33.34903831779957\n",
      "\tBatch 139/400 processed in 5.64 seconds.\n",
      "total loss: 0.22757141292095184\n",
      "\tBatch 140/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22757141292095184\n",
      "Epoch loss: 33.57660973072052\n",
      "\tBatch 140/400 processed in 5.62 seconds.\n",
      "total loss: 0.2391841858625412\n",
      "\tBatch 141/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2391841858625412\n",
      "Epoch loss: 33.81579391658306\n",
      "\tBatch 141/400 processed in 5.71 seconds.\n",
      "total loss: 0.23840022087097168\n",
      "\tBatch 142/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23840022087097168\n",
      "Epoch loss: 34.05419413745403\n",
      "\tBatch 142/400 processed in 5.62 seconds.\n",
      "total loss: 0.23089231550693512\n",
      "\tBatch 143/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23089231550693512\n",
      "Epoch loss: 34.28508645296097\n",
      "\tBatch 143/400 processed in 5.77 seconds.\n",
      "total loss: 0.25240135192871094\n",
      "\tBatch 144/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.25240135192871094\n",
      "Epoch loss: 34.53748780488968\n",
      "\tBatch 144/400 processed in 5.64 seconds.\n",
      "total loss: 0.2447933852672577\n",
      "\tBatch 145/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2447933852672577\n",
      "Epoch loss: 34.78228119015694\n",
      "\tBatch 145/400 processed in 5.64 seconds.\n",
      "total loss: 0.2095555067062378\n",
      "\tBatch 146/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2095555067062378\n",
      "Epoch loss: 34.991836696863174\n",
      "\tBatch 146/400 processed in 5.67 seconds.\n",
      "total loss: 0.2148110568523407\n",
      "\tBatch 147/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2148110568523407\n",
      "Epoch loss: 35.206647753715515\n",
      "\tBatch 147/400 processed in 5.63 seconds.\n",
      "total loss: 0.21081340312957764\n",
      "\tBatch 148/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21081340312957764\n",
      "Epoch loss: 35.41746115684509\n",
      "\tBatch 148/400 processed in 5.68 seconds.\n",
      "total loss: 0.23375630378723145\n",
      "\tBatch 149/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23375630378723145\n",
      "Epoch loss: 35.651217460632324\n",
      "\tBatch 149/400 processed in 5.71 seconds.\n",
      "total loss: 0.2296791821718216\n",
      "\tBatch 150/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2296791821718216\n",
      "Epoch loss: 35.880896642804146\n",
      "\tBatch 150/400 processed in 5.61 seconds.\n",
      "total loss: 0.21845361590385437\n",
      "\tBatch 151/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21845361590385437\n",
      "Epoch loss: 36.099350258708\n",
      "\tBatch 151/400 processed in 5.59 seconds.\n",
      "total loss: 0.22746695578098297\n",
      "\tBatch 152/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22746695578098297\n",
      "Epoch loss: 36.32681721448898\n",
      "\tBatch 152/400 processed in 5.64 seconds.\n",
      "total loss: 0.24567246437072754\n",
      "\tBatch 153/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24567246437072754\n",
      "Epoch loss: 36.57248967885971\n",
      "\tBatch 153/400 processed in 5.77 seconds.\n",
      "total loss: 0.23554423451423645\n",
      "\tBatch 154/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23554423451423645\n",
      "Epoch loss: 36.80803391337395\n",
      "\tBatch 154/400 processed in 5.60 seconds.\n",
      "total loss: 0.2048286348581314\n",
      "\tBatch 155/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2048286348581314\n",
      "Epoch loss: 37.01286254823208\n",
      "\tBatch 155/400 processed in 5.61 seconds.\n",
      "total loss: 0.21952976286411285\n",
      "\tBatch 156/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21952976286411285\n",
      "Epoch loss: 37.23239231109619\n",
      "\tBatch 156/400 processed in 5.70 seconds.\n",
      "total loss: 0.21086101233959198\n",
      "\tBatch 157/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21086101233959198\n",
      "Epoch loss: 37.44325332343578\n",
      "\tBatch 157/400 processed in 5.58 seconds.\n",
      "total loss: 0.2235601246356964\n",
      "\tBatch 158/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2235601246356964\n",
      "Epoch loss: 37.66681344807148\n",
      "\tBatch 158/400 processed in 5.58 seconds.\n",
      "total loss: 0.19212238490581512\n",
      "\tBatch 159/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.19212238490581512\n",
      "Epoch loss: 37.858935832977295\n",
      "\tBatch 159/400 processed in 5.65 seconds.\n",
      "total loss: 0.24700100719928741\n",
      "\tBatch 160/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24700100719928741\n",
      "Epoch loss: 38.10593684017658\n",
      "\tBatch 160/400 processed in 5.63 seconds.\n",
      "total loss: 0.22281891107559204\n",
      "\tBatch 161/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22281891107559204\n",
      "Epoch loss: 38.328755751252174\n",
      "\tBatch 161/400 processed in 5.55 seconds.\n",
      "total loss: 0.2210204303264618\n",
      "\tBatch 162/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2210204303264618\n",
      "Epoch loss: 38.549776181578636\n",
      "\tBatch 162/400 processed in 5.59 seconds.\n",
      "total loss: 0.22646808624267578\n",
      "\tBatch 163/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22646808624267578\n",
      "Epoch loss: 38.77624426782131\n",
      "\tBatch 163/400 processed in 5.68 seconds.\n",
      "total loss: 0.23399390280246735\n",
      "\tBatch 164/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23399390280246735\n",
      "Epoch loss: 39.01023817062378\n",
      "\tBatch 164/400 processed in 5.64 seconds.\n",
      "total loss: 0.2394113540649414\n",
      "\tBatch 165/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2394113540649414\n",
      "Epoch loss: 39.24964952468872\n",
      "\tBatch 165/400 processed in 5.56 seconds.\n",
      "total loss: 0.2471589595079422\n",
      "\tBatch 166/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2471589595079422\n",
      "Epoch loss: 39.49680848419666\n",
      "\tBatch 166/400 processed in 5.53 seconds.\n",
      "total loss: 0.25830328464508057\n",
      "\tBatch 167/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.25830328464508057\n",
      "Epoch loss: 39.75511176884174\n",
      "\tBatch 167/400 processed in 5.56 seconds.\n",
      "total loss: 0.2290785163640976\n",
      "\tBatch 168/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2290785163640976\n",
      "Epoch loss: 39.98419028520584\n",
      "\tBatch 168/400 processed in 5.56 seconds.\n",
      "total loss: 0.23728622496128082\n",
      "\tBatch 169/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23728622496128082\n",
      "Epoch loss: 40.22147651016712\n",
      "\tBatch 169/400 processed in 5.73 seconds.\n",
      "total loss: 0.24794520437717438\n",
      "\tBatch 170/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24794520437717438\n",
      "Epoch loss: 40.469421714544296\n",
      "\tBatch 170/400 processed in 5.54 seconds.\n",
      "total loss: 0.23946690559387207\n",
      "\tBatch 171/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23946690559387207\n",
      "Epoch loss: 40.70888862013817\n",
      "\tBatch 171/400 processed in 5.59 seconds.\n",
      "total loss: 0.22886793315410614\n",
      "\tBatch 172/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22886793315410614\n",
      "Epoch loss: 40.937756553292274\n",
      "\tBatch 172/400 processed in 5.66 seconds.\n",
      "total loss: 0.24044181406497955\n",
      "\tBatch 173/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24044181406497955\n",
      "Epoch loss: 41.178198367357254\n",
      "\tBatch 173/400 processed in 5.59 seconds.\n",
      "total loss: 0.2363373190164566\n",
      "\tBatch 174/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2363373190164566\n",
      "Epoch loss: 41.41453568637371\n",
      "\tBatch 174/400 processed in 6.09 seconds.\n",
      "total loss: 0.22384898364543915\n",
      "\tBatch 175/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22384898364543915\n",
      "Epoch loss: 41.63838467001915\n",
      "\tBatch 175/400 processed in 5.57 seconds.\n",
      "total loss: 0.25045710802078247\n",
      "\tBatch 176/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.25045710802078247\n",
      "Epoch loss: 41.88884177803993\n",
      "\tBatch 176/400 processed in 5.63 seconds.\n",
      "total loss: 0.24098125100135803\n",
      "\tBatch 177/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24098125100135803\n",
      "Epoch loss: 42.12982302904129\n",
      "\tBatch 177/400 processed in 5.68 seconds.\n",
      "total loss: 0.20611606538295746\n",
      "\tBatch 178/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.20611606538295746\n",
      "Epoch loss: 42.33593909442425\n",
      "\tBatch 178/400 processed in 5.65 seconds.\n",
      "total loss: 0.23304523527622223\n",
      "\tBatch 179/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23304523527622223\n",
      "Epoch loss: 42.56898432970047\n",
      "\tBatch 179/400 processed in 5.56 seconds.\n",
      "total loss: 0.23332542181015015\n",
      "\tBatch 180/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23332542181015015\n",
      "Epoch loss: 42.80230975151062\n",
      "\tBatch 180/400 processed in 5.82 seconds.\n",
      "total loss: 0.22291390597820282\n",
      "\tBatch 181/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22291390597820282\n",
      "Epoch loss: 43.02522365748882\n",
      "\tBatch 181/400 processed in 5.59 seconds.\n",
      "total loss: 0.23901306092739105\n",
      "\tBatch 182/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23901306092739105\n",
      "Epoch loss: 43.264236718416214\n",
      "\tBatch 182/400 processed in 5.56 seconds.\n",
      "total loss: 0.23192939162254333\n",
      "\tBatch 183/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23192939162254333\n",
      "Epoch loss: 43.49616611003876\n",
      "\tBatch 183/400 processed in 5.61 seconds.\n",
      "total loss: 0.2334848791360855\n",
      "\tBatch 184/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2334848791360855\n",
      "Epoch loss: 43.72965098917484\n",
      "\tBatch 184/400 processed in 5.63 seconds.\n",
      "total loss: 0.2362900823354721\n",
      "\tBatch 185/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2362900823354721\n",
      "Epoch loss: 43.965941071510315\n",
      "\tBatch 185/400 processed in 5.72 seconds.\n",
      "total loss: 0.23660503327846527\n",
      "\tBatch 186/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23660503327846527\n",
      "Epoch loss: 44.20254610478878\n",
      "\tBatch 186/400 processed in 5.53 seconds.\n",
      "total loss: 0.24628818035125732\n",
      "\tBatch 187/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24628818035125732\n",
      "Epoch loss: 44.44883428514004\n",
      "\tBatch 187/400 processed in 5.55 seconds.\n",
      "total loss: 0.21489784121513367\n",
      "\tBatch 188/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21489784121513367\n",
      "Epoch loss: 44.66373212635517\n",
      "\tBatch 188/400 processed in 5.62 seconds.\n",
      "total loss: 0.24941295385360718\n",
      "\tBatch 189/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24941295385360718\n",
      "Epoch loss: 44.91314508020878\n",
      "\tBatch 189/400 processed in 5.55 seconds.\n",
      "total loss: 0.2053968608379364\n",
      "\tBatch 190/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2053968608379364\n",
      "Epoch loss: 45.118541941046715\n",
      "\tBatch 190/400 processed in 5.57 seconds.\n",
      "total loss: 0.22940567135810852\n",
      "\tBatch 191/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22940567135810852\n",
      "Epoch loss: 45.34794761240482\n",
      "\tBatch 191/400 processed in 5.55 seconds.\n",
      "total loss: 0.21536050736904144\n",
      "\tBatch 192/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21536050736904144\n",
      "Epoch loss: 45.563308119773865\n",
      "\tBatch 192/400 processed in 5.58 seconds.\n",
      "total loss: 0.23669083416461945\n",
      "\tBatch 193/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23669083416461945\n",
      "Epoch loss: 45.799998953938484\n",
      "\tBatch 193/400 processed in 5.57 seconds.\n",
      "total loss: 0.23461124300956726\n",
      "\tBatch 194/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23461124300956726\n",
      "Epoch loss: 46.03461019694805\n",
      "\tBatch 194/400 processed in 5.58 seconds.\n",
      "total loss: 0.22074107825756073\n",
      "\tBatch 195/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22074107825756073\n",
      "Epoch loss: 46.25535127520561\n",
      "\tBatch 195/400 processed in 5.63 seconds.\n",
      "total loss: 0.2257871776819229\n",
      "\tBatch 196/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2257871776819229\n",
      "Epoch loss: 46.481138452887535\n",
      "\tBatch 196/400 processed in 5.68 seconds.\n",
      "total loss: 0.22328658401966095\n",
      "\tBatch 197/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22328658401966095\n",
      "Epoch loss: 46.704425036907196\n",
      "\tBatch 197/400 processed in 5.62 seconds.\n",
      "total loss: 0.2528458833694458\n",
      "\tBatch 198/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2528458833694458\n",
      "Epoch loss: 46.95727092027664\n",
      "\tBatch 198/400 processed in 5.56 seconds.\n",
      "total loss: 0.25144049525260925\n",
      "\tBatch 199/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.25144049525260925\n",
      "Epoch loss: 47.20871141552925\n",
      "\tBatch 199/400 processed in 5.88 seconds.\n",
      "total loss: 0.23326587677001953\n",
      "\tBatch 200/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23326587677001953\n",
      "Epoch loss: 47.44197729229927\n",
      "\tBatch 200/400 processed in 5.73 seconds.\n",
      "total loss: 0.22029335796833038\n",
      "\tBatch 201/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22029335796833038\n",
      "Epoch loss: 47.6622706502676\n",
      "\tBatch 201/400 processed in 5.98 seconds.\n",
      "total loss: 0.2450532615184784\n",
      "\tBatch 202/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2450532615184784\n",
      "Epoch loss: 47.90732391178608\n",
      "\tBatch 202/400 processed in 5.66 seconds.\n",
      "total loss: 0.22656461596488953\n",
      "\tBatch 203/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22656461596488953\n",
      "Epoch loss: 48.13388852775097\n",
      "\tBatch 203/400 processed in 5.63 seconds.\n",
      "total loss: 0.1995251178741455\n",
      "\tBatch 204/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.1995251178741455\n",
      "Epoch loss: 48.333413645625114\n",
      "\tBatch 204/400 processed in 5.65 seconds.\n",
      "total loss: 0.236399307847023\n",
      "\tBatch 205/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.236399307847023\n",
      "Epoch loss: 48.56981295347214\n",
      "\tBatch 205/400 processed in 5.64 seconds.\n",
      "total loss: 0.2128097265958786\n",
      "\tBatch 206/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2128097265958786\n",
      "Epoch loss: 48.782622680068016\n",
      "\tBatch 206/400 processed in 5.80 seconds.\n",
      "total loss: 0.22525501251220703\n",
      "\tBatch 207/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22525501251220703\n",
      "Epoch loss: 49.00787769258022\n",
      "\tBatch 207/400 processed in 5.58 seconds.\n",
      "total loss: 0.2099544256925583\n",
      "\tBatch 208/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2099544256925583\n",
      "Epoch loss: 49.21783211827278\n",
      "\tBatch 208/400 processed in 5.62 seconds.\n",
      "total loss: 0.2184709906578064\n",
      "\tBatch 209/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2184709906578064\n",
      "Epoch loss: 49.43630310893059\n",
      "\tBatch 209/400 processed in 5.59 seconds.\n",
      "total loss: 0.2305224984884262\n",
      "\tBatch 210/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2305224984884262\n",
      "Epoch loss: 49.666825607419014\n",
      "\tBatch 210/400 processed in 5.70 seconds.\n",
      "total loss: 0.23935005068778992\n",
      "\tBatch 211/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23935005068778992\n",
      "Epoch loss: 49.906175658106804\n",
      "\tBatch 211/400 processed in 5.58 seconds.\n",
      "total loss: 0.22730888426303864\n",
      "\tBatch 212/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22730888426303864\n",
      "Epoch loss: 50.13348454236984\n",
      "\tBatch 212/400 processed in 5.58 seconds.\n",
      "total loss: 0.22024382650852203\n",
      "\tBatch 213/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22024382650852203\n",
      "Epoch loss: 50.353728368878365\n",
      "\tBatch 213/400 processed in 5.58 seconds.\n",
      "total loss: 0.23990006744861603\n",
      "\tBatch 214/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23990006744861603\n",
      "Epoch loss: 50.59362843632698\n",
      "\tBatch 214/400 processed in 5.58 seconds.\n",
      "total loss: 0.2246171534061432\n",
      "\tBatch 215/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2246171534061432\n",
      "Epoch loss: 50.818245589733124\n",
      "\tBatch 215/400 processed in 5.55 seconds.\n",
      "total loss: 0.2350625991821289\n",
      "\tBatch 216/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2350625991821289\n",
      "Epoch loss: 51.05330818891525\n",
      "\tBatch 216/400 processed in 5.59 seconds.\n",
      "total loss: 0.2315060794353485\n",
      "\tBatch 217/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2315060794353485\n",
      "Epoch loss: 51.2848142683506\n",
      "\tBatch 217/400 processed in 5.73 seconds.\n",
      "total loss: 0.21864566206932068\n",
      "\tBatch 218/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21864566206932068\n",
      "Epoch loss: 51.50345993041992\n",
      "\tBatch 218/400 processed in 5.59 seconds.\n",
      "total loss: 0.24444381892681122\n",
      "\tBatch 219/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24444381892681122\n",
      "Epoch loss: 51.74790374934673\n",
      "\tBatch 219/400 processed in 5.57 seconds.\n",
      "total loss: 0.24551883339881897\n",
      "\tBatch 220/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24551883339881897\n",
      "Epoch loss: 51.99342258274555\n",
      "\tBatch 220/400 processed in 5.58 seconds.\n",
      "total loss: 0.22753283381462097\n",
      "\tBatch 221/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22753283381462097\n",
      "Epoch loss: 52.22095541656017\n",
      "\tBatch 221/400 processed in 5.55 seconds.\n",
      "total loss: 0.2372749298810959\n",
      "\tBatch 222/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2372749298810959\n",
      "Epoch loss: 52.45823034644127\n",
      "\tBatch 222/400 processed in 5.76 seconds.\n",
      "total loss: 0.2277156412601471\n",
      "\tBatch 223/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2277156412601471\n",
      "Epoch loss: 52.685945987701416\n",
      "\tBatch 223/400 processed in 5.61 seconds.\n",
      "total loss: 0.23296624422073364\n",
      "\tBatch 224/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23296624422073364\n",
      "Epoch loss: 52.91891223192215\n",
      "\tBatch 224/400 processed in 5.71 seconds.\n",
      "total loss: 0.23395873606204987\n",
      "\tBatch 225/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23395873606204987\n",
      "Epoch loss: 53.1528709679842\n",
      "\tBatch 225/400 processed in 5.57 seconds.\n",
      "total loss: 0.23072221875190735\n",
      "\tBatch 226/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23072221875190735\n",
      "Epoch loss: 53.38359318673611\n",
      "\tBatch 226/400 processed in 5.55 seconds.\n",
      "total loss: 0.213346928358078\n",
      "\tBatch 227/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.213346928358078\n",
      "Epoch loss: 53.596940115094185\n",
      "\tBatch 227/400 processed in 5.60 seconds.\n",
      "total loss: 0.2408759593963623\n",
      "\tBatch 228/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2408759593963623\n",
      "Epoch loss: 53.83781607449055\n",
      "\tBatch 228/400 processed in 5.58 seconds.\n",
      "total loss: 0.2236393690109253\n",
      "\tBatch 229/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2236393690109253\n",
      "Epoch loss: 54.06145544350147\n",
      "\tBatch 229/400 processed in 5.58 seconds.\n",
      "total loss: 0.21676842868328094\n",
      "\tBatch 230/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21676842868328094\n",
      "Epoch loss: 54.27822387218475\n",
      "\tBatch 230/400 processed in 5.57 seconds.\n",
      "total loss: 0.25365692377090454\n",
      "\tBatch 231/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.25365692377090454\n",
      "Epoch loss: 54.53188079595566\n",
      "\tBatch 231/400 processed in 5.64 seconds.\n",
      "total loss: 0.24064718186855316\n",
      "\tBatch 232/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24064718186855316\n",
      "Epoch loss: 54.77252797782421\n",
      "\tBatch 232/400 processed in 5.57 seconds.\n",
      "total loss: 0.24088725447654724\n",
      "\tBatch 233/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24088725447654724\n",
      "Epoch loss: 55.01341523230076\n",
      "\tBatch 233/400 processed in 5.77 seconds.\n",
      "total loss: 0.2368050366640091\n",
      "\tBatch 234/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2368050366640091\n",
      "Epoch loss: 55.25022026896477\n",
      "\tBatch 234/400 processed in 5.60 seconds.\n",
      "total loss: 0.2276727259159088\n",
      "\tBatch 235/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2276727259159088\n",
      "Epoch loss: 55.477892994880676\n",
      "\tBatch 235/400 processed in 5.62 seconds.\n",
      "total loss: 0.23787391185760498\n",
      "\tBatch 236/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23787391185760498\n",
      "Epoch loss: 55.71576690673828\n",
      "\tBatch 236/400 processed in 5.58 seconds.\n",
      "total loss: 0.21853290498256683\n",
      "\tBatch 237/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21853290498256683\n",
      "Epoch loss: 55.93429981172085\n",
      "\tBatch 237/400 processed in 5.60 seconds.\n",
      "total loss: 0.2312910407781601\n",
      "\tBatch 238/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2312910407781601\n",
      "Epoch loss: 56.16559085249901\n",
      "\tBatch 238/400 processed in 5.74 seconds.\n",
      "total loss: 0.21406184136867523\n",
      "\tBatch 239/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21406184136867523\n",
      "Epoch loss: 56.37965269386768\n",
      "\tBatch 239/400 processed in 5.61 seconds.\n",
      "total loss: 0.21811050176620483\n",
      "\tBatch 240/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21811050176620483\n",
      "Epoch loss: 56.59776319563389\n",
      "\tBatch 240/400 processed in 5.60 seconds.\n",
      "total loss: 0.2481732815504074\n",
      "\tBatch 241/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2481732815504074\n",
      "Epoch loss: 56.845936477184296\n",
      "\tBatch 241/400 processed in 5.69 seconds.\n",
      "total loss: 0.2235807627439499\n",
      "\tBatch 242/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2235807627439499\n",
      "Epoch loss: 57.069517239928246\n",
      "\tBatch 242/400 processed in 5.61 seconds.\n",
      "total loss: 0.2111012190580368\n",
      "\tBatch 243/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2111012190580368\n",
      "Epoch loss: 57.28061845898628\n",
      "\tBatch 243/400 processed in 7.03 seconds.\n",
      "total loss: 0.25705206394195557\n",
      "\tBatch 244/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.25705206394195557\n",
      "Epoch loss: 57.53767052292824\n",
      "\tBatch 244/400 processed in 5.62 seconds.\n",
      "total loss: 0.23255576193332672\n",
      "\tBatch 245/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23255576193332672\n",
      "Epoch loss: 57.770226284861565\n",
      "\tBatch 245/400 processed in 5.64 seconds.\n",
      "total loss: 0.24348582327365875\n",
      "\tBatch 246/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24348582327365875\n",
      "Epoch loss: 58.01371210813522\n",
      "\tBatch 246/400 processed in 5.59 seconds.\n",
      "total loss: 0.22678102552890778\n",
      "\tBatch 247/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22678102552890778\n",
      "Epoch loss: 58.24049313366413\n",
      "\tBatch 247/400 processed in 5.66 seconds.\n",
      "total loss: 0.2607320249080658\n",
      "\tBatch 248/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2607320249080658\n",
      "Epoch loss: 58.5012251585722\n",
      "\tBatch 248/400 processed in 5.61 seconds.\n",
      "total loss: 0.22191505134105682\n",
      "\tBatch 249/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22191505134105682\n",
      "Epoch loss: 58.723140209913254\n",
      "\tBatch 249/400 processed in 5.70 seconds.\n",
      "total loss: 0.22046910226345062\n",
      "\tBatch 250/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22046910226345062\n",
      "Epoch loss: 58.943609312176704\n",
      "\tBatch 250/400 processed in 5.60 seconds.\n",
      "total loss: 0.2390998750925064\n",
      "\tBatch 251/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2390998750925064\n",
      "Epoch loss: 59.18270918726921\n",
      "\tBatch 251/400 processed in 5.61 seconds.\n",
      "total loss: 0.22913573682308197\n",
      "\tBatch 252/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22913573682308197\n",
      "Epoch loss: 59.41184492409229\n",
      "\tBatch 252/400 processed in 5.54 seconds.\n",
      "total loss: 0.24045045673847198\n",
      "\tBatch 253/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24045045673847198\n",
      "Epoch loss: 59.652295380830765\n",
      "\tBatch 253/400 processed in 5.54 seconds.\n",
      "total loss: 0.23085437715053558\n",
      "\tBatch 254/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23085437715053558\n",
      "Epoch loss: 59.8831497579813\n",
      "\tBatch 254/400 processed in 5.62 seconds.\n",
      "total loss: 0.23226521909236908\n",
      "\tBatch 255/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23226521909236908\n",
      "Epoch loss: 60.11541497707367\n",
      "\tBatch 255/400 processed in 5.56 seconds.\n",
      "total loss: 0.20677775144577026\n",
      "\tBatch 256/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.20677775144577026\n",
      "Epoch loss: 60.32219272851944\n",
      "\tBatch 256/400 processed in 5.58 seconds.\n",
      "total loss: 0.24590003490447998\n",
      "\tBatch 257/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24590003490447998\n",
      "Epoch loss: 60.56809276342392\n",
      "\tBatch 257/400 processed in 5.56 seconds.\n",
      "total loss: 0.2194751650094986\n",
      "\tBatch 258/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2194751650094986\n",
      "Epoch loss: 60.78756792843342\n",
      "\tBatch 258/400 processed in 5.59 seconds.\n",
      "total loss: 0.23742499947547913\n",
      "\tBatch 259/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23742499947547913\n",
      "Epoch loss: 61.0249929279089\n",
      "\tBatch 259/400 processed in 5.72 seconds.\n",
      "total loss: 0.23656098544597626\n",
      "\tBatch 260/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23656098544597626\n",
      "Epoch loss: 61.261553913354874\n",
      "\tBatch 260/400 processed in 5.57 seconds.\n",
      "total loss: 0.242498978972435\n",
      "\tBatch 261/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.242498978972435\n",
      "Epoch loss: 61.50405289232731\n",
      "\tBatch 261/400 processed in 5.56 seconds.\n",
      "total loss: 0.24223287403583527\n",
      "\tBatch 262/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24223287403583527\n",
      "Epoch loss: 61.746285766363144\n",
      "\tBatch 262/400 processed in 5.53 seconds.\n",
      "total loss: 0.2363233119249344\n",
      "\tBatch 263/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2363233119249344\n",
      "Epoch loss: 61.98260907828808\n",
      "\tBatch 263/400 processed in 5.70 seconds.\n",
      "total loss: 0.2421777993440628\n",
      "\tBatch 264/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2421777993440628\n",
      "Epoch loss: 62.22478687763214\n",
      "\tBatch 264/400 processed in 5.57 seconds.\n",
      "total loss: 0.23224632441997528\n",
      "\tBatch 265/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23224632441997528\n",
      "Epoch loss: 62.457033202052116\n",
      "\tBatch 265/400 processed in 5.58 seconds.\n",
      "total loss: 0.22636979818344116\n",
      "\tBatch 266/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22636979818344116\n",
      "Epoch loss: 62.68340300023556\n",
      "\tBatch 266/400 processed in 5.57 seconds.\n",
      "total loss: 0.22959290444850922\n",
      "\tBatch 267/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22959290444850922\n",
      "Epoch loss: 62.91299590468407\n",
      "\tBatch 267/400 processed in 5.57 seconds.\n",
      "total loss: 0.2500911355018616\n",
      "\tBatch 268/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2500911355018616\n",
      "Epoch loss: 63.16308704018593\n",
      "\tBatch 268/400 processed in 5.59 seconds.\n",
      "total loss: 0.22355720400810242\n",
      "\tBatch 269/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22355720400810242\n",
      "Epoch loss: 63.38664424419403\n",
      "\tBatch 269/400 processed in 5.64 seconds.\n",
      "total loss: 0.21382060647010803\n",
      "\tBatch 270/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21382060647010803\n",
      "Epoch loss: 63.60046485066414\n",
      "\tBatch 270/400 processed in 5.76 seconds.\n",
      "total loss: 0.24636657536029816\n",
      "\tBatch 271/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24636657536029816\n",
      "Epoch loss: 63.84683142602444\n",
      "\tBatch 271/400 processed in 5.78 seconds.\n",
      "total loss: 0.24492529034614563\n",
      "\tBatch 272/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24492529034614563\n",
      "Epoch loss: 64.09175671637058\n",
      "\tBatch 272/400 processed in 5.67 seconds.\n",
      "total loss: 0.25521475076675415\n",
      "\tBatch 273/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.25521475076675415\n",
      "Epoch loss: 64.34697146713734\n",
      "\tBatch 273/400 processed in 5.65 seconds.\n",
      "total loss: 0.2205750048160553\n",
      "\tBatch 274/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2205750048160553\n",
      "Epoch loss: 64.56754647195339\n",
      "\tBatch 274/400 processed in 5.66 seconds.\n",
      "total loss: 0.21533988416194916\n",
      "\tBatch 275/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21533988416194916\n",
      "Epoch loss: 64.78288635611534\n",
      "\tBatch 275/400 processed in 5.76 seconds.\n",
      "total loss: 0.23524606227874756\n",
      "\tBatch 276/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23524606227874756\n",
      "Epoch loss: 65.01813241839409\n",
      "\tBatch 276/400 processed in 5.61 seconds.\n",
      "total loss: 0.2145143449306488\n",
      "\tBatch 277/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2145143449306488\n",
      "Epoch loss: 65.23264676332474\n",
      "\tBatch 277/400 processed in 5.64 seconds.\n",
      "total loss: 0.2687319219112396\n",
      "\tBatch 278/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2687319219112396\n",
      "Epoch loss: 65.50137868523598\n",
      "\tBatch 278/400 processed in 5.69 seconds.\n",
      "total loss: 0.22955138981342316\n",
      "\tBatch 279/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22955138981342316\n",
      "Epoch loss: 65.7309300750494\n",
      "\tBatch 279/400 processed in 5.59 seconds.\n",
      "total loss: 0.23666001856327057\n",
      "\tBatch 280/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23666001856327057\n",
      "Epoch loss: 65.96759009361267\n",
      "\tBatch 280/400 processed in 5.60 seconds.\n",
      "total loss: 0.22629469633102417\n",
      "\tBatch 281/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22629469633102417\n",
      "Epoch loss: 66.1938847899437\n",
      "\tBatch 281/400 processed in 5.57 seconds.\n",
      "total loss: 0.21533076465129852\n",
      "\tBatch 282/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21533076465129852\n",
      "Epoch loss: 66.409215554595\n",
      "\tBatch 282/400 processed in 5.68 seconds.\n",
      "total loss: 0.2215930074453354\n",
      "\tBatch 283/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2215930074453354\n",
      "Epoch loss: 66.63080856204033\n",
      "\tBatch 283/400 processed in 5.65 seconds.\n",
      "total loss: 0.21273405849933624\n",
      "\tBatch 284/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21273405849933624\n",
      "Epoch loss: 66.84354262053967\n",
      "\tBatch 284/400 processed in 5.58 seconds.\n",
      "total loss: 0.23175090551376343\n",
      "\tBatch 285/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23175090551376343\n",
      "Epoch loss: 67.07529352605343\n",
      "\tBatch 285/400 processed in 5.70 seconds.\n",
      "total loss: 0.25827401876449585\n",
      "\tBatch 286/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.25827401876449585\n",
      "Epoch loss: 67.33356754481792\n",
      "\tBatch 286/400 processed in 5.69 seconds.\n",
      "total loss: 0.23301956057548523\n",
      "\tBatch 287/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23301956057548523\n",
      "Epoch loss: 67.56658710539341\n",
      "\tBatch 287/400 processed in 5.60 seconds.\n",
      "total loss: 0.2583933174610138\n",
      "\tBatch 288/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2583933174610138\n",
      "Epoch loss: 67.82498042285442\n",
      "\tBatch 288/400 processed in 5.56 seconds.\n",
      "total loss: 0.21339310705661774\n",
      "\tBatch 289/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21339310705661774\n",
      "Epoch loss: 68.03837352991104\n",
      "\tBatch 289/400 processed in 5.56 seconds.\n",
      "total loss: 0.21241359412670135\n",
      "\tBatch 290/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21241359412670135\n",
      "Epoch loss: 68.25078712403774\n",
      "\tBatch 290/400 processed in 5.57 seconds.\n",
      "total loss: 0.23194068670272827\n",
      "\tBatch 291/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23194068670272827\n",
      "Epoch loss: 68.48272781074047\n",
      "\tBatch 291/400 processed in 5.72 seconds.\n",
      "total loss: 0.23022252321243286\n",
      "\tBatch 292/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23022252321243286\n",
      "Epoch loss: 68.7129503339529\n",
      "\tBatch 292/400 processed in 5.55 seconds.\n",
      "total loss: 0.23702944815158844\n",
      "\tBatch 293/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23702944815158844\n",
      "Epoch loss: 68.94997978210449\n",
      "\tBatch 293/400 processed in 5.55 seconds.\n",
      "total loss: 0.22452467679977417\n",
      "\tBatch 294/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22452467679977417\n",
      "Epoch loss: 69.17450445890427\n",
      "\tBatch 294/400 processed in 5.66 seconds.\n",
      "total loss: 0.2627090513706207\n",
      "\tBatch 295/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2627090513706207\n",
      "Epoch loss: 69.43721351027489\n",
      "\tBatch 295/400 processed in 5.59 seconds.\n",
      "total loss: 0.21783147752285004\n",
      "\tBatch 296/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21783147752285004\n",
      "Epoch loss: 69.65504498779774\n",
      "\tBatch 296/400 processed in 5.58 seconds.\n",
      "total loss: 0.2367658168077469\n",
      "\tBatch 297/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2367658168077469\n",
      "Epoch loss: 69.89181080460548\n",
      "\tBatch 297/400 processed in 5.58 seconds.\n",
      "total loss: 0.24504348635673523\n",
      "\tBatch 298/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24504348635673523\n",
      "Epoch loss: 70.13685429096222\n",
      "\tBatch 298/400 processed in 5.60 seconds.\n",
      "total loss: 0.21366256475448608\n",
      "\tBatch 299/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21366256475448608\n",
      "Epoch loss: 70.3505168557167\n",
      "\tBatch 299/400 processed in 5.60 seconds.\n",
      "total loss: 0.2324332445859909\n",
      "\tBatch 300/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2324332445859909\n",
      "Epoch loss: 70.5829501003027\n",
      "\tBatch 300/400 processed in 5.59 seconds.\n",
      "total loss: 0.20711779594421387\n",
      "\tBatch 301/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.20711779594421387\n",
      "Epoch loss: 70.79006789624691\n",
      "\tBatch 301/400 processed in 5.64 seconds.\n",
      "total loss: 0.2199307382106781\n",
      "\tBatch 302/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2199307382106781\n",
      "Epoch loss: 71.00999863445759\n",
      "\tBatch 302/400 processed in 5.78 seconds.\n",
      "total loss: 0.21533066034317017\n",
      "\tBatch 303/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21533066034317017\n",
      "Epoch loss: 71.22532929480076\n",
      "\tBatch 303/400 processed in 5.59 seconds.\n",
      "total loss: 0.23880933225154877\n",
      "\tBatch 304/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23880933225154877\n",
      "Epoch loss: 71.46413862705231\n",
      "\tBatch 304/400 processed in 5.62 seconds.\n",
      "total loss: 0.22204197943210602\n",
      "\tBatch 305/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22204197943210602\n",
      "Epoch loss: 71.68618060648441\n",
      "\tBatch 305/400 processed in 5.58 seconds.\n",
      "total loss: 0.23648180067539215\n",
      "\tBatch 306/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23648180067539215\n",
      "Epoch loss: 71.9226624071598\n",
      "\tBatch 306/400 processed in 5.57 seconds.\n",
      "total loss: 0.22301094233989716\n",
      "\tBatch 307/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22301094233989716\n",
      "Epoch loss: 72.1456733494997\n",
      "\tBatch 307/400 processed in 5.89 seconds.\n",
      "total loss: 0.21803618967533112\n",
      "\tBatch 308/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21803618967533112\n",
      "Epoch loss: 72.36370953917503\n",
      "\tBatch 308/400 processed in 5.55 seconds.\n",
      "total loss: 0.21430784463882446\n",
      "\tBatch 309/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21430784463882446\n",
      "Epoch loss: 72.57801738381386\n",
      "\tBatch 309/400 processed in 5.54 seconds.\n",
      "total loss: 0.24109680950641632\n",
      "\tBatch 310/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24109680950641632\n",
      "Epoch loss: 72.81911419332027\n",
      "\tBatch 310/400 processed in 5.64 seconds.\n",
      "total loss: 0.21793025732040405\n",
      "\tBatch 311/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21793025732040405\n",
      "Epoch loss: 73.03704445064068\n",
      "\tBatch 311/400 processed in 5.57 seconds.\n",
      "total loss: 0.23217257857322693\n",
      "\tBatch 312/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23217257857322693\n",
      "Epoch loss: 73.2692170292139\n",
      "\tBatch 312/400 processed in 5.82 seconds.\n",
      "total loss: 0.22662189602851868\n",
      "\tBatch 313/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22662189602851868\n",
      "Epoch loss: 73.49583892524242\n",
      "\tBatch 313/400 processed in 5.70 seconds.\n",
      "total loss: 0.22805552184581757\n",
      "\tBatch 314/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22805552184581757\n",
      "Epoch loss: 73.72389444708824\n",
      "\tBatch 314/400 processed in 5.55 seconds.\n",
      "total loss: 0.21846294403076172\n",
      "\tBatch 315/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21846294403076172\n",
      "Epoch loss: 73.942357391119\n",
      "\tBatch 315/400 processed in 5.58 seconds.\n",
      "total loss: 0.22591136395931244\n",
      "\tBatch 316/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22591136395931244\n",
      "Epoch loss: 74.16826875507832\n",
      "\tBatch 316/400 processed in 5.66 seconds.\n",
      "total loss: 0.22049063444137573\n",
      "\tBatch 317/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22049063444137573\n",
      "Epoch loss: 74.38875938951969\n",
      "\tBatch 317/400 processed in 5.62 seconds.\n",
      "total loss: 0.22842265665531158\n",
      "\tBatch 318/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22842265665531158\n",
      "Epoch loss: 74.617182046175\n",
      "\tBatch 318/400 processed in 5.58 seconds.\n",
      "total loss: 0.22450092434883118\n",
      "\tBatch 319/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22450092434883118\n",
      "Epoch loss: 74.84168297052383\n",
      "\tBatch 319/400 processed in 5.61 seconds.\n",
      "total loss: 0.2295091450214386\n",
      "\tBatch 320/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2295091450214386\n",
      "Epoch loss: 75.07119211554527\n",
      "\tBatch 320/400 processed in 5.59 seconds.\n",
      "total loss: 0.21364937722682953\n",
      "\tBatch 321/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21364937722682953\n",
      "Epoch loss: 75.2848414927721\n",
      "\tBatch 321/400 processed in 5.62 seconds.\n",
      "total loss: 0.23947873711585999\n",
      "\tBatch 322/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23947873711585999\n",
      "Epoch loss: 75.52432022988796\n",
      "\tBatch 322/400 processed in 5.61 seconds.\n",
      "total loss: 0.21694105863571167\n",
      "\tBatch 323/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21694105863571167\n",
      "Epoch loss: 75.74126128852367\n",
      "\tBatch 323/400 processed in 5.83 seconds.\n",
      "total loss: 0.23281943798065186\n",
      "\tBatch 324/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23281943798065186\n",
      "Epoch loss: 75.97408072650433\n",
      "\tBatch 324/400 processed in 5.64 seconds.\n",
      "total loss: 0.23376862704753876\n",
      "\tBatch 325/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23376862704753876\n",
      "Epoch loss: 76.20784935355186\n",
      "\tBatch 325/400 processed in 5.57 seconds.\n",
      "total loss: 0.2440878003835678\n",
      "\tBatch 326/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2440878003835678\n",
      "Epoch loss: 76.45193715393543\n",
      "\tBatch 326/400 processed in 5.64 seconds.\n",
      "total loss: 0.22948235273361206\n",
      "\tBatch 327/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22948235273361206\n",
      "Epoch loss: 76.68141950666904\n",
      "\tBatch 327/400 processed in 5.67 seconds.\n",
      "total loss: 0.23831526935100555\n",
      "\tBatch 328/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23831526935100555\n",
      "Epoch loss: 76.91973477602005\n",
      "\tBatch 328/400 processed in 5.81 seconds.\n",
      "total loss: 0.2284044623374939\n",
      "\tBatch 329/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2284044623374939\n",
      "Epoch loss: 77.14813923835754\n",
      "\tBatch 329/400 processed in 5.61 seconds.\n",
      "total loss: 0.24903354048728943\n",
      "\tBatch 330/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24903354048728943\n",
      "Epoch loss: 77.39717277884483\n",
      "\tBatch 330/400 processed in 5.62 seconds.\n",
      "total loss: 0.23553501069545746\n",
      "\tBatch 331/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23553501069545746\n",
      "Epoch loss: 77.63270778954029\n",
      "\tBatch 331/400 processed in 5.59 seconds.\n",
      "total loss: 0.24186870455741882\n",
      "\tBatch 332/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24186870455741882\n",
      "Epoch loss: 77.87457649409771\n",
      "\tBatch 332/400 processed in 5.78 seconds.\n",
      "total loss: 0.2568809688091278\n",
      "\tBatch 333/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2568809688091278\n",
      "Epoch loss: 78.13145746290684\n",
      "\tBatch 333/400 processed in 5.60 seconds.\n",
      "total loss: 0.23726563155651093\n",
      "\tBatch 334/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23726563155651093\n",
      "Epoch loss: 78.36872309446335\n",
      "\tBatch 334/400 processed in 5.58 seconds.\n",
      "total loss: 0.24297738075256348\n",
      "\tBatch 335/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24297738075256348\n",
      "Epoch loss: 78.61170047521591\n",
      "\tBatch 335/400 processed in 5.57 seconds.\n",
      "total loss: 0.24304671585559845\n",
      "\tBatch 336/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24304671585559845\n",
      "Epoch loss: 78.85474719107151\n",
      "\tBatch 336/400 processed in 5.60 seconds.\n",
      "total loss: 0.24945220351219177\n",
      "\tBatch 337/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24945220351219177\n",
      "Epoch loss: 79.1041993945837\n",
      "\tBatch 337/400 processed in 5.58 seconds.\n",
      "total loss: 0.23541311919689178\n",
      "\tBatch 338/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23541311919689178\n",
      "Epoch loss: 79.3396125137806\n",
      "\tBatch 338/400 processed in 5.60 seconds.\n",
      "total loss: 0.2681410610675812\n",
      "\tBatch 339/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2681410610675812\n",
      "Epoch loss: 79.60775357484818\n",
      "\tBatch 339/400 processed in 5.76 seconds.\n",
      "total loss: 0.2410847544670105\n",
      "\tBatch 340/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2410847544670105\n",
      "Epoch loss: 79.84883832931519\n",
      "\tBatch 340/400 processed in 5.62 seconds.\n",
      "total loss: 0.2324431985616684\n",
      "\tBatch 341/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2324431985616684\n",
      "Epoch loss: 80.08128152787685\n",
      "\tBatch 341/400 processed in 6.33 seconds.\n",
      "total loss: 0.23475103080272675\n",
      "\tBatch 342/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23475103080272675\n",
      "Epoch loss: 80.31603255867958\n",
      "\tBatch 342/400 processed in 5.55 seconds.\n",
      "total loss: 0.22270271182060242\n",
      "\tBatch 343/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22270271182060242\n",
      "Epoch loss: 80.53873527050018\n",
      "\tBatch 343/400 processed in 5.67 seconds.\n",
      "total loss: 0.2266595959663391\n",
      "\tBatch 344/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2266595959663391\n",
      "Epoch loss: 80.76539486646652\n",
      "\tBatch 344/400 processed in 5.80 seconds.\n",
      "total loss: 0.2155994027853012\n",
      "\tBatch 345/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2155994027853012\n",
      "Epoch loss: 80.98099426925182\n",
      "\tBatch 345/400 processed in 5.63 seconds.\n",
      "total loss: 0.2235691398382187\n",
      "\tBatch 346/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2235691398382187\n",
      "Epoch loss: 81.20456340909004\n",
      "\tBatch 346/400 processed in 5.70 seconds.\n",
      "total loss: 0.25772911310195923\n",
      "\tBatch 347/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.25772911310195923\n",
      "Epoch loss: 81.462292522192\n",
      "\tBatch 347/400 processed in 5.60 seconds.\n",
      "total loss: 0.25531312823295593\n",
      "\tBatch 348/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.25531312823295593\n",
      "Epoch loss: 81.71760565042496\n",
      "\tBatch 348/400 processed in 5.61 seconds.\n",
      "total loss: 0.23935900628566742\n",
      "\tBatch 349/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23935900628566742\n",
      "Epoch loss: 81.95696465671062\n",
      "\tBatch 349/400 processed in 5.66 seconds.\n",
      "total loss: 0.2417854219675064\n",
      "\tBatch 350/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2417854219675064\n",
      "Epoch loss: 82.19875007867813\n",
      "\tBatch 350/400 processed in 5.61 seconds.\n",
      "total loss: 0.21083831787109375\n",
      "\tBatch 351/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21083831787109375\n",
      "Epoch loss: 82.40958839654922\n",
      "\tBatch 351/400 processed in 5.60 seconds.\n",
      "total loss: 0.23615561425685883\n",
      "\tBatch 352/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23615561425685883\n",
      "Epoch loss: 82.64574401080608\n",
      "\tBatch 352/400 processed in 5.58 seconds.\n",
      "total loss: 0.25934135913848877\n",
      "\tBatch 353/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.25934135913848877\n",
      "Epoch loss: 82.90508536994457\n",
      "\tBatch 353/400 processed in 5.59 seconds.\n",
      "total loss: 0.2160131335258484\n",
      "\tBatch 354/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2160131335258484\n",
      "Epoch loss: 83.12109850347042\n",
      "\tBatch 354/400 processed in 5.61 seconds.\n",
      "total loss: 0.23187953233718872\n",
      "\tBatch 355/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23187953233718872\n",
      "Epoch loss: 83.35297803580761\n",
      "\tBatch 355/400 processed in 5.68 seconds.\n",
      "total loss: 0.23922546207904816\n",
      "\tBatch 356/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23922546207904816\n",
      "Epoch loss: 83.59220349788666\n",
      "\tBatch 356/400 processed in 5.60 seconds.\n",
      "total loss: 0.2192288041114807\n",
      "\tBatch 357/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2192288041114807\n",
      "Epoch loss: 83.81143230199814\n",
      "\tBatch 357/400 processed in 5.70 seconds.\n",
      "total loss: 0.2331082671880722\n",
      "\tBatch 358/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2331082671880722\n",
      "Epoch loss: 84.04454056918621\n",
      "\tBatch 358/400 processed in 5.58 seconds.\n",
      "total loss: 0.20956166088581085\n",
      "\tBatch 359/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.20956166088581085\n",
      "Epoch loss: 84.25410223007202\n",
      "\tBatch 359/400 processed in 5.59 seconds.\n",
      "total loss: 0.21713097393512726\n",
      "\tBatch 360/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21713097393512726\n",
      "Epoch loss: 84.47123320400715\n",
      "\tBatch 360/400 processed in 5.68 seconds.\n",
      "total loss: 0.22503024339675903\n",
      "\tBatch 361/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22503024339675903\n",
      "Epoch loss: 84.69626344740391\n",
      "\tBatch 361/400 processed in 5.61 seconds.\n",
      "total loss: 0.21770289540290833\n",
      "\tBatch 362/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21770289540290833\n",
      "Epoch loss: 84.91396634280682\n",
      "\tBatch 362/400 processed in 5.58 seconds.\n",
      "total loss: 0.23315133154392242\n",
      "\tBatch 363/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23315133154392242\n",
      "Epoch loss: 85.14711767435074\n",
      "\tBatch 363/400 processed in 5.68 seconds.\n",
      "total loss: 0.2425311654806137\n",
      "\tBatch 364/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2425311654806137\n",
      "Epoch loss: 85.38964883983135\n",
      "\tBatch 364/400 processed in 5.58 seconds.\n",
      "total loss: 0.22531013190746307\n",
      "\tBatch 365/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22531013190746307\n",
      "Epoch loss: 85.61495897173882\n",
      "\tBatch 365/400 processed in 5.72 seconds.\n",
      "total loss: 0.20606231689453125\n",
      "\tBatch 366/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.20606231689453125\n",
      "Epoch loss: 85.82102128863335\n",
      "\tBatch 366/400 processed in 5.58 seconds.\n",
      "total loss: 0.22879301011562347\n",
      "\tBatch 367/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22879301011562347\n",
      "Epoch loss: 86.04981429874897\n",
      "\tBatch 367/400 processed in 5.58 seconds.\n",
      "total loss: 0.2346145212650299\n",
      "\tBatch 368/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2346145212650299\n",
      "Epoch loss: 86.284428820014\n",
      "\tBatch 368/400 processed in 5.58 seconds.\n",
      "total loss: 0.23485572636127472\n",
      "\tBatch 369/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23485572636127472\n",
      "Epoch loss: 86.51928454637527\n",
      "\tBatch 369/400 processed in 5.56 seconds.\n",
      "total loss: 0.21863380074501038\n",
      "\tBatch 370/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21863380074501038\n",
      "Epoch loss: 86.73791834712029\n",
      "\tBatch 370/400 processed in 5.60 seconds.\n",
      "total loss: 0.2637796401977539\n",
      "\tBatch 371/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2637796401977539\n",
      "Epoch loss: 87.00169798731804\n",
      "\tBatch 371/400 processed in 5.67 seconds.\n",
      "total loss: 0.20941612124443054\n",
      "\tBatch 372/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.20941612124443054\n",
      "Epoch loss: 87.21111410856247\n",
      "\tBatch 372/400 processed in 5.60 seconds.\n",
      "total loss: 0.2200683355331421\n",
      "\tBatch 373/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2200683355331421\n",
      "Epoch loss: 87.43118244409561\n",
      "\tBatch 373/400 processed in 5.58 seconds.\n",
      "total loss: 0.22088667750358582\n",
      "\tBatch 374/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22088667750358582\n",
      "Epoch loss: 87.6520691215992\n",
      "\tBatch 374/400 processed in 5.58 seconds.\n",
      "total loss: 0.20239219069480896\n",
      "\tBatch 375/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.20239219069480896\n",
      "Epoch loss: 87.854461312294\n",
      "\tBatch 375/400 processed in 5.61 seconds.\n",
      "total loss: 0.24270804226398468\n",
      "\tBatch 376/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24270804226398468\n",
      "Epoch loss: 88.09716935455799\n",
      "\tBatch 376/400 processed in 5.64 seconds.\n",
      "total loss: 0.2215152531862259\n",
      "\tBatch 377/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2215152531862259\n",
      "Epoch loss: 88.31868460774422\n",
      "\tBatch 377/400 processed in 5.59 seconds.\n",
      "total loss: 0.21654897928237915\n",
      "\tBatch 378/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21654897928237915\n",
      "Epoch loss: 88.5352335870266\n",
      "\tBatch 378/400 processed in 5.61 seconds.\n",
      "total loss: 0.24829789996147156\n",
      "\tBatch 379/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24829789996147156\n",
      "Epoch loss: 88.78353148698807\n",
      "\tBatch 379/400 processed in 5.56 seconds.\n",
      "total loss: 0.23453868925571442\n",
      "\tBatch 380/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23453868925571442\n",
      "Epoch loss: 89.01807017624378\n",
      "\tBatch 380/400 processed in 5.58 seconds.\n",
      "total loss: 0.21153675019741058\n",
      "\tBatch 381/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21153675019741058\n",
      "Epoch loss: 89.22960692644119\n",
      "\tBatch 381/400 processed in 5.72 seconds.\n",
      "total loss: 0.22805531322956085\n",
      "\tBatch 382/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22805531322956085\n",
      "Epoch loss: 89.45766223967075\n",
      "\tBatch 382/400 processed in 5.59 seconds.\n",
      "total loss: 0.22684718668460846\n",
      "\tBatch 383/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22684718668460846\n",
      "Epoch loss: 89.68450942635536\n",
      "\tBatch 383/400 processed in 5.56 seconds.\n",
      "total loss: 0.2342001348733902\n",
      "\tBatch 384/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2342001348733902\n",
      "Epoch loss: 89.91870956122875\n",
      "\tBatch 384/400 processed in 5.60 seconds.\n",
      "total loss: 0.22752349078655243\n",
      "\tBatch 385/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22752349078655243\n",
      "Epoch loss: 90.1462330520153\n",
      "\tBatch 385/400 processed in 5.67 seconds.\n",
      "total loss: 0.2416718602180481\n",
      "\tBatch 386/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2416718602180481\n",
      "Epoch loss: 90.38790491223335\n",
      "\tBatch 386/400 processed in 5.61 seconds.\n",
      "total loss: 0.22611784934997559\n",
      "\tBatch 387/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22611784934997559\n",
      "Epoch loss: 90.61402276158333\n",
      "\tBatch 387/400 processed in 5.58 seconds.\n",
      "total loss: 0.2199586033821106\n",
      "\tBatch 388/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2199586033821106\n",
      "Epoch loss: 90.83398136496544\n",
      "\tBatch 388/400 processed in 5.59 seconds.\n",
      "total loss: 0.2455187886953354\n",
      "\tBatch 389/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.2455187886953354\n",
      "Epoch loss: 91.07950015366077\n",
      "\tBatch 389/400 processed in 5.59 seconds.\n",
      "total loss: 0.23099058866500854\n",
      "\tBatch 390/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23099058866500854\n",
      "Epoch loss: 91.31049074232578\n",
      "\tBatch 390/400 processed in 5.59 seconds.\n",
      "total loss: 0.20991748571395874\n",
      "\tBatch 391/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.20991748571395874\n",
      "Epoch loss: 91.52040822803974\n",
      "\tBatch 391/400 processed in 5.59 seconds.\n",
      "total loss: 0.23067845404148102\n",
      "\tBatch 392/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23067845404148102\n",
      "Epoch loss: 91.75108668208122\n",
      "\tBatch 392/400 processed in 5.85 seconds.\n",
      "total loss: 0.22661900520324707\n",
      "\tBatch 393/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22661900520324707\n",
      "Epoch loss: 91.97770568728447\n",
      "\tBatch 393/400 processed in 5.67 seconds.\n",
      "total loss: 0.23057517409324646\n",
      "\tBatch 394/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.23057517409324646\n",
      "Epoch loss: 92.20828086137772\n",
      "\tBatch 394/400 processed in 5.59 seconds.\n",
      "total loss: 0.24311012029647827\n",
      "\tBatch 395/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.24311012029647827\n",
      "Epoch loss: 92.4513909816742\n",
      "\tBatch 395/400 processed in 5.63 seconds.\n",
      "total loss: 0.20740161836147308\n",
      "\tBatch 396/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.20740161836147308\n",
      "Epoch loss: 92.65879260003567\n",
      "\tBatch 396/400 processed in 5.63 seconds.\n",
      "total loss: 0.21823817491531372\n",
      "\tBatch 397/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.21823817491531372\n",
      "Epoch loss: 92.87703077495098\n",
      "\tBatch 397/400 processed in 6.11 seconds.\n",
      "total loss: 0.25387096405029297\n",
      "\tBatch 398/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.25387096405029297\n",
      "Epoch loss: 93.13090173900127\n",
      "\tBatch 398/400 processed in 6.01 seconds.\n",
      "total loss: 0.22016213834285736\n",
      "\tBatch 399/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.22016213834285736\n",
      "Epoch loss: 93.35106387734413\n",
      "\tBatch 399/400 processed in 5.71 seconds.\n",
      "total loss: 0.20681314170360565\n",
      "\tBatch 400/400, Forward pass done, starting backward pass.\n",
      "total loss item: 0.20681314170360565\n",
      "Epoch loss: 93.55787701904774\n",
      "\tBatch 400/400 processed in 5.73 seconds.\n",
      "Epoch [1/100] completed in 2266.36 seconds, Avg Loss: 0.23389469254761935\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 67\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_loader, device, num_epochs, accumulation_steps, checkpoint_path)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mepoch_start_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds, Avg Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# Validation phase\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m     avg_val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] validation completed, Avg Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_val_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(model, val_loader, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, _ \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[1;32m      9\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 10\u001b[0m     reconstructed_x, f_x, sx_matrix \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m     11\u001b[0m     reconstruction_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmse_loss(reconstructed_x, data)\n\u001b[1;32m     12\u001b[0m     total_val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reconstruction_loss\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# Accumulate the validation loss\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "# Define the input shape, latent dimensions, and output shape\n",
    "input_shape = (1, 32, 32)  # Shape for grayscale CIFAR-10 (1 channel, 32x32 images)\n",
    "latent_dims = [32, 64, 128, 256, 512]  # Updated latent dimensions\n",
    "output_shape = (1, 32, 32)  # Shape for grayscale CIFAR-10\n",
    "\n",
    "# Create an instance of the EchoModel\n",
    "model = EchoModel(input_shape, latent_dims, output_shape).to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Define the number of epochs and loss weights\n",
    "num_epochs = 100\n",
    "\n",
    "# Train the model\n",
    "trained_model = train(model, optimizer, train_loader, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d9bfca1-61e3-4e98-90a1-dcc1062be352",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Validation phase\u001b[39;00m\n\u001b[1;32m     15\u001b[0m avg_val_loss \u001b[38;5;241m=\u001b[39m validate(model, val_loader, device)\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] validation completed, Avg Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_val_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch' is not defined"
     ]
    }
   ],
   "source": [
    "def validate(model, val_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient computation during validation\n",
    "        for data, _ in val_loader:\n",
    "            data = data.to(device)\n",
    "            reconstructed_x = model(data)\n",
    "            reconstruction_loss = nn.functional.mse_loss(reconstructed_x, data)\n",
    "            total_val_loss += reconstruction_loss.item()  # Accumulate the validation loss\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)  # Calculate average loss\n",
    "    return avg_val_loss\n",
    "\n",
    "# Validation phase\n",
    "avg_val_loss = validate(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0530ce7-2a32-4d74-a4e5-11f5d0cae3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Epoch [{epoch+1}/{num_epochs}] validation completed, Avg Validation Loss: {avg_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "qmfrva2uRuvu",
   "metadata": {
    "id": "qmfrva2uRuvu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at epoch 1 to checkpoint-gaussian.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, filename=\"checkpoint.pth\"):\n",
    "    \"\"\"Saves the model and optimizer state at the specified path.\"\"\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, filename)\n",
    "    print(f\"Checkpoint saved at epoch {epoch} to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5040ed-080f-48d4-a005-f2e528884474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
